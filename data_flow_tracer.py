#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ 
Googleæ¤œç´¢â†’ãƒ‡ãƒ¼ã‚¿åé›†â†’ä¿å­˜ã®å®Œå…¨è¿½è·¡
"""

import json
import os
import sys
import time
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import traceback

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from core.activity_learning_engine import ActivityLearningEngine

class DataFlowTracer:
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.trace_log = []
        self.data_checkpoints = {}
        self.session_states = {}
        
        # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.trace_dir = Path("D:/setsuna_bot/debug_logs/data_flow")
        self.trace_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡é–‹å§‹: {datetime.now().strftime('%H:%M:%S')}")
    
    def trace_point(self, checkpoint: str, data: Any, metadata: Dict = None):
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡ãƒã‚¤ãƒ³ãƒˆ"""
        timestamp = datetime.now().isoformat()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        data_str = json.dumps(data, sort_keys=True, default=str)
        data_hash = hashlib.md5(data_str.encode()).hexdigest()[:8]
        
        trace_entry = {
            "timestamp": timestamp,
            "checkpoint": checkpoint,
            "data_hash": data_hash,
            "data_type": type(data).__name__,
            "data_size": len(data_str),
            "metadata": metadata or {}
        }
        
        self.trace_log.append(trace_entry)
        self.data_checkpoints[checkpoint] = {
            "data": data,
            "trace_entry": trace_entry
        }
        
        print(f"ğŸ“ [{timestamp}] {checkpoint}: {data_hash} ({trace_entry['data_size']} bytes)")
        if metadata:
            for key, value in metadata.items():
                print(f"   {key}: {value}")
    
    def compare_checkpoints(self, checkpoint1: str, checkpoint2: str) -> Dict[str, Any]:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“æ¯”è¼ƒ"""
        if checkpoint1 not in self.data_checkpoints or checkpoint2 not in self.data_checkpoints:
            return {"error": "ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        cp1 = self.data_checkpoints[checkpoint1]
        cp2 = self.data_checkpoints[checkpoint2]
        
        comparison = {
            "checkpoint1": checkpoint1,
            "checkpoint2": checkpoint2,
            "hash_match": cp1["trace_entry"]["data_hash"] == cp2["trace_entry"]["data_hash"],
            "size_diff": cp2["trace_entry"]["data_size"] - cp1["trace_entry"]["data_size"],
            "time_diff": (
                datetime.fromisoformat(cp2["trace_entry"]["timestamp"]) - 
                datetime.fromisoformat(cp1["trace_entry"]["timestamp"])
            ).total_seconds()
        }
        
        return comparison
    
    def trace_google_search_flow(self, engine, query: str):
        """Googleæ¤œç´¢ãƒ•ãƒ­ãƒ¼è¿½è·¡"""
        print(f"\nğŸ” Googleæ¤œç´¢ãƒ•ãƒ­ãƒ¼è¿½è·¡: {query}")
        
        try:
            # æ¤œç´¢å‰çŠ¶æ…‹
            self.trace_point("search_request", {
                "query": query,
                "engine_status": engine.search_manager.get_status()
            })
            
            # æ¤œç´¢å®Ÿè¡Œ
            search_start = time.time()
            search_result = engine._perform_web_search_detailed(query)
            search_time = time.time() - search_start
            
            # æ¤œç´¢çµæœè¿½è·¡
            self.trace_point("search_result", search_result, {
                "execution_time": search_time,
                "success": search_result.get("success", False),
                "source_count": len(search_result.get("sources", []))
            })
            
            return search_result
            
        except Exception as e:
            self.trace_point("search_error", {
                "error": str(e),
                "traceback": traceback.format_exc()
            })
            return {"success": False, "error": str(e)}
    
    def trace_session_data_flow(self, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡"""
        print(f"\nğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡: {session_id}")
        
        try:
            # ActivityLearningEngineåˆæœŸåŒ–
            engine = ActivityLearningEngine()
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆãƒ•ãƒ­ãƒ¼
            session = engine.create_session(
                theme="ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡ãƒ†ã‚¹ãƒˆ",
                learning_type="æ¦‚è¦",
                depth_level=1,
                time_limit=60,
                budget_limit=0.5,
                tags=["ãƒ‡ãƒãƒƒã‚°", "è¿½è·¡"]
            )
            
            self.trace_point("session_created", {
                "session_id": session,
                "engine_status": "initialized"
            })
            
            # æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆè¿½è·¡
            queries = engine._generate_search_queries("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡ãƒ†ã‚¹ãƒˆ", 1)
            self.trace_point("queries_generated", queries)
            
            # å„æ¤œç´¢ã®è©³ç´°è¿½è·¡
            all_sources = []
            for i, query in enumerate(queries[:2]):  # æœ€åˆã®2ä»¶ã®ã¿ãƒ†ã‚¹ãƒˆ
                print(f"\nğŸ” æ¤œç´¢ {i+1}/{len(queries[:2])}: {query}")
                
                search_result = self.trace_google_search_flow(engine, query)
                
                if search_result.get("success"):
                    sources = search_result.get("sources", [])
                    all_sources.extend(sources)
                    
                    self.trace_point(f"search_{i+1}_sources", sources, {
                        "query": query,
                        "source_count": len(sources)
                    })
                else:
                    self.trace_point(f"search_{i+1}_failed", search_result)
            
            # åé›†çµæœçµ±åˆè¿½è·¡
            self.trace_point("all_sources_collected", all_sources, {
                "total_sources": len(all_sources)
            })
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰è¿½è·¡
            session_data = {
                "collection_results": {
                    "information_sources": all_sources,
                    "raw_content_count": len(all_sources),
                    "filtered_content_count": len(all_sources),
                    "search_queries": queries,
                    "collection_timestamp": datetime.now().isoformat()
                }
            }
            
            self.trace_point("session_data_built", session_data)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡
            sessions_dir = Path("D:/setsuna_bot/data/activity_knowledge/sessions")
            session_file = sessions_dir / f"{session}.json"
            
            # ä¿å­˜å‰çŠ¶æ…‹
            self.trace_point("pre_save_state", {
                "file_path": str(session_file),
                "file_exists": session_file.exists(),
                "dir_exists": sessions_dir.exists(),
                "dir_writable": os.access(sessions_dir, os.W_OK)
            })
            
            # å®Ÿéš›ã®ä¿å­˜ãƒ†ã‚¹ãƒˆ
            try:
                with open(session_file, 'w', encoding='utf-8') as f:
                    json.dump(session_data, f, ensure_ascii=False, indent=2, default=str)
                
                # ä¿å­˜å¾Œæ¤œè¨¼
                if session_file.exists():
                    file_size = session_file.stat().st_size
                    
                    # èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
                    with open(session_file, 'r', encoding='utf-8') as f:
                        saved_data = json.load(f)
                    
                    self.trace_point("save_success", {
                        "file_path": str(session_file),
                        "file_size": file_size,
                        "data_integrity": saved_data == session_data
                    })
                    
                    # ãƒ‡ãƒ¼ã‚¿æ¯”è¼ƒ
                    comparison = self.compare_checkpoints("session_data_built", "save_success")
                    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§: {comparison}")
                    
                else:
                    self.trace_point("save_failed", {
                        "error": "ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
                    })
                    
            except Exception as save_error:
                self.trace_point("save_error", {
                    "error": str(save_error),
                    "traceback": traceback.format_exc()
                })
            
            return session
            
        except Exception as e:
            self.trace_point("session_flow_error", {
                "error": str(e),
                "traceback": traceback.format_exc()
            })
            return None
    
    def analyze_data_flow(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼åˆ†æ"""
        print(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼åˆ†æ:")
        print("=" * 50)
        
        # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒã‚¤ãƒ³ãƒˆä¸€è¦§
        print(f"ç·ãƒˆãƒ¬ãƒ¼ã‚¹ãƒã‚¤ãƒ³ãƒˆ: {len(self.trace_log)}")
        for i, entry in enumerate(self.trace_log, 1):
            print(f"{i:2d}. [{entry['timestamp']}] {entry['checkpoint']}")
            print(f"     ãƒãƒƒã‚·ãƒ¥: {entry['data_hash']}, ã‚µã‚¤ã‚º: {entry['data_size']} bytes")
        
        # ãƒ‡ãƒ¼ã‚¿å¤‰åŒ–åˆ†æ
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å¤‰åŒ–åˆ†æ:")
        checkpoints = list(self.data_checkpoints.keys())
        for i in range(len(checkpoints) - 1):
            comparison = self.compare_checkpoints(checkpoints[i], checkpoints[i + 1])
            if "error" not in comparison:
                change_indicator = "ğŸ”„" if not comparison["hash_match"] else "ğŸ”’"
                print(f"{change_indicator} {checkpoints[i]} â†’ {checkpoints[i + 1]}")
                print(f"    æ™‚é–“å·®: {comparison['time_diff']:.2f}ç§’")
                print(f"    ã‚µã‚¤ã‚ºå·®: {comparison['size_diff']:+d} bytes")
                print(f"    ãƒ‡ãƒ¼ã‚¿å¤‰åŒ–: {'ã‚ã‚Š' if not comparison['hash_match'] else 'ãªã—'}")
    
    def save_trace_report(self, session_id: str = None):
        """ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        report = {
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "trace_log": self.trace_log,
            "data_checkpoints": {
                k: {
                    "trace_entry": v["trace_entry"],
                    "data_preview": str(v["data"])[:500]
                }
                for k, v in self.data_checkpoints.items()
            }
        }
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.trace_dir / f"data_flow_trace_{timestamp}.json"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"\nğŸ“Š ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
            return str(report_file)
            
        except Exception as e:
            print(f"âŒ ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    tracer = DataFlowTracer()
    
    print("ğŸš€ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡
    session_id = tracer.trace_session_data_flow("test_session")
    
    # åˆ†æçµæœè¡¨ç¤º
    tracer.analyze_data_flow()
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    tracer.save_trace_report(session_id)
    
    print("\nâœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡å®Œäº†")

if __name__ == "__main__":
    main()