# ç¬¬5ç« : çµ±åˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚·ã‚¹ãƒ†ãƒ 

## **ç« ã®æ¦‚è¦**

ã“ã®ç« ã§ã¯ã€YouTubeãƒŠãƒ¬ãƒƒã‚¸ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã€è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ã€æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã¾ã§ã€å®Ÿè·µçš„ãªãƒ‡ãƒ¼ã‚¿ç®¡ç†æŠ€è¡“ã‚’ä½“ç³»çš„ã«å­¦ã³ã¾ã™ã€‚

**å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**: `storage/unified_storage.py` (ç´„447è¡Œ)  
**ä¸»è¦æŠ€è¡“**: JSON Database, ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ, ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ , ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³

---

## **ğŸ“‹ unified_storage.pyãƒ•ã‚¡ã‚¤ãƒ«ã®å…¨ä½“åƒ**

### **ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„ã¨å½¹å‰²**

`storage/unified_storage.py`ã¯ã€YouTubeãƒŠãƒ¬ãƒƒã‚¸ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹**ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ã®ä¸­æ ¸ã‚¨ãƒ³ã‚¸ãƒ³**ã§ã™ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ‹…ã†å½¹å‰²ã¯ï¼š

1. **çµ±åˆãƒ‡ãƒ¼ã‚¿ç®¡ç†**: å‹•ç”»ãƒ»ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆãƒ»åˆ†æçµæœã®ä¸€å…ƒç®¡ç†
2. **ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹DB**: JSONã«ã‚ˆã‚‹è»½é‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å®Ÿè£…
3. **è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: ãƒ‡ãƒ¼ã‚¿ä¿è­·ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
4. **é«˜é€Ÿæ¤œç´¢**: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®åŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿æ¤œç´¢
5. **ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ**: æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®æ®µéšçš„ç§»è¡Œã‚µãƒãƒ¼ãƒˆ

### **ã‚·ã‚¹ãƒ†ãƒ å†…ã§ã®ä½ç½®ã¥ã‘**

```
YouTube Knowledge System ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  collectors/multi_playlist_collector.py â”‚  â† ãƒ‡ãƒ¼ã‚¿åé›†
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼ åé›†ãƒ‡ãƒ¼ã‚¿
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  analyzers/description_analyzer.py   â”‚  â† AIåˆ†æ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼ åˆ†æçµæœ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  storage/unified_storage.py          â”‚  â† ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”‚  ãƒ»JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†               â”‚
â”‚  ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§                â”‚
â”‚  ãƒ»æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹                  â”‚
â”‚  ãƒ»çµ±è¨ˆæƒ…å ±ç®¡ç†                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼ æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  gui/video_main_window.py           â”‚  â† GUIè¡¨ç¤º
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **ä»–ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®é–¢é€£æ€§**

- **`core/data_models.py`**: `KnowledgeDatabase`ãƒ»`Video`ãƒ»`Playlist`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä¿å­˜ãƒ»å¾©å…ƒ
- **`collectors/multi_playlist_collector.py`**: åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ä¿å­˜
- **`analyzers/description_analyzer.py`**: AIåˆ†æçµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµ±åˆ
- **`managers/playlist_config_manager.py`**: ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆè¨­å®šã¨ã®é€£æº
- **`gui/video_main_window.py`**: GUIæ“ä½œã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ›´æ–°

### **ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆï¼ˆ447è¡Œã®å†…è¨³ï¼‰**

1. **åˆæœŸåŒ–ãƒ»è¨­å®š** (1-35è¡Œ): ã‚¯ãƒ©ã‚¹å®šç¾©ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®šã€åŸºæœ¬æ§‹é€ 
2. **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†** (36-93è¡Œ): èª­ã¿è¾¼ã¿ãƒ»ä¿å­˜ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½
3. **CRUDæ“ä½œ** (94-212è¡Œ): ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆãƒ»èª­ã¿å–ã‚Šãƒ»æ›´æ–°ãƒ»å‰Šé™¤
4. **æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿** (213-275è¡Œ): ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹é«˜é€Ÿæ¤œç´¢
5. **çµ±è¨ˆãƒ»åˆ†æ** (276-318è¡Œ): ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
6. **ä¿å®ˆãƒ»ç§»è¡Œ** (319-447è¡Œ): ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†ã€ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

---

## **ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ**

### **ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹DBã¨ã¯ï¼ˆåˆå¿ƒè€…å‘ã‘è§£èª¬ï¼‰**

#### **ğŸ—ƒï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬æ¦‚å¿µ**

**å¾“æ¥ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ vs ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹DB**

```
å¾“æ¥ã®ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒŠãƒ«DBï¼ˆMySQL, PostgreSQLç­‰ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å°‚ç”¨ã‚µãƒ¼ãƒãƒ¼      â”‚  â† åˆ¥é€”ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒãƒ¼ãŒå¿…è¦
â”‚  è¤‡é›‘ãªè¨­å®š       â”‚  â† ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»è¨­å®šãŒè¤‡é›‘
â”‚  SQLè¨€èª         â”‚  â† å°‚ç”¨ã‚¯ã‚¨ãƒªè¨€èªã®å­¦ç¿’å¿…è¦
â”‚  é«˜æ€§èƒ½ãƒ»é«˜æ©Ÿèƒ½    â”‚  â† å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ å‘ã‘
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹DBï¼ˆJSON, SQLiteç­‰ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«      â”‚  â† ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼ã§ç°¡å˜ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
â”‚  ç°¡å˜ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—   â”‚  â† è¿½åŠ ã‚½ãƒ•ãƒˆä¸è¦
â”‚  ãƒ—ãƒ­ã‚°ãƒ©ãƒ è¨€èª    â”‚  â† Pythonã®è¾æ›¸æ“ä½œã§ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
â”‚  ä¸­å°è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ   â”‚  â† é©åº¦ãªæ€§èƒ½ã¨ç°¡ä¾¿æ€§
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹DBã®åˆ©ç‚¹**

1. **ã‚·ãƒ³ãƒ—ãƒ«ã•**: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»è¨­å®šä¸è¦
2. **ãƒãƒ¼ã‚¿ãƒ“ãƒªãƒ†ã‚£**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
3. **å¯èª­æ€§**: ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ã§ä¸­èº«ç¢ºèªå¯èƒ½
4. **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®¹æ˜“æ€§**: é€šå¸¸ã®ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã§å®Œäº†
5. **é–‹ç™ºåŠ¹ç‡**: è¿½åŠ å­¦ç¿’ã‚³ã‚¹ãƒˆæœ€å°

#### **ğŸ”§ JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´**

**JSONå½¢å¼ã®åˆ©ç‚¹**

```json
// äººé–“ãŒèª­ã¿ã‚„ã™ã„æ§‹é€ 
{
  "videos": {
    "video_123": {
      "title": "ã‚µãƒ³ãƒ—ãƒ«å‹•ç”»",
      "creators": {
        "vocal": "ç”°ä¸­å¤ªéƒ",
        "composer": "éˆ´æœ¨èŠ±å­"
      },
      "analysis_date": "2024-06-25T10:30:00"
    }
  },
  "playlists": {
    "playlist_456": {
      "title": "ãŠæ°—ã«å…¥ã‚Šæ¥½æ›²",
      "video_ids": ["video_123", "video_789"]
    }
  }
}
```

**åˆå¿ƒè€…å‘ã‘: JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆ¶ç´„**

Webæ¤œç´¢çµæœã‚ˆã‚Šï¼šJSONã®èª²é¡Œã¨å¯¾ç­–

```
åˆ¶ç´„1: æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
å•é¡Œ: å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§ã®ç·šå½¢æ¤œç´¢ã¯é…ã„
å¯¾ç­–: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹é€ ã®ä½µç”¨

åˆ¶ç´„2: åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
å•é¡Œ: è¤‡æ•°ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ã®åŒæ™‚æ›¸ãè¾¼ã¿
å¯¾ç­–: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯æ©Ÿæ§‹ã®å®Ÿè£…

åˆ¶ç´„3: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
å•é¡Œ: éƒ¨åˆ†çš„ãªæ›¸ãè¾¼ã¿å¤±æ•—
å¯¾ç­–: ã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿ï¼ˆä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«çµŒç”±ï¼‰
```

### **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ ã®å®Ÿè£…**

#### **ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®è¨­è¨ˆ**

```python
class UnifiedStorage:
    def __init__(self, data_dir: Path = None):
        self.data_dir = data_dir or DATA_DIR
        self.db_file = self.data_dir / "unified_knowledge_db.json"
        self.backup_dir = self.data_dir / "backups"
        self.legacy_dir = self.data_dir / "legacy"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        self.legacy_dir.mkdir(parents=True, exist_ok=True)
        
        self._database: Optional[KnowledgeDatabase] = None
```

**ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®å½¹å‰²**

```
data/
â”œâ”€â”€ unified_knowledge_db.json    # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
â”œâ”€â”€ backups/                     # è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
â”‚   â”œâ”€â”€ unified_knowledge_db_20250625_103000.json
â”‚   â”œâ”€â”€ unified_knowledge_db_20250625_104500.json
â”‚   â””â”€â”€ ...
â””â”€â”€ legacy/                      # æ—§å½¢å¼ãƒ‡ãƒ¼ã‚¿
    â”œâ”€â”€ old_playlist_data.json
    â””â”€â”€ ...
```

#### **ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ã‚·ã‚¹ãƒ†ãƒ **

```python
def load_database(self) -> KnowledgeDatabase:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
    if self._database is None:
        if self.db_file.exists():
            try:
                with open(self.db_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                self._database = KnowledgeDatabase.from_dict(data)
                print(f"çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {self._database.total_videos}å‹•ç”», {self._database.total_playlists}ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ")
            except Exception as e:
                print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                print("æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã™")
                self._database = create_empty_database()
        else:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ç¢ºèª
            legacy_files = self._find_legacy_files()
            if legacy_files:
                print("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ–°ã—ã„å½¢å¼ã«ç§»è¡Œã—ã¾ã™...")
                self._database = self._migrate_legacy_data(legacy_files)
                self.save_database()
            else:
                self._database = create_empty_database()
    
    return self._database
```

**èª­ã¿è¾¼ã¿å‡¦ç†ã®ç‰¹å¾´**

1. **é…å»¶èª­ã¿è¾¼ã¿**: åˆå›ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã«ã®ã¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
2. **ã‚¨ãƒ©ãƒ¼å¾©æ—§**: ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®è‡ªå‹•å¾©æ—§
3. **ãƒ¬ã‚¬ã‚·ãƒ¼å¯¾å¿œ**: æ—§å½¢å¼ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•æ¤œå‡ºãƒ»ç§»è¡Œ
4. **ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥**: èª­ã¿è¾¼ã¿å¾Œã¯ãƒ¡ãƒ¢ãƒªä¸Šã§é«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹

### **ã‚¢ãƒˆãƒŸãƒƒã‚¯ä¿å­˜ã‚·ã‚¹ãƒ†ãƒ **

#### **ğŸ›¡ï¸ ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ä¿è¨¼**

```python
def save_database(self, create_backup: bool = True) -> None:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜"""
    if self._database is None:
        return
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    if create_backup and self.db_file.exists():
        backup_file = self.backup_dir / f"unified_knowledge_db_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        shutil.copy2(self.db_file, backup_file)
        print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã—ã¾ã—ãŸ: {backup_file}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
    try:
        self._database.last_updated = datetime.now()
        with open(self.db_file, 'w', encoding='utf-8') as f:
            json.dump(self._database.to_dict(), f, ensure_ascii=False, indent=2)
        print(f"çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {self.db_file}")
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        raise
```

**ã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿ã®æ”¹è‰¯ç‰ˆå®Ÿè£…ä¾‹**

```python
def save_database_atomic(self, create_backup: bool = True) -> None:
    """ã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿ã«ã‚ˆã‚‹å®‰å…¨ãªä¿å­˜"""
    if self._database is None:
        return
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ãŸã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿
    temp_file = self.db_file.with_suffix('.tmp')
    
    try:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        self._database.last_updated = datetime.now()
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(self._database.to_dict(), f, ensure_ascii=False, indent=2)
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆæ›¸ãè¾¼ã¿æˆåŠŸå¾Œï¼‰
        if create_backup and self.db_file.exists():
            backup_file = self.backup_dir / f"unified_knowledge_db_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            shutil.copy2(self.db_file, backup_file)
        
        # ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªç½®ãæ›ãˆï¼ˆOSãƒ¬ãƒ™ãƒ«ã§ä¿è¨¼ã•ã‚Œã‚‹ï¼‰
        temp_file.replace(self.db_file)
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if temp_file.exists():
            temp_file.unlink()
        raise e
```

**åˆå¿ƒè€…å‘ã‘: ã‚¢ãƒˆãƒŸãƒƒã‚¯æ“ä½œã¨ã¯**

```
ã‚¢ãƒˆãƒŸãƒƒã‚¯æ“ä½œ = ã€Œå…¨ã¦æˆåŠŸã€ã¾ãŸã¯ã€Œå…¨ã¦å¤±æ•—ã€
ï¼ˆéƒ¨åˆ†çš„ãªæˆåŠŸã¯å­˜åœ¨ã—ãªã„ï¼‰

ä¾‹ï¼šéŠ€è¡Œé€é‡‘
âŒ éã‚¢ãƒˆãƒŸãƒƒã‚¯: Aå£åº§ã‹ã‚‰å¼•ãè½ã¨ã—æˆåŠŸã€Bå£åº§ã¸ã®å…¥é‡‘å¤±æ•—
âœ… ã‚¢ãƒˆãƒŸãƒƒã‚¯: å…¨å·¥ç¨‹æˆåŠŸ or å…¨å·¥ç¨‹ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã§ã®ã‚¢ãƒˆãƒŸãƒƒã‚¯æ€§:
âŒ ç›´æ¥æ›¸ãè¾¼ã¿: é€”ä¸­ã§ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹ã¨ç ´æãƒ•ã‚¡ã‚¤ãƒ«
âœ… ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«çµŒç”±: å®Œå…¨æ›¸ãè¾¼ã¿å¾Œã«ç½®ãæ›ãˆ
```

---

## **ğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã¨ãƒ‡ãƒ¼ã‚¿æ¤œç´¢**

### **æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨­è¨ˆæ€æƒ³**

#### **âš¡ é«˜é€Ÿæ¤œç´¢ã®å®Ÿç¾**

**ç·šå½¢æ¤œç´¢ vs ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢**

```python
# âŒ éåŠ¹ç‡: ç·šå½¢æ¤œç´¢ï¼ˆå…¨å‹•ç”»ã‚’é †æ¬¡ç¢ºèªï¼‰
def find_videos_by_creator_slow(creator_name: str):
    results = []
    for video_id, video in all_videos.items():  # O(n)
        if video.creative_insight:
            for creator in video.creative_insight.creators:
                if creator.name == creator_name:
                    results.append(video)
    return results

# âœ… åŠ¹ç‡çš„: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢ï¼ˆäº‹å‰è¨ˆç®—æ¸ˆã¿ï¼‰
def find_videos_by_creator_fast(creator_name: str):
    video_ids = creator_index.get(creator_name, [])  # O(1)
    return [videos[vid] for vid in video_ids]         # O(m) m=çµæœæ•°
```

**è¨ˆç®—é‡ã®æ¯”è¼ƒ**

```
ãƒ‡ãƒ¼ã‚¿é‡: 10,000å‹•ç”»ã®å ´åˆ

ç·šå½¢æ¤œç´¢: O(n) = 10,000å›ã®æ¯”è¼ƒå‡¦ç†
ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢: O(1) + O(m) = 1å›ã®è¾æ›¸æ¤œç´¢ + çµæœæ•°åˆ†ã®å–å¾—

é€Ÿåº¦å‘ä¸Š: ç´„100-1000å€ï¼ˆçµæœæ•°ã«ã‚ˆã‚‹ï¼‰
```

#### **ğŸ—‚ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹é€ ã®å®Ÿè£…**

**å®Ÿéš›ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹é€ **

```python
# core/data_models.py ã® KnowledgeDatabase ã‚ˆã‚Š
class KnowledgeDatabase:
    def __init__(self):
        self.videos: Dict[str, Video] = {}
        self.playlists: Dict[str, Playlist] = {}
        
        # æ¨ªæ–­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.creator_index: Dict[str, List[str]] = {}  # creator_name -> video_ids
        self.tag_index: Dict[str, List[str]] = {}      # tag -> video_ids  
        self.theme_index: Dict[str, List[str]] = {}    # theme -> video_ids
```

**ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ã‚·ã‚¹ãƒ†ãƒ **

```python
def rebuild_indexes(self):
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰"""
    self.creator_index = {}
    self.tag_index = {}
    self.theme_index = {}
    
    for video_id, video in self.videos.items():
        # ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        if video.creative_insight:
            for creator in video.creative_insight.creators:
                if creator.name not in self.creator_index:
                    self.creator_index[creator.name] = []
                self.creator_index[creator.name].append(video_id)
        
        # ã‚¿ã‚°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        for tag in video.metadata.tags:
            if tag not in self.tag_index:
                self.tag_index[tag] = []
            self.tag_index[tag].append(video_id)
        
        # ãƒ†ãƒ¼ãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        if video.creative_insight:
            for theme in video.creative_insight.themes:
                if theme not in self.theme_index:
                    self.theme_index[theme] = []
                self.theme_index[theme].append(video_id)
    
    # çµ±è¨ˆæ›´æ–°
    self.total_videos = len(self.videos)
    self.total_playlists = len(self.playlists)
    self.last_updated = datetime.now()
```

### **æ¤œç´¢æ©Ÿèƒ½ã®å®Ÿè£…**

#### **ğŸ¯ å¤šæ§˜ãªæ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

```python
def search_videos_by_creator(self, creator_name: str) -> List[Video]:
    """ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼åã§å‹•ç”»æ¤œç´¢"""
    db = self.load_database()
    return db.get_videos_by_creator(creator_name)

def search_videos_by_tag(self, tag: str) -> List[Video]:
    """ã‚¿ã‚°ã§å‹•ç”»æ¤œç´¢"""
    db = self.load_database()
    return db.get_videos_by_tag(tag)

def search_videos_by_theme(self, theme: str) -> List[Video]:
    """ãƒ†ãƒ¼ãƒã§å‹•ç”»æ¤œç´¢"""
    db = self.load_database()
    return db.get_videos_by_theme(theme)
```

**æ¤œç´¢çµæœã®çµ±åˆæ©Ÿèƒ½ä¾‹**

```python
def advanced_search(self, 
                   creator: Optional[str] = None,
                   tag: Optional[str] = None, 
                   theme: Optional[str] = None,
                   playlist_id: Optional[str] = None) -> List[Video]:
    """è¤‡åˆæ¡ä»¶ã§ã®é«˜åº¦æ¤œç´¢"""
    
    db = self.load_database()
    result_sets = []
    
    # å„æ¡ä»¶ã§ã®æ¤œç´¢çµæœã‚’å–å¾—
    if creator:
        creator_videos = set(v.metadata.id for v in db.get_videos_by_creator(creator))
        result_sets.append(creator_videos)
    
    if tag:
        tag_videos = set(v.metadata.id for v in db.get_videos_by_tag(tag))
        result_sets.append(tag_videos)
    
    if theme:
        theme_videos = set(v.metadata.id for v in db.get_videos_by_theme(theme))
        result_sets.append(theme_videos)
    
    if playlist_id:
        playlist_videos = set(v.metadata.id for v in self.get_videos_by_playlist(playlist_id))
        result_sets.append(playlist_videos)
    
    # ç©é›†åˆï¼ˆANDæ¤œç´¢ï¼‰
    if result_sets:
        final_video_ids = result_sets[0]
        for video_set in result_sets[1:]:
            final_video_ids &= video_set
        
        return [db.videos[vid] for vid in final_video_ids if vid in db.videos]
    
    return []
```

#### **ğŸ“Š æ¤œç´¢çµæœã®çµ±è¨ˆæƒ…å ±**

```python
def get_search_statistics(self) -> Dict[str, Any]:
    """æ¤œç´¢é–¢é€£ã®çµ±è¨ˆæƒ…å ±"""
    db = self.load_database()
    
    return {
        "total_creators": len(db.creator_index),
        "total_tags": len(db.tag_index), 
        "total_themes": len(db.theme_index),
        "top_creators": sorted(
            [(name, len(videos)) for name, videos in db.creator_index.items()],
            key=lambda x: x[1], reverse=True
        )[:10],
        "top_tags": sorted(
            [(tag, len(videos)) for tag, videos in db.tag_index.items()],
            key=lambda x: x[1], reverse=True
        )[:10],
        "index_efficiency": {
            "creator_index_size": sum(len(videos) for videos in db.creator_index.values()),
            "tag_index_size": sum(len(videos) for videos in db.tag_index.values()),
            "theme_index_size": sum(len(videos) for videos in db.theme_index.values())
        }
    }
```

---

## **ğŸ”„ è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ **

### **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥ã®è¨­è¨ˆ**

#### **ğŸ’¾ éšå±¤åŒ–ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ **

**ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°**

```python
def save_database(self, create_backup: bool = True) -> None:
    """ä¿å­˜æ™‚ã®è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    # 1. æ¯å›ã®ä¿å­˜æ™‚ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    if create_backup and self.db_file.exists():
        backup_file = self.backup_dir / f"unified_knowledge_db_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        shutil.copy2(self.db_file, backup_file)
```

**ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®å‘½åè¦å‰‡**

```
ãƒ‘ã‚¿ãƒ¼ãƒ³: unified_knowledge_db_YYYYMMDD_HHMMSS.json

ä¾‹:
unified_knowledge_db_20250625_103045.json  # 2025å¹´6æœˆ25æ—¥ 10:30:45
unified_knowledge_db_20250625_143021.json  # 2025å¹´6æœˆ25æ—¥ 14:30:21
unified_knowledge_db_20250625_173315.json  # 2025å¹´6æœˆ25æ—¥ 17:33:15
```

**é•·æœŸä¿å­˜æˆ¦ç•¥**

```python
def cleanup_old_backups(self, keep_days: int = 30) -> None:
    """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    cutoff_time = datetime.now().timestamp() - (keep_days * 24 * 60 * 60)
    
    deleted_count = 0
    for backup_file in self.backup_dir.glob("unified_knowledge_db_*.json"):
        if backup_file.stat().st_mtime < cutoff_time:
            backup_file.unlink()
            deleted_count += 1
            print(f"å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {backup_file}")
    
    print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {deleted_count}ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤")
```

#### **âš¡ å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å®Ÿè£…ä¾‹**

```python
def create_incremental_backup(self) -> Optional[Path]:
    """å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆå®Ÿè£…ä¾‹ï¼‰"""
    current_db = self.load_database()
    
    # æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    backup_files = sorted(self.backup_dir.glob("unified_knowledge_db_*.json"))
    if not backup_files:
        # åˆå›ã¯å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        return self.create_full_backup()
    
    latest_backup = backup_files[-1]
    
    try:
        # å‰å›ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’èª­ã¿è¾¼ã¿
        with open(latest_backup, 'r', encoding='utf-8') as f:
            previous_data = json.load(f)
        previous_db = KnowledgeDatabase.from_dict(previous_data)
        
        # å·®åˆ†ã‚’è¨ˆç®—
        changes = self._calculate_changes(previous_db, current_db)
        
        if not changes['has_changes']:
            print("å¤‰æ›´ãªã—ã€å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ã‚¹ã‚­ãƒƒãƒ—")
            return None
        
        # å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        increment_file = self.backup_dir / f"increment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(increment_file, 'w', encoding='utf-8') as f:
            json.dump(changes, f, ensure_ascii=False, indent=2)
        
        print(f"å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {increment_file}")
        return increment_file
        
    except Exception as e:
        print(f"å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return self.create_full_backup()  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

def _calculate_changes(self, old_db: KnowledgeDatabase, new_db: KnowledgeDatabase) -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–“ã®å·®åˆ†ã‚’è¨ˆç®—"""
    changes = {
        'timestamp': datetime.now().isoformat(),
        'has_changes': False,
        'added_videos': {},
        'modified_videos': {},
        'deleted_videos': [],
        'added_playlists': {},
        'modified_playlists': {},
        'deleted_playlists': []
    }
    
    # å‹•ç”»ã®å·®åˆ†
    old_video_ids = set(old_db.videos.keys())
    new_video_ids = set(new_db.videos.keys())
    
    # è¿½åŠ ã•ã‚ŒãŸå‹•ç”»
    for video_id in new_video_ids - old_video_ids:
        changes['added_videos'][video_id] = new_db.videos[video_id].to_dict()
        changes['has_changes'] = True
    
    # å‰Šé™¤ã•ã‚ŒãŸå‹•ç”»
    for video_id in old_video_ids - new_video_ids:
        changes['deleted_videos'].append(video_id)
        changes['has_changes'] = True
    
    # å¤‰æ›´ã•ã‚ŒãŸå‹•ç”»
    for video_id in old_video_ids & new_video_ids:
        old_video = old_db.videos[video_id]
        new_video = new_db.videos[video_id]
        if old_video.updated_at != new_video.updated_at:
            changes['modified_videos'][video_id] = new_video.to_dict()
            changes['has_changes'] = True
    
    return changes
```

### **ãƒ‡ãƒ¼ã‚¿å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ **

#### **ğŸ”§ è‡ªå‹•å¾©æ—§æ©Ÿèƒ½**

```python
def recover_from_backup(self, backup_file: Optional[Path] = None) -> bool:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©æ—§"""
    try:
        if backup_file is None:
            # æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’è‡ªå‹•é¸æŠ
            backup_files = sorted(self.backup_dir.glob("unified_knowledge_db_*.json"))
            if not backup_files:
                print("âŒ åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            backup_file = backup_files[-1]
        
        print(f"ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©æ—§é–‹å§‹: {backup_file}")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        with open(backup_file, 'r', encoding='utf-8') as f:
            backup_data = json.load(f)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦å¾©å…ƒå¯èƒ½ã‹ãƒ†ã‚¹ãƒˆ
        test_db = KnowledgeDatabase.from_dict(backup_data)
        print(f"   âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼æˆåŠŸ")
        print(f"   ğŸ“Š å¾©æ—§ãƒ‡ãƒ¼ã‚¿: {test_db.total_videos}å‹•ç”», {test_db.total_playlists}ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ")
        
        # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆç ´æãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼‰
        if self.db_file.exists():
            corrupted_backup = self.backup_dir / f"corrupted_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            shutil.move(str(self.db_file), str(corrupted_backup))
            print(f"   ğŸ“ ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {corrupted_backup}")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¾©å…ƒ
        shutil.copy2(backup_file, self.db_file)
        
        # ãƒ¡ãƒ¢ãƒªä¸Šã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
        self._database = test_db
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§å®Œäº†")
        return True
        
    except Exception as e:
        print(f"âŒ å¾©æ—§å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def verify_database_integrity(self) -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
    try:
        db = self.load_database()
        
        issues = []
        warnings = []
        
        # åŸºæœ¬æ§‹é€ ãƒã‚§ãƒƒã‚¯
        if not isinstance(db.videos, dict):
            issues.append("videos ãŒè¾æ›¸å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        if not isinstance(db.playlists, dict):
            issues.append("playlists ãŒè¾æ›¸å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        
        # å‚ç…§æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        for playlist_id, playlist in db.playlists.items():
            for video_id in playlist.video_ids:
                if video_id not in db.videos:
                    issues.append(f"ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ {playlist_id} ã«å­˜åœ¨ã—ãªã„å‹•ç”» {video_id} ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        for creator_name, video_ids in db.creator_index.items():
            for video_id in video_ids:
                if video_id not in db.videos:
                    warnings.append(f"creator_index ã«å­˜åœ¨ã—ãªã„å‹•ç”» {video_id} ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
        
        # çµ±è¨ˆæƒ…å ±ãƒã‚§ãƒƒã‚¯
        actual_video_count = len(db.videos)
        if db.total_videos != actual_video_count:
            warnings.append(f"total_videos ã®ä¸æ•´åˆ: è¨˜éŒ²å€¤={db.total_videos}, å®Ÿéš›={actual_video_count}")
        
        return {
            'is_valid': len(issues) == 0,
            'issues': issues,
            'warnings': warnings,
            'checked_at': datetime.now().isoformat(),
            'total_videos': len(db.videos),
            'total_playlists': len(db.playlists)
        }
        
    except Exception as e:
        return {
            'is_valid': False,
            'issues': [f"æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"],
            'warnings': [],
            'checked_at': datetime.now().isoformat()
        }
```

---

## **ğŸ“Š ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ‡ãƒ¼ã‚¿ç§»è¡Œæ©Ÿèƒ½**

### **æ®µéšçš„ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ **

#### **ğŸ”„ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•æ¤œå‡º**

```python
def _find_legacy_files(self) -> Dict[str, Path]:
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢"""
    legacy_files = {}
    
    # ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    playlist_files = list(self.data_dir.glob("playlists/playlist_*.json"))
    if playlist_files:
        legacy_files['playlists'] = playlist_files
        print(f"   ğŸ“‹ æ—¢å­˜ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {len(playlist_files)}ä»¶")
    
    # åˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«
    analysis_files = list(self.data_dir.glob("analyzed_*.json"))
    if analysis_files:
        legacy_files['analysis'] = analysis_files  
        print(f"   ğŸ¤– æ—¢å­˜åˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«: {len(analysis_files)}ä»¶")
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå¤ã„å½¢å¼ï¼‰
    old_config_files = list(self.data_dir.glob("config_*.json"))
    if old_config_files:
        legacy_files['configs'] = old_config_files
        print(f"   âš™ï¸ æ—¢å­˜è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {len(old_config_files)}ä»¶")
    
    return legacy_files
```

**ç§»è¡Œå‡¦ç†ã®å®Ÿè£…**

```python
def _migrate_legacy_data(self, legacy_files: Dict[str, Any]) -> KnowledgeDatabase:
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œ"""
    print("ğŸ”„ ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚’é–‹å§‹ã—ã¾ã™...")
    db = create_empty_database()
    migration_stats = {
        'migrated_videos': 0,
        'migrated_playlists': 0,
        'failed_files': [],
        'start_time': datetime.now()
    }
    
    # ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    if 'playlists' in legacy_files:
        for playlist_file in legacy_files['playlists']:
            try:
                print(f"   ğŸ“‹ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œä¸­: {playlist_file.name}")
                
                # æ—§å½¢å¼ã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                with open(playlist_file, 'r', encoding='utf-8') as f:
                    old_data = json.load(f)
                
                # æ–°å½¢å¼ã«å¤‰æ›
                converted_db = self._convert_legacy_playlist(old_data)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
                for video_id, video in converted_db.videos.items():
                    db.add_video(video)
                    migration_stats['migrated_videos'] += 1
                
                for playlist_id, playlist in converted_db.playlists.items():
                    db.add_playlist(playlist)
                    migration_stats['migrated_playlists'] += 1
                
                # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç§»å‹•
                legacy_target = self.legacy_dir / playlist_file.name
                shutil.move(str(playlist_file), str(legacy_target))
                print(f"   âœ… ç§»è¡Œå®Œäº† â†’ {legacy_target}")
                
            except Exception as e:
                print(f"   âŒ ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œã‚¨ãƒ©ãƒ¼ {playlist_file}: {e}")
                migration_stats['failed_files'].append(str(playlist_file))
    
    # åˆ†æçµæœã‚’çµ±åˆ
    if 'analysis' in legacy_files:
        for analysis_file in legacy_files['analysis']:
            try:
                print(f"   ğŸ¤– åˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œä¸­: {analysis_file.name}")
                
                with open(analysis_file, 'r', encoding='utf-8') as f:
                    analysis_data = json.load(f)
                
                # åˆ†æçµæœã‚’æ—¢å­˜å‹•ç”»ã«çµ±åˆ
                merged_count = self._merge_analysis_results(db, analysis_data)
                print(f"   âœ… {merged_count}ä»¶ã®åˆ†æçµæœã‚’çµ±åˆ")
                
                # åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç§»å‹•
                legacy_target = self.legacy_dir / analysis_file.name
                shutil.move(str(analysis_file), str(legacy_target))
                
            except Exception as e:
                print(f"   âŒ åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œã‚¨ãƒ©ãƒ¼ {analysis_file}: {e}")
                migration_stats['failed_files'].append(str(analysis_file))
    
    # ç§»è¡Œçµ±è¨ˆè¡¨ç¤º
    end_time = datetime.now()
    duration = end_time - migration_stats['start_time']
    
    print(f"\nğŸ“Š ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ‡ãƒ¼ã‚¿ç§»è¡Œå®Œäº†")
    print(f"   å‹•ç”»: {migration_stats['migrated_videos']}ä»¶")
    print(f"   ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆ: {migration_stats['migrated_playlists']}ä»¶")
    print(f"   å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«: {len(migration_stats['failed_files'])}ä»¶")
    print(f"   å‡¦ç†æ™‚é–“: {duration.total_seconds():.1f}ç§’")
    
    if migration_stats['failed_files']:
        print(f"   âš ï¸ å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«: {migration_stats['failed_files']}")
    
    return db

def _convert_legacy_playlist(self, old_data: Dict[str, Any]) -> KnowledgeDatabase:
    """æ—§ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆå½¢å¼ã‚’æ–°å½¢å¼ã«å¤‰æ›"""
    from core.data_models import Video, Playlist, VideoMetadata, PlaylistMetadata, ContentSource, AnalysisStatus
    
    db = create_empty_database()
    
    # æ—§å½¢å¼ã®æ§‹é€ ä¾‹:
    # {
    #   "playlist_info": {...},
    #   "videos": [...],
    #   "last_updated": "..."
    # }
    
    playlist_info = old_data.get('playlist_info', {})
    videos_data = old_data.get('videos', [])
    
    # ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
    playlist = Playlist(
        source=ContentSource.YOUTUBE,
        metadata=PlaylistMetadata(
            id=playlist_info.get('id', ''),
            title=playlist_info.get('title', ''),
            description=playlist_info.get('description', ''),
            channel_title=playlist_info.get('channel_title', ''),
            channel_id=playlist_info.get('channel_id', ''),
            published_at=datetime.fromisoformat(playlist_info.get('published_at', datetime.now().isoformat())),
            item_count=len(videos_data),
            collected_at=datetime.now()
        ),
        video_ids=[],
        last_full_sync=datetime.now(),
        last_incremental_sync=None,
        sync_settings={},
        total_videos=len(videos_data),
        analyzed_videos=0,
        analysis_success_rate=0.0,
        created_at=datetime.now(),
        updated_at=datetime.now()
    )
    
    # å‹•ç”»ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
    for video_data in videos_data:
        try:
            video = Video(
                source=ContentSource.YOUTUBE,
                metadata=VideoMetadata.from_youtube_api(video_data),
                playlists=[playlist.metadata.id],
                playlist_positions={playlist.metadata.id: video_data.get('position', 0)},
                analysis_status=AnalysisStatus.PENDING,
                creative_insight=None,
                analysis_error=None,
                created_at=datetime.now(),
                updated_at=datetime.now()
            )
            
            db.add_video(video)
            playlist.video_ids.append(video.metadata.id)
            
        except Exception as e:
            print(f"   âš ï¸ å‹•ç”»å¤‰æ›ã‚¨ãƒ©ãƒ¼ {video_data.get('id', 'unknown')}: {e}")
    
    db.add_playlist(playlist)
    return db
```

### **ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ»çµ±è¨ˆæ©Ÿèƒ½**

#### **ğŸ“ˆ è©³ç´°çµ±è¨ˆæƒ…å ±ã®æä¾›**

```python
def get_statistics(self) -> Dict[str, Any]:
    """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    db = self.load_database()
    
    analyzed_videos = sum(1 for v in db.videos.values() if v.creative_insight is not None)
    analysis_success_rate = analyzed_videos / len(db.videos) if db.videos else 0
    
    # ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆåˆ¥çµ±è¨ˆ
    playlist_stats = {}
    for pid, playlist in db.playlists.items():
        playlist_videos = [db.videos[vid] for vid in playlist.video_ids if vid in db.videos]
        analyzed_in_playlist = sum(1 for v in playlist_videos if v.creative_insight is not None)
        
        playlist_stats[pid] = {
            'title': playlist.metadata.title,
            'total_videos': len(playlist_videos),
            'analyzed_videos': analyzed_in_playlist,
            'analysis_rate': analyzed_in_playlist / len(playlist_videos) if playlist_videos else 0,
            'last_sync': playlist.last_full_sync.isoformat(),
            'channel_title': playlist.metadata.channel_title
        }
    
    # æ™‚ç³»åˆ—çµ±è¨ˆï¼ˆåˆ†æã®é€²æ—ï¼‰
    analysis_timeline = self._calculate_analysis_timeline(db)
    
    return {
        'database_overview': {
            'total_videos': db.total_videos,
            'total_playlists': db.total_playlists,
            'analyzed_videos': analyzed_videos,
            'analysis_success_rate': analysis_success_rate,
            'last_updated': db.last_updated.isoformat(),
            'database_version': db.database_version
        },
        'content_statistics': {
            'total_creators': len(db.creator_index),
            'total_tags': len(db.tag_index),
            'total_themes': len(db.theme_index),
            'top_creators': sorted(
                [(name, len(videos)) for name, videos in db.creator_index.items()],
                key=lambda x: x[1], reverse=True
            )[:10],
            'popular_tags': sorted(
                [(tag, len(videos)) for tag, videos in db.tag_index.items()],
                key=lambda x: x[1], reverse=True
            )[:10]
        },
        'playlists': playlist_stats,
        'analysis_timeline': analysis_timeline,
        'storage_info': {
            'database_file_size': self.db_file.stat().st_size if self.db_file.exists() else 0,
            'backup_count': len(list(self.backup_dir.glob("unified_knowledge_db_*.json"))),
            'legacy_file_count': len(list(self.legacy_dir.glob("*")))
        }
    }

def _calculate_analysis_timeline(self, db: KnowledgeDatabase) -> Dict[str, Any]:
    """åˆ†æé€²æ—ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä½œæˆ"""
    timeline = {}
    
    for video in db.videos.values():
        if video.creative_insight and video.creative_insight.analysis_timestamp:
            date_key = video.creative_insight.analysis_timestamp.strftime('%Y-%m-%d')
            if date_key not in timeline:
                timeline[date_key] = 0
            timeline[date_key] += 1
    
    return {
        'daily_analysis_count': timeline,
        'peak_analysis_day': max(timeline.items(), key=lambda x: x[1]) if timeline else None,
        'analysis_span_days': len(timeline)
    }
```

ã“ã®ç« ã§ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å®Ÿè·µçš„ãªå®Ÿè£…ã‚’é€šã˜ã¦ã€ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã€æ¤œç´¢æœ€é©åŒ–ã®æŠ€è¡“ã‚’ä½“ç³»çš„ã«å­¦ã³ã¾ã—ãŸã€‚æ¬¡ç« ã§ã¯ã€ã“ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚·ã‚¹ãƒ†ãƒ ã¨é€£æºã™ã‚‹è¨­å®šç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚