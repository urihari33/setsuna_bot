#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã›ã¤ãªè¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
PersonalityMemoryã¨CollaborationMemoryã®æ¨ªæ–­çš„çµ±åˆãƒ»é–¢é€£ä»˜ã‘ç®¡ç†
"""

import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import hashlib
import re

class MemoryIntegrationSystem:
    """ã›ã¤ãªã®è¨˜æ†¶çµ±åˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, personality_memory=None, collaboration_memory=None, memory_mode="normal"):
        """
        è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        
        Args:
            personality_memory: PersonalityMemoryã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            collaboration_memory: CollaborationMemoryã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            memory_mode: "normal" ã¾ãŸã¯ "test"
        """
        self.personality_memory = personality_memory
        self.collaboration_memory = collaboration_memory
        self.memory_mode = memory_mode
        
        # ç’°å¢ƒã«å¿œã˜ã¦ãƒ‘ã‚¹ã‚’æ±ºå®š
        if os.path.exists("/mnt/d/setsuna_bot"):
            base_path = Path("/mnt/d/setsuna_bot")
        else:
            base_path = Path("D:/setsuna_bot")
        
        if memory_mode == "test":
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«
            import tempfile
            self.integration_file = base_path / "temp" / f"test_memory_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.makedirs(self.integration_file.parent, exist_ok=True)
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: æ°¸ç¶šãƒ•ã‚¡ã‚¤ãƒ«
            self.integration_file = base_path / "enhanced_memory" / "memory_integration.json"
            os.makedirs(self.integration_file.parent, exist_ok=True)
        
        # çµ±åˆè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        self.integration_data = {
            "memory_relationships": [],    # è¨˜æ†¶é–“é–¢ä¿‚æ€§
            "memory_clusters": {},         # è¨˜æ†¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
            "relationship_types": {        # é–¢ä¿‚æ€§ã‚¿ã‚¤ãƒ—å®šç¾©
                "causal": "å› æœé–¢ä¿‚",
                "temporal": "æ™‚ç³»åˆ—é–¢ä¿‚", 
                "thematic": "ãƒ†ãƒ¼ãƒé–¢ä¿‚",
                "reinforcing": "ç›¸äº’å¼·åŒ–",
                "contradictory": "çŸ›ç›¾é–¢ä¿‚"
            },
            "integration_patterns": [],    # çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³
            "adaptive_insights": {},       # é©å¿œçš„æ´å¯Ÿ
            "cross_references": {},        # ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹
            "created_at": datetime.now().isoformat(),
            "version": "1.0"
        }
        
        # é–¢é€£ä»˜ã‘ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¨­å®š
        self.relationship_config = {
            "temporal_threshold_hours": 24,     # æ™‚ç³»åˆ—é–¢é€£ã®é–¾å€¤ï¼ˆæ™‚é–“ï¼‰
            "theme_similarity_threshold": 0.3,  # ãƒ†ãƒ¼ãƒé¡ä¼¼åº¦é–¾å€¤
            "causal_confidence_threshold": 0.6, # å› æœé–¢ä¿‚ä¿¡é ¼åº¦é–¾å€¤
            "keyword_weight": 0.4,              # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡ã¿
            "emotion_weight": 0.3,              # æ„Ÿæƒ…é‡ã¿
            "temporal_weight": 0.3              # æ™‚ç³»åˆ—é‡ã¿
        }
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self._load_integration_data()
        
        print(f"ğŸ”— è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº† ({memory_mode}ãƒ¢ãƒ¼ãƒ‰)")
    
    def _load_integration_data(self):
        """çµ±åˆè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if self.integration_file.exists() and self.memory_mode == "normal":
            try:
                with open(self.integration_file, 'r', encoding='utf-8') as f:
                    loaded_data = json.load(f)
                    self.integration_data.update(loaded_data)
                
                relationships_count = len(self.integration_data.get("memory_relationships", []))
                clusters_count = len(self.integration_data.get("memory_clusters", {}))
                print(f"âœ… è¨˜æ†¶çµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: é–¢ä¿‚æ€§{relationships_count}ä»¶, ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼{clusters_count}ä»¶")
                
            except Exception as e:
                print(f"âš ï¸ è¨˜æ†¶çµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            print("ğŸ†• æ–°è¦è¨˜æ†¶çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ")
    
    def save_integration_data(self):
        """çµ±åˆè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if self.memory_mode == "test":
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã¯ä¿å­˜ã—ãªã„
            return
        
        try:
            with open(self.integration_file, 'w', encoding='utf-8') as f:
                json.dump(self.integration_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ è¨˜æ†¶çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
        except Exception as e:
            print(f"âŒ è¨˜æ†¶çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def analyze_memory_relationships(self) -> Dict[str, int]:
        """
        è¨˜æ†¶é–“ã®é–¢ä¿‚æ€§ã‚’åˆ†æãƒ»æ›´æ–°
        
        Returns:
            Dict[str, int]: åˆ†æçµæœçµ±è¨ˆ
        """
        try:
            print("ğŸ” è¨˜æ†¶é–¢ä¿‚æ€§åˆ†æé–‹å§‹...")
            
            # æ—¢å­˜ã®é–¢ä¿‚æ€§ã‚’ã‚¯ãƒªã‚¢ï¼ˆå†åˆ†æï¼‰
            old_count = len(self.integration_data["memory_relationships"])
            self.integration_data["memory_relationships"] = []
            
            analysis_stats = {
                "temporal_relationships": 0,
                "thematic_relationships": 0,
                "causal_relationships": 0,
                "total_relationships": 0
            }
            
            if not self.personality_memory or not self.collaboration_memory:
                print("âš ï¸ è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return analysis_stats
            
            # å€‹äººè¨˜æ†¶ã¨å”åƒè¨˜æ†¶ã®å–å¾—
            personality_experiences = self.personality_memory.personality_data.get("personal_experiences", [])
            collaboration_successes = self.collaboration_memory.collaboration_data.get("success_patterns", [])
            collaboration_patterns = self.collaboration_memory.collaboration_data.get("work_patterns", [])
            
            # 1. æ™‚ç³»åˆ—é–¢ä¿‚åˆ†æ
            analysis_stats["temporal_relationships"] = self._analyze_temporal_relationships(
                personality_experiences, collaboration_successes, collaboration_patterns
            )
            
            # 2. ãƒ†ãƒ¼ãƒé–¢ä¿‚åˆ†æ  
            analysis_stats["thematic_relationships"] = self._analyze_thematic_relationships(
                personality_experiences, collaboration_successes
            )
            
            # 3. å› æœé–¢ä¿‚åˆ†æ
            analysis_stats["causal_relationships"] = self._analyze_causal_relationships(
                personality_experiences, collaboration_successes
            )
            
            analysis_stats["total_relationships"] = len(self.integration_data["memory_relationships"])
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ›´æ–°
            self._update_memory_clusters()
            
            print(f"âœ… è¨˜æ†¶é–¢ä¿‚æ€§åˆ†æå®Œäº†: {old_count}ä»¶ -> {analysis_stats['total_relationships']}ä»¶")
            print(f"   - æ™‚ç³»åˆ—é–¢ä¿‚: {analysis_stats['temporal_relationships']}ä»¶")
            print(f"   - ãƒ†ãƒ¼ãƒé–¢ä¿‚: {analysis_stats['thematic_relationships']}ä»¶")
            print(f"   - å› æœé–¢ä¿‚: {analysis_stats['causal_relationships']}ä»¶")
            
            return analysis_stats
            
        except Exception as e:
            print(f"âŒ è¨˜æ†¶é–¢ä¿‚æ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _analyze_temporal_relationships(self, experiences: List[Dict], 
                                      successes: List[Dict], patterns: List[Dict]) -> int:
        """æ™‚ç³»åˆ—é–¢ä¿‚ã‚’åˆ†æ"""
        temporal_count = 0
        threshold_hours = self.relationship_config["temporal_threshold_hours"]
        
        # å€‹äººä½“é¨“ã¨å”åƒæˆåŠŸã®æ™‚ç³»åˆ—é–¢ä¿‚
        for exp in experiences:
            exp_time = datetime.fromisoformat(exp["date"])
            
            for success in successes:
                success_time = datetime.fromisoformat(success["date"])
                time_diff = abs((exp_time - success_time).total_seconds() / 3600)
                
                if time_diff <= threshold_hours:
                    relationship = self._create_relationship(
                        source={"type": "personality", "id": exp["id"]},
                        target={"type": "collaboration_success", "id": success["id"]},
                        relationship_type="temporal",
                        strength=max(0.1, 1.0 - (time_diff / threshold_hours)),
                        context=f"{time_diff:.1f}æ™‚é–“ä»¥å†…ã®è¿‘æ¥ä½“é¨“",
                        metadata={
                            "time_diff_hours": time_diff,
                            "exp_emotion": exp.get("emotion", "unknown"),
                            "success_type": success.get("success_type", "unknown")
                        }
                    )
                    self.integration_data["memory_relationships"].append(relationship)
                    temporal_count += 1
        
        return temporal_count
    
    def _analyze_thematic_relationships(self, experiences: List[Dict], successes: List[Dict]) -> int:
        """ãƒ†ãƒ¼ãƒé–¢ä¿‚ã‚’åˆ†æ"""
        thematic_count = 0
        threshold = self.relationship_config["theme_similarity_threshold"]
        
        for exp in experiences:
            for success in successes:
                similarity_score = self._calculate_thematic_similarity(exp, success)
                
                if similarity_score >= threshold:
                    relationship = self._create_relationship(
                        source={"type": "personality", "id": exp["id"]},
                        target={"type": "collaboration_success", "id": success["id"]},
                        relationship_type="thematic",
                        strength=similarity_score,
                        context=f"ãƒ†ãƒ¼ãƒé¡ä¼¼åº¦: {similarity_score:.2f}",
                        metadata={
                            "common_themes": self._extract_common_themes(exp, success),
                            "exp_type": exp.get("type", "unknown"),
                            "success_type": success.get("success_type", "unknown")
                        }
                    )
                    self.integration_data["memory_relationships"].append(relationship)
                    thematic_count += 1
        
        return thematic_count
    
    def _analyze_causal_relationships(self, experiences: List[Dict], successes: List[Dict]) -> int:
        """å› æœé–¢ä¿‚ã‚’åˆ†æ"""
        causal_count = 0
        threshold = self.relationship_config["causal_confidence_threshold"]
        
        for exp in experiences:
            for success in successes:
                causal_confidence = self._estimate_causal_relationship(exp, success)
                
                if causal_confidence >= threshold:
                    relationship = self._create_relationship(
                        source={"type": "personality", "id": exp["id"]},
                        target={"type": "collaboration_success", "id": success["id"]},
                        relationship_type="causal",
                        strength=causal_confidence,
                        context=f"å› æœé–¢ä¿‚æ¨å®šä¿¡é ¼åº¦: {causal_confidence:.2f}",
                        metadata={
                            "causal_factors": self._identify_causal_factors(exp, success),
                            "learning_connection": exp.get("learning", ""),
                            "success_factors": success.get("key_factors", [])
                        }
                    )
                    self.integration_data["memory_relationships"].append(relationship)
                    causal_count += 1
        
        return causal_count
    
    def _calculate_thematic_similarity(self, exp: Dict, success: Dict) -> float:
        """ãƒ†ãƒ¼ãƒé¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºãƒ»æ¯”è¼ƒ
        exp_text = f"{exp.get('description', '')} {exp.get('learning', '')}"
        success_text = f"{success.get('context', '')} {success.get('outcome', '')}"
        
        exp_keywords = self._extract_keywords(exp_text)
        success_keywords = self._extract_keywords(success_text)
        
        if not exp_keywords or not success_keywords:
            return 0.0
        
        # å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç‡
        common_keywords = set(exp_keywords) & set(success_keywords)
        keyword_similarity = len(common_keywords) / max(len(exp_keywords), len(success_keywords))
        
        # æ„Ÿæƒ…ä¸€è‡´åº¦
        emotion_similarity = 0.0
        if exp.get("emotion") and success.get("success_type"):
            emotion_map = {
                "excited": ["creative_breakthrough", "project_completion"],
                "proud": ["creative_breakthrough", "problem_solving"],
                "curious": ["learning", "efficient_workflow"],
                "determined": ["problem_solving", "project_completion"]
            }
            exp_emotion = exp.get("emotion")
            if exp_emotion in emotion_map and success.get("success_type") in emotion_map[exp_emotion]:
                emotion_similarity = 1.0
        
        # é‡ã¿ä»˜ãç·åˆé¡ä¼¼åº¦
        total_similarity = (
            keyword_similarity * self.relationship_config["keyword_weight"] +
            emotion_similarity * self.relationship_config["emotion_weight"]
        )
        
        return min(1.0, total_similarity)
    
    def _estimate_causal_relationship(self, exp: Dict, success: Dict) -> float:
        """å› æœé–¢ä¿‚ã®ä¿¡é ¼åº¦ã‚’æ¨å®š"""
        confidence_factors = []
        
        # æ™‚ç³»åˆ—é †åºï¼ˆä½“é¨“ãŒæˆåŠŸã‚ˆã‚Šå‰ï¼‰
        try:
            exp_time = datetime.fromisoformat(exp["date"])
            success_time = datetime.fromisoformat(success["date"])
            
            if exp_time < success_time:
                time_diff_days = (success_time - exp_time).days
                if time_diff_days <= 30:  # 30æ—¥ä»¥å†…
                    temporal_factor = max(0.1, 1.0 - (time_diff_days / 30))
                    confidence_factors.append(temporal_factor)
        except:
            pass
        
        # å­¦ç¿’å†…å®¹ã¨æˆåŠŸè¦å› ã®é–¢é€£æ€§
        exp_learning = exp.get("learning", "").lower()
        success_factors = [f.lower() for f in success.get("key_factors", [])]
        
        learning_factor = 0.0
        for factor in success_factors:
            if any(word in factor for word in exp_learning.split()):
                learning_factor = 0.8
                break
        
        if learning_factor > 0:
            confidence_factors.append(learning_factor)
        
        # ã‚¿ã‚¤ãƒ—é–¢é€£æ€§
        type_factor = 0.0
        exp_type = exp.get("type", "")
        success_type = success.get("success_type", "")
        
        type_correlations = {
            "learning": ["problem_solving", "efficient_workflow"],
            "creation": ["creative_breakthrough", "project_completion"],
            "challenge": ["problem_solving", "project_completion"]
        }
        
        if exp_type in type_correlations and success_type in type_correlations[exp_type]:
            type_factor = 0.7
            confidence_factors.append(type_factor)
        
        # ç·åˆä¿¡é ¼åº¦
        if not confidence_factors:
            return 0.0
        
        return min(1.0, sum(confidence_factors) / len(confidence_factors))
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        if not text:
            return []
        
        # åŸºæœ¬çš„ãªå½¢æ…‹ç´ è§£æã®ä»£æ›¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
        keywords = []
        
        # æ—¥æœ¬èªã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        important_words = [
            "éŸ³æ¥½", "å‰µä½œ", "æ­Œ", "å‹•ç”»", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ", "å­¦ç¿’", "æŠ€è¡“", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°",
            "ã‚¢ã‚¤ãƒ‡ã‚¢", "åˆ¶ä½œ", "ä½œæ¥­", "åŠ¹ç‡", "æ”¹å–„", "æˆåŠŸ", "å®Œæˆ", "æŒ‘æˆ¦", "çµŒé¨“",
            "é›†ä¸­", "ç†è§£", "ç¿’å¾—", "é–‹ç™º", "è¨­è¨ˆ", "ä¼ç”»", "è¨ˆç”»", "å®Ÿè£…", "ãƒ†ã‚¹ãƒˆ"
        ]
        
        for word in important_words:
            if word in text:
                keywords.append(word)
        
        # è‹±å˜èªã®æŠ½å‡º
        english_words = re.findall(r'[a-zA-Z]+', text)
        keywords.extend([w.lower() for w in english_words if len(w) >= 3])
        
        return list(set(keywords))
    
    def _extract_common_themes(self, exp: Dict, success: Dict) -> List[str]:
        """å…±é€šãƒ†ãƒ¼ãƒã‚’æŠ½å‡º"""
        exp_text = f"{exp.get('description', '')} {exp.get('learning', '')}"
        success_text = f"{success.get('context', '')} {success.get('outcome', '')}"
        
        exp_keywords = set(self._extract_keywords(exp_text))
        success_keywords = set(self._extract_keywords(success_text))
        
        return list(exp_keywords & success_keywords)
    
    def _identify_causal_factors(self, exp: Dict, success: Dict) -> List[str]:
        """å› æœè¦å› ã‚’ç‰¹å®š"""
        factors = []
        
        # å­¦ç¿’å†…å®¹ã®å½±éŸ¿
        if exp.get("learning"):
            factors.append(f"å­¦ç¿’: {exp['learning'][:30]}...")
        
        # æ„Ÿæƒ…çŠ¶æ…‹ã®å½±éŸ¿
        if exp.get("emotion"):
            factors.append(f"æ„Ÿæƒ…: {exp['emotion']}")
        
        # ä½“é¨“ã‚¿ã‚¤ãƒ—ã®å½±éŸ¿
        if exp.get("type"):
            factors.append(f"ä½“é¨“ã‚¿ã‚¤ãƒ—: {exp['type']}")
        
        return factors
    
    def _create_relationship(self, source: Dict, target: Dict, relationship_type: str,
                           strength: float, context: str, metadata: Dict = None) -> Dict:
        """é–¢ä¿‚æ€§ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ"""
        relationship_id = self._generate_relationship_id(source, target, relationship_type)
        
        return {
            "relationship_id": relationship_id,
            "source_memory": source,
            "target_memory": target,
            "relationship_type": relationship_type,
            "strength": round(strength, 3),
            "context": context,
            "metadata": metadata or {},
            "created_at": datetime.now().isoformat(),
            "last_referenced": None,
            "reference_count": 0
        }
    
    def _generate_relationship_id(self, source: Dict, target: Dict, rel_type: str) -> str:
        """é–¢ä¿‚æ€§IDã‚’ç”Ÿæˆ"""
        content = f"{source['type']}_{source['id']}_{target['type']}_{target['id']}_{rel_type}"
        hash_value = hashlib.md5(content.encode()).hexdigest()[:8]
        return f"rel_{hash_value}"
    
    def _update_memory_clusters(self):
        """è¨˜æ†¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’æ›´æ–°"""
        clusters = {}
        
        # é–¢ä¿‚æ€§ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ç”Ÿæˆ
        for rel in self.integration_data["memory_relationships"]:
            # å¼·ã„é–¢ä¿‚æ€§ï¼ˆ0.6ä»¥ä¸Šï¼‰ã®ã¿ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–
            if rel["strength"] >= 0.6:
                cluster_theme = self._determine_cluster_theme(rel)
                
                if cluster_theme not in clusters:
                    clusters[cluster_theme] = {
                        "personality_memories": [],
                        "collaboration_memories": [],
                        "theme": cluster_theme,
                        "total_relationships": 0,
                        "average_strength": 0.0
                    }
                
                # ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«è¿½åŠ 
                if rel["source_memory"]["type"] == "personality":
                    if rel["source_memory"]["id"] not in clusters[cluster_theme]["personality_memories"]:
                        clusters[cluster_theme]["personality_memories"].append(rel["source_memory"]["id"])
                
                if rel["target_memory"]["type"].startswith("collaboration"):
                    if rel["target_memory"]["id"] not in clusters[cluster_theme]["collaboration_memories"]:
                        clusters[cluster_theme]["collaboration_memories"].append(rel["target_memory"]["id"])
                
                clusters[cluster_theme]["total_relationships"] += 1
        
        # å¹³å‡å¼·åº¦ã‚’è¨ˆç®—
        for cluster_name, cluster_data in clusters.items():
            related_rels = [rel for rel in self.integration_data["memory_relationships"] 
                          if self._determine_cluster_theme(rel) == cluster_name]
            if related_rels:
                cluster_data["average_strength"] = sum(rel["strength"] for rel in related_rels) / len(related_rels)
        
        self.integration_data["memory_clusters"] = clusters
        print(f"ğŸ”— è¨˜æ†¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ›´æ–°: {len(clusters)}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç”Ÿæˆ")
    
    def _determine_cluster_theme(self, relationship: Dict) -> str:
        """é–¢ä¿‚æ€§ã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒã‚’æ±ºå®š"""
        metadata = relationship.get("metadata", {})
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…±é€šãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
        if "common_themes" in metadata and metadata["common_themes"]:
            primary_theme = metadata["common_themes"][0]
            return f"{primary_theme}_cluster"
        
        # ä½“é¨“ã‚¿ã‚¤ãƒ—ã¨æˆåŠŸã‚¿ã‚¤ãƒ—ã‹ã‚‰æ¨å®š
        exp_type = metadata.get("exp_type", "")
        success_type = metadata.get("success_type", "")
        
        if exp_type == "creation" or "creative" in success_type:
            return "creative_growth"
        elif exp_type == "learning" or "problem" in success_type:
            return "learning_development"
        elif exp_type == "challenge":
            return "challenge_mastery"
        else:
            return "general_development"
    
    def find_related_memories(self, memory_id: str, memory_type: str, 
                            max_results: int = 5) -> List[Dict]:
        """
        æŒ‡å®šè¨˜æ†¶ã«é–¢é€£ã™ã‚‹è¨˜æ†¶ã‚’æ¤œç´¢
        
        Args:
            memory_id: æ¤œç´¢åŸºæº–ã¨ãªã‚‹è¨˜æ†¶ID
            memory_type: è¨˜æ†¶ã‚¿ã‚¤ãƒ— ("personality" or "collaboration")
            max_results: æœ€å¤§çµæœæ•°
            
        Returns:
            List[Dict]: é–¢é€£è¨˜æ†¶ãƒªã‚¹ãƒˆï¼ˆé–¢é€£åº¦é †ï¼‰
        """
        related_memories = []
        
        for rel in self.integration_data["memory_relationships"]:
            source = rel["source_memory"]
            target = rel["target_memory"]
            
            # æŒ‡å®šè¨˜æ†¶ãŒé–¢ä¿‚æ€§ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if (source["type"] == memory_type and source["id"] == memory_id):
                related_memories.append({
                    "memory": target,
                    "relationship": rel,
                    "relevance_score": rel["strength"]
                })
            elif (target["type"] == memory_type and target["id"] == memory_id):
                related_memories.append({
                    "memory": source,
                    "relationship": rel,
                    "relevance_score": rel["strength"]
                })
        
        # é–¢é€£åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        related_memories.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return related_memories[:max_results]
    
    def generate_integrated_context(self, user_input: str = "", context_type: str = "full") -> str:
        """
        çµ±åˆè¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼ˆé–¢é€£è¨˜æ†¶æ¤œç´¢ç”¨ï¼‰
            context_type: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚¤ãƒ— ("full", "recent", "relevant")
            
        Returns:
            str: çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            context_parts = []
            
            if context_type == "full":
                # å®Œå…¨ãªçµ±åˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                context_parts.extend(self._generate_relationship_summary())
                context_parts.extend(self._generate_cluster_insights())
                context_parts.extend(self._generate_adaptive_insights())
                
            elif context_type == "recent":
                # æœ€è¿‘ã®çµ±åˆè¨˜æ†¶ã®ã¿
                context_parts.extend(self._generate_recent_relationships())
                
            elif context_type == "relevant":
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«é–¢é€£ã™ã‚‹è¨˜æ†¶ã®ã¿
                context_parts.extend(self._generate_relevant_context(user_input))
            
            return "\n".join(context_parts) if context_parts else ""
            
        except Exception as e:
            print(f"âŒ çµ±åˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def _generate_relationship_summary(self) -> List[str]:
        """é–¢ä¿‚æ€§ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        summary_parts = []
        relationships = self.integration_data["memory_relationships"]
        
        if not relationships:
            return summary_parts
        
        # å¼·ã„é–¢ä¿‚æ€§ã®ã¿è¡¨ç¤º
        strong_rels = [r for r in relationships if r["strength"] >= 0.7]
        
        if strong_rels:
            summary_parts.append("ã€è¨˜æ†¶é–“ã®é‡è¦ãªé–¢é€£æ€§ã€‘")
            
            for rel in strong_rels[:3]:  # ä¸Šä½3ä»¶
                rel_type_name = self.integration_data["relationship_types"].get(
                    rel["relationship_type"], rel["relationship_type"]
                )
                summary_parts.append(
                    f"- {rel_type_name}: {rel['context']} (é–¢é€£åº¦: {rel['strength']:.2f})"
                )
        
        return summary_parts
    
    def _generate_cluster_insights(self) -> List[str]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ´å¯Ÿã‚’ç”Ÿæˆ"""
        insights = []
        clusters = self.integration_data["memory_clusters"]
        
        if not clusters:
            return insights
        
        insights.append("\nã€çµ±åˆè¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘")
        
        # æœ€ã‚‚æ´»ç™ºãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆè¨˜æ†¶æ•°ãŒå¤šã„ï¼‰ã‚’è¡¨ç¤º
        sorted_clusters = sorted(
            clusters.items(), 
            key=lambda x: len(x[1]["personality_memories"]) + len(x[1]["collaboration_memories"]),
            reverse=True
        )
        
        for cluster_name, cluster_data in sorted_clusters[:2]:  # ä¸Šä½2ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
            total_memories = len(cluster_data["personality_memories"]) + len(cluster_data["collaboration_memories"])
            avg_strength = cluster_data.get("average_strength", 0)
            
            insights.append(
                f"- {cluster_data['theme']}: {total_memories}ä»¶ã®é–¢é€£è¨˜æ†¶ (é–¢é€£åº¦: {avg_strength:.2f})"
            )
        
        return insights
    
    def _generate_adaptive_insights(self) -> List[str]:
        """é©å¿œçš„æ´å¯Ÿã‚’ç”Ÿæˆ"""
        insights = []
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ã®æ´å¯Ÿ
        performance_insight = self._analyze_performance_patterns()
        if performance_insight:
            insights.append("\nã€å­¦ç¿’ãƒ»æˆé•·ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘")
            insights.append(f"- {performance_insight}")
        
        # å”åƒåŠ¹æœã®æ´å¯Ÿ
        collaboration_insight = self._analyze_collaboration_effectiveness()
        if collaboration_insight:
            if not insights:
                insights.append("\nã€å”åƒåŠ¹æœåˆ†æã€‘")
            else:
                insights.append("- " + collaboration_insight)
        
        return insights
    
    def _generate_recent_relationships(self) -> List[str]:
        """æœ€è¿‘ã®é–¢ä¿‚æ€§ã‚’ç”Ÿæˆ"""
        recent_parts = []
        relationships = self.integration_data["memory_relationships"]
        
        # éå»7æ—¥ä»¥å†…ã®é–¢ä¿‚æ€§
        cutoff_date = datetime.now() - timedelta(days=7)
        recent_rels = [
            rel for rel in relationships 
            if datetime.fromisoformat(rel["created_at"]) >= cutoff_date
        ]
        
        if recent_rels:
            recent_parts.append("ã€æœ€è¿‘ç™ºè¦‹ã•ã‚ŒãŸè¨˜æ†¶é–¢é€£æ€§ã€‘")
            
            for rel in recent_rels[-3:]:  # æœ€æ–°3ä»¶
                recent_parts.append(f"- {rel['context']}")
        
        return recent_parts
    
    def _generate_relevant_context(self, user_input: str) -> List[str]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        relevant_parts = []
        
        if not user_input:
            return relevant_parts
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        input_keywords = self._extract_keywords(user_input)
        
        if not input_keywords:
            return relevant_parts
        
        # é–¢é€£ã™ã‚‹é–¢ä¿‚æ€§ã‚’æ¤œç´¢
        relevant_relationships = []
        
        for rel in self.integration_data["memory_relationships"]:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é–¢é€£æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            metadata = rel.get("metadata", {})
            
            # å…±é€šãƒ†ãƒ¼ãƒã¨ã®ä¸€è‡´
            common_themes = metadata.get("common_themes", [])
            if any(theme in input_keywords for theme in common_themes):
                relevant_relationships.append(rel)
                continue
            
            # é–¢ä¿‚æ€§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã®ä¸€è‡´
            context_keywords = self._extract_keywords(rel.get("context", ""))
            if set(input_keywords) & set(context_keywords):
                relevant_relationships.append(rel)
        
        if relevant_relationships:
            relevant_parts.append("ã€é–¢é€£ã™ã‚‹éå»ã®ä½“é¨“ãƒ»æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘")
            
            # é–¢é€£åº¦ã§ã‚½ãƒ¼ãƒˆ
            relevant_relationships.sort(key=lambda x: x["strength"], reverse=True)
            
            for rel in relevant_relationships[:3]:  # ä¸Šä½3ä»¶
                relevant_parts.append(f"- {rel['context']} (é–¢é€£åº¦: {rel['strength']:.2f})")
        
        return relevant_parts
    
    def _analyze_performance_patterns(self) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        if not self.personality_memory or not self.collaboration_memory:
            return ""
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é€²åŒ–ã¨å”åƒæˆåŠŸã®é–¢é€£æ€§ã‚’åˆ†æ
        evolution = self.personality_memory.personality_data.get("character_evolution", {})
        partnership = self.collaboration_memory.collaboration_data.get("partnership_evolution", {})
        
        confidence = evolution.get("confidence_level", 0.5)
        creative_exp = evolution.get("creative_experience", 0.7)
        creative_compat = partnership.get("creative_compatibility", 0.6)
        
        if confidence >= 0.7 and creative_compat >= 0.7:
            return "è‡ªä¿¡ã¨å‰µä½œé©åˆæ€§ã®å‘ä¸ŠãŒç›¸äº’ã«å¼·åŒ–ã—ã¦ã„ã‚‹"
        elif creative_exp >= 0.8 and creative_compat >= 0.6:
            return "å‰µä½œçµŒé¨“ã®è“„ç©ãŒå”åƒå“è³ªå‘ä¸Šã«å¯„ä¸ã—ã¦ã„ã‚‹"
        elif confidence < 0.4 and creative_compat < 0.5:
            return "è‡ªä¿¡å‘ä¸ŠãŒå”åƒåŠ¹ç‡æ”¹å–„ã®éµã¨ãªã‚‹å¯èƒ½æ€§"
        
        return ""
    
    def _analyze_collaboration_effectiveness(self) -> str:
        """å”åƒåŠ¹æœã‚’åˆ†æ"""
        if not self.collaboration_memory:
            return ""
        
        # ä½œæ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®é–¢é€£æ€§
        work_patterns = self.collaboration_memory.collaboration_data.get("work_patterns", [])
        success_patterns = self.collaboration_memory.collaboration_data.get("success_patterns", [])
        
        if not work_patterns or not success_patterns:
            return ""
        
        # é«˜åŠ¹ç‡ä½œæ¥­ã¨æˆåŠŸã®ç›¸é–¢
        high_efficiency_works = [w for w in work_patterns if w.get("efficiency_score", 0) >= 0.7]
        high_impact_successes = [s for s in success_patterns if s.get("impact_rating", 0) >= 0.7]
        
        if len(high_efficiency_works) >= 2 and len(high_impact_successes) >= 1:
            return "é«˜åŠ¹ç‡ä½œæ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæˆåŠŸã«ã¤ãªãŒã‚‹å‚¾å‘"
        
        return ""
    
    def suggest_adaptive_responses(self, user_input: str, current_context: Dict = None) -> List[str]:
        """
        é©å¿œçš„å¿œç­”ã‚’ææ¡ˆ
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
            current_context: ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
            
        Returns:
            List[str]: å¿œç­”ææ¡ˆãƒªã‚¹ãƒˆ
        """
        suggestions = []
        
        try:
            # é–¢é€£ã™ã‚‹éå»ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ææ¡ˆ
            related_memories = self._find_memories_by_keywords(user_input)
            
            for memory in related_memories[:2]:  # ä¸Šä½2ä»¶
                if memory["memory"]["type"] == "collaboration_success":
                    success_data = self._get_success_data_by_id(memory["memory"]["id"])
                    if success_data:
                        suggestions.append(
                            f"éå»ã®{success_data.get('success_type', 'æˆåŠŸ')}ä½“é¨“ã‚’æ´»ã‹ã—ã¦: {success_data.get('outcome', '')[:40]}..."
                        )
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æƒ…å ±ã‹ã‚‰ææ¡ˆ
            cluster_suggestions = self._get_cluster_based_suggestions(user_input)
            suggestions.extend(cluster_suggestions)
            
            return suggestions[:3]  # æœ€å¤§3ã¤ã®ææ¡ˆ
            
        except Exception as e:
            print(f"âŒ é©å¿œçš„å¿œç­”ææ¡ˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _find_memories_by_keywords(self, user_input: str) -> List[Dict]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§è¨˜æ†¶ã‚’æ¤œç´¢"""
        input_keywords = self._extract_keywords(user_input)
        
        if not input_keywords:
            return []
        
        matching_memories = []
        
        for rel in self.integration_data["memory_relationships"]:
            metadata = rel.get("metadata", {})
            common_themes = metadata.get("common_themes", [])
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´åº¦è¨ˆç®—
            match_score = len(set(input_keywords) & set(common_themes)) / max(len(input_keywords), 1)
            
            if match_score > 0:
                matching_memories.append({
                    "memory": rel["target_memory"],
                    "relevance_score": match_score * rel["strength"],
                    "relationship": rel
                })
        
        # é–¢é€£åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        matching_memories.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return matching_memories
    
    def _get_success_data_by_id(self, success_id: str) -> Optional[Dict]:
        """æˆåŠŸãƒ‡ãƒ¼ã‚¿ã‚’IDã§å–å¾—"""
        if not self.collaboration_memory:
            return None
        
        successes = self.collaboration_memory.collaboration_data.get("success_patterns", [])
        
        for success in successes:
            if success.get("id") == success_id:
                return success
        
        return None
    
    def _get_cluster_based_suggestions(self, user_input: str) -> List[str]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ææ¡ˆã‚’ç”Ÿæˆ"""
        suggestions = []
        input_keywords = self._extract_keywords(user_input)
        
        for cluster_name, cluster_data in self.integration_data["memory_clusters"].items():
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒã¨ã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯
            theme = cluster_data.get("theme", "")
            
            if any(keyword in theme for keyword in input_keywords):
                suggestion = f"{theme}ã®çµŒé¨“ã‚’æ´»ã‹ã—ãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒãŠã™ã™ã‚"
                suggestions.append(suggestion)
        
        return suggestions
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è¨˜æ†¶çµ±åˆçµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        relationships = self.integration_data["memory_relationships"]
        clusters = self.integration_data["memory_clusters"]
        
        # é–¢ä¿‚æ€§ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        type_stats = {}
        for rel_type in self.integration_data["relationship_types"].keys():
            type_stats[rel_type] = len([r for r in relationships if r["relationship_type"] == rel_type])
        
        # å¼·åº¦åˆ¥çµ±è¨ˆ
        strong_relationships = len([r for r in relationships if r["strength"] >= 0.7])
        moderate_relationships = len([r for r in relationships if 0.4 <= r["strength"] < 0.7])
        weak_relationships = len([r for r in relationships if r["strength"] < 0.4])
        
        return {
            "total_relationships": len(relationships),
            "total_clusters": len(clusters),
            "relationship_types": type_stats,
            "strength_distribution": {
                "strong": strong_relationships,
                "moderate": moderate_relationships,
                "weak": weak_relationships
            },
            "memory_mode": self.memory_mode,
            "last_analysis": datetime.now().isoformat()
        }

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("=== MemoryIntegrationSystem ãƒ†ã‚¹ãƒˆ ===")
    
    # åŸºæœ¬åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
    print("\n--- åŸºæœ¬åˆæœŸåŒ– ---")
    integration = MemoryIntegrationSystem(memory_mode="test")
    
    # çµ±è¨ˆæƒ…å ±ãƒ†ã‚¹ãƒˆ
    stats = integration.get_memory_stats()
    print(f"åˆæœŸçµ±è¨ˆ: {stats}")
    
    print("\nâœ… MemoryIntegrationSystem åŸºç›¤ãƒ†ã‚¹ãƒˆå®Œäº†")