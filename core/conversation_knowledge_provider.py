#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ConversationKnowledgeProvider - Phase 4çµ±åˆ
éŸ³å£°å¯¾è©±æ™‚ã®çŸ¥è­˜æ¤œç´¢ãƒ»åˆ†æãƒ»å¿œç­”çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import re
from collections import defaultdict
from difflib import SequenceMatcher

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ç¢ºå®Ÿã«ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from core.knowledge_analysis.knowledge_analysis_engine import KnowledgeAnalysisEngine
    KNOWLEDGE_ENGINE_AVAILABLE = True
except ImportError:
    KNOWLEDGE_ENGINE_AVAILABLE = False
    print("âš ï¸ KnowledgeAnalysisEngineãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

# ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
try:
    from logging_system import get_logger
    LOGGER_AVAILABLE = True
except ImportError:
    LOGGER_AVAILABLE = False

# Windowsç’°å¢ƒã®ãƒ‘ã‚¹è¨­å®š
if os.name == 'nt':
    DATA_DIR = Path("D:/setsuna_bot/data/activity_knowledge")
else:
    DATA_DIR = Path("/mnt/d/setsuna_bot/data/activity_knowledge")

@dataclass
class KnowledgeContext:
    """çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    context_id: str
    user_input: str
    detected_topics: List[str]
    relevant_knowledge: List[Dict[str, Any]]
    confidence_score: float
    knowledge_types: List[str]  # "factual", "experiential", "analytical", "predictive"
    application_suggestions: List[str]
    
@dataclass
class ConversationEnhancement:
    """ä¼šè©±å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    enhancement_type: str  # "knowledge_injection", "insight_sharing", "trend_analysis"
    knowledge_source: str  # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã¾ãŸã¯çµ±åˆçŸ¥è­˜ID
    enhancement_content: str
    relevance_score: float
    timing_suggestion: str  # "immediate", "follow_up", "related_topic"

class ConversationKnowledgeProvider:
    """éŸ³å£°å¯¾è©±ç”¨çŸ¥è­˜æä¾›ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        if LOGGER_AVAILABLE:
            self.logger = get_logger()
            self.logger.info("conversation_knowledge", "init", "ConversationKnowledgeProvideråˆæœŸåŒ–é–‹å§‹")
        else:
            self.logger = None
        
        # çŸ¥è­˜åˆ†æã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        if KNOWLEDGE_ENGINE_AVAILABLE:
            self.knowledge_engine = KnowledgeAnalysisEngine(
                progress_callback=self._knowledge_progress_callback
            )
            if self.logger:
                self.logger.info("conversation_knowledge", "init", "KnowledgeAnalysisEngineåˆæœŸåŒ–æˆåŠŸ")
        else:
            self.knowledge_engine = None
            if self.logger:
                self.logger.warning("conversation_knowledge", "init", "KnowledgeAnalysisEngineåˆ©ç”¨ä¸å¯")
        
        self.knowledge_dir = DATA_DIR
        
        # é–¢é€£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.sessions_dir = self.knowledge_dir / "sessions"
        self.knowledge_graph_dir = self.knowledge_dir / "knowledge_graph"
        self.integrated_dir = self.knowledge_dir / "integrated_knowledge"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.cache_dir = Path("D:/setsuna_bot/conversation_knowledge_cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # çŸ¥è­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.knowledge_cache: Dict[str, Any] = {}
        self.topic_index: Dict[str, List[str]] = defaultdict(list)  # ãƒˆãƒ”ãƒƒã‚¯ -> çŸ¥è­˜IDãƒªã‚¹ãƒˆ
        self.entity_index: Dict[str, List[str]] = defaultdict(list)  # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ -> çŸ¥è­˜IDãƒªã‚¹ãƒˆ
        
        # ä¼šè©±å±¥æ­´
        self.conversation_history: List[Dict] = []
        self.active_contexts: List[KnowledgeContext] = []
        
        # è¨­å®š
        self.provider_config = {
            "max_relevant_knowledge": 5,
            "relevance_threshold": 0.6,
            "context_window_size": 3,  # éå»3ç™ºè¨€ã‚’è€ƒæ…®
            "knowledge_freshness_days": 30,
            "auto_enhancement_enabled": True,
            "proactive_suggestion_enabled": True
        }
        
        # çŸ¥è­˜çµ±åˆè¨­å®š
        self.integration_config = {
            "enable_realtime_search": True,
            "max_search_time_seconds": 30,
            "context_injection_mode": "summary",  # "full", "summary", "keywords"
            "cache_knowledge_hours": 24,
            "min_relevance_score": 0.3
        }
        
        # ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        self.topic_patterns = {
            "AIæŠ€è¡“": [r"AI", r"äººå·¥çŸ¥èƒ½", r"æ©Ÿæ¢°å­¦ç¿’", r"æ·±å±¤å­¦ç¿’", r"ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«", r"ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ "],
            "éŸ³æ¥½ç”Ÿæˆ": [r"éŸ³æ¥½", r"ä½œæ›²", r"æ¥½æ›²", r"ãƒ¡ãƒ­ãƒ‡ã‚£", r"ãƒªã‚ºãƒ ", r"ãƒãƒ¼ãƒ¢ãƒ‹ãƒ¼"],
            "æŠ€è¡“å‹•å‘": [r"ãƒˆãƒ¬ãƒ³ãƒ‰", r"å‹•å‘", r"æœ€æ–°", r"æ–°æŠ€è¡“", r"é©æ–°", r"ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³"],
            "ãƒ„ãƒ¼ãƒ«": [r"ãƒ„ãƒ¼ãƒ«", r"ã‚¢ãƒ—ãƒª", r"ã‚½ãƒ•ãƒˆ", r"ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ", r"ã‚µãƒ¼ãƒ“ã‚¹"],
            "å¸‚å ´åˆ†æ": [r"å¸‚å ´", r"ãƒ“ã‚¸ãƒã‚¹", r"ç”£æ¥­", r"æ¥­ç•Œ", r"ç«¶åˆ", r"ã‚·ã‚§ã‚¢"],
            "å®Ÿç”¨æ€§": [r"å®Ÿç”¨", r"æ´»ç”¨", r"å¿œç”¨", r"å®Ÿè·µ", r"ä½¿ã„æ–¹", r"å®Ÿè£…"]
        }
        
        self._initialize_knowledge_cache()
        
        print("[ä¼šè©±çŸ¥è­˜] âœ… ConversationKnowledgeProvideråˆæœŸåŒ–å®Œäº†")
    
    def _knowledge_progress_callback(self, stage: str, progress: float, message: str):
        """çŸ¥è­˜åˆ†æé€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        if self.logger:
            self.logger.info("conversation_knowledge", "progress", 
                           f"[{stage}] {progress:.1%} - {message}")
        print(f"ğŸ” [{stage}] {progress:.1%} - {message}")
    
    def get_knowledge_context(self, user_input: str, mode: str = "full_search") -> Dict[str, Any]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«åŸºã¥ãçŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—"""
        start_time = datetime.now()
        
        if self.logger:
            self.logger.info("conversation_knowledge", "get_context", 
                           f"çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—é–‹å§‹ - mode: {mode}, input: {user_input[:50]}...")
        
        knowledge_context = {
            "has_knowledge": False,
            "search_performed": False,
            "knowledge_summary": "",
            "key_insights": [],
            "related_topics": [],
            "search_details": {},
            "context_injection_text": "",
            "processing_time": 0.0,
            "mode": mode
        }
        
        try:
            # ãƒ¢ãƒ¼ãƒ‰åˆ¥å‡¦ç†
            if mode == "full_search" and self.knowledge_engine:
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŸ¥è­˜æ¤œç´¢
                knowledge_context = self._perform_realtime_search(user_input, knowledge_context)
            
            elif mode in ["fast_response", "ultra_fast"]:
                # é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçŸ¥è­˜æ¤œç´¢
                knowledge_context = self._search_cached_knowledge(user_input, knowledge_context)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ³¨å…¥ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            if knowledge_context["has_knowledge"]:
                knowledge_context["context_injection_text"] = self._generate_context_injection(knowledge_context)
            
            # å‡¦ç†æ™‚é–“è¨˜éŒ²
            processing_time = (datetime.now() - start_time).total_seconds()
            knowledge_context["processing_time"] = processing_time
            
            if self.logger:
                self.logger.info("conversation_knowledge", "get_context", 
                               f"çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—å®Œäº† - æ™‚é–“: {processing_time:.2f}s, çŸ¥è­˜æœ‰ç„¡: {knowledge_context['has_knowledge']}")
            
            return knowledge_context
            
        except Exception as e:
            if self.logger:
                self.logger.error("conversation_knowledge", "get_context", f"çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            knowledge_context["error"] = str(e)
            return knowledge_context
    
    def _perform_realtime_search(self, user_input: str, context: Dict) -> Dict:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŸ¥è­˜æ¤œç´¢å®Ÿè¡Œ"""
        if not self.knowledge_engine:
            return context
        
        try:
            print("ğŸ” ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŸ¥è­˜æ¤œç´¢é–‹å§‹...")
            
            # çŸ¥è­˜åˆ†æå®Ÿè¡Œï¼ˆç°¡æ˜“æ¤œç´¢ãƒ»åˆ†æï¼‰
            search_results = self.knowledge_engine._execute_large_scale_search(user_input)
            
            if search_results:
                # åˆ†æå®Ÿè¡Œ
                analysis_result = self.knowledge_engine._execute_batch_analysis(search_results, user_input)
                
                # ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ã«å¤‰æ›
                report = {
                    "analysis_summary": analysis_result.get("analysis", ""),
                    "key_insights": [],
                    "related_topics": [],
                    "search_count": len(search_results),
                    "data_quality": 0.7 if search_results else 0.0,
                    "cost": analysis_result.get("total_cost", 0.0),
                    "processing_time": analysis_result.get("analysis_log", {}).get("summary", {}).get("total_time", 0)
                }
                
                # åˆ†æçµæœã‹ã‚‰æ´å¯ŸæŠ½å‡º
                if "batch_summaries" in analysis_result:
                    for batch_summary in analysis_result["batch_summaries"][:3]:
                        if isinstance(batch_summary, str) and len(batch_summary) > 10:
                            report["key_insights"].append(batch_summary[:100])
                
                # é–¢é€£ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
                prompt_words = user_input.split()
                report["related_topics"] = [word for word in prompt_words if len(word) > 2][:5]
            else:
                report = None
            
            if report and isinstance(report, dict):
                context["has_knowledge"] = True
                context["search_performed"] = True
                context["knowledge_summary"] = report.get("analysis_summary", "")
                context["key_insights"] = report.get("key_insights", [])
                context["related_topics"] = report.get("related_topics", [])
                context["search_details"] = {
                    "search_count": report.get("search_count", 0),
                    "data_quality": report.get("data_quality", 0.0),
                    "cost": report.get("cost", 0.0),
                    "processing_time": report.get("processing_time", 0)
                }
                
                # çŸ¥è­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
                self._cache_knowledge(user_input, report)
                
                print(f"âœ… çŸ¥è­˜æ¤œç´¢å®Œäº† - {report.get('search_count', 0)}ä»¶ã®æ¤œç´¢çµæœã‚’åˆ†æ")
            else:
                print("âš ï¸ çŸ¥è­˜æ¤œç´¢çµæœãŒç©ºã§ã—ãŸ")
            
            return context
            
        except Exception as e:
            print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŸ¥è­˜æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            if self.logger:
                self.logger.error("conversation_knowledge", "realtime_search", f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return context
    
    def _search_cached_knowledge(self, user_input: str, context: Dict) -> Dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçŸ¥è­˜æ¤œç´¢"""
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
            cache_files = list(self.cache_dir.glob("*.json"))
            relevant_knowledge = []
            
            for cache_file in cache_files:
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cached_data = json.load(f)
                    
                    # é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    relevance_score = self._calculate_relevance(user_input, cached_data)
                    if relevance_score >= self.integration_config["min_relevance_score"]:
                        relevant_knowledge.append({
                            "data": cached_data,
                            "relevance": relevance_score,
                            "cache_time": cache_file.stat().st_mtime
                        })
                
                except Exception:
                    continue
            
            # é–¢é€£åº¦é †ã«ã‚½ãƒ¼ãƒˆ
            relevant_knowledge.sort(key=lambda x: x["relevance"], reverse=True)
            
            if relevant_knowledge:
                best_knowledge = relevant_knowledge[0]["data"]
                context["has_knowledge"] = True
                context["knowledge_summary"] = best_knowledge.get("analysis_summary", "")
                context["key_insights"] = best_knowledge.get("key_insights", [])
                context["related_topics"] = best_knowledge.get("related_topics", [])
                context["search_details"] = {
                    "cache_source": True,
                    "relevance_score": relevant_knowledge[0]["relevance"]
                }
                
                print(f"ğŸ“š ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é–¢é€£çŸ¥è­˜ã‚’å–å¾— - é–¢é€£åº¦: {relevant_knowledge[0]['relevance']:.2f}")
            
            return context
            
        except Exception as e:
            print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŸ¥è­˜æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return context
    
    def _calculate_relevance(self, user_input: str, cached_data: Dict) -> float:
        """é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            user_words = set(user_input.lower().split())
            
            # æ¯”è¼ƒå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            comparison_text = ""
            if "user_prompt" in cached_data:
                comparison_text += cached_data["user_prompt"].lower() + " "
            if "analysis_summary" in cached_data:
                comparison_text += cached_data["analysis_summary"].lower() + " "
            if "key_insights" in cached_data:
                comparison_text += " ".join(cached_data["key_insights"]).lower() + " "
            
            cached_words = set(comparison_text.split())
            
            # Jaccardé¡ä¼¼åº¦è¨ˆç®—
            intersection = len(user_words.intersection(cached_words))
            union = len(user_words.union(cached_words))
            
            return intersection / union if union > 0 else 0.0
            
        except Exception:
            return 0.0
    
    def _cache_knowledge(self, user_input: str, report: Dict):
        """çŸ¥è­˜ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            cache_filename = f"knowledge_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            cache_path = self.cache_dir / cache_filename
            
            cache_data = {
                "user_input": user_input,
                "report": report,
                "cached_at": datetime.now().isoformat()
            }
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤
            self._cleanup_old_cache()
            
        except Exception as e:
            if self.logger:
                self.logger.warning("conversation_knowledge", "cache", f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _cleanup_old_cache(self):
        """å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤"""
        try:
            cutoff_time = datetime.now().timestamp() - (self.integration_config["cache_knowledge_hours"] * 3600)
            
            for cache_file in self.cache_dir.glob("*.json"):
                if cache_file.stat().st_mtime < cutoff_time:
                    cache_file.unlink()
            
        except Exception:
            pass
    
    def _generate_context_injection(self, knowledge_context: Dict) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ³¨å…¥ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        mode = self.integration_config["context_injection_mode"]
        
        if mode == "keywords":
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿æ³¨å…¥
            topics = knowledge_context.get("related_topics", [])
            if topics:
                return f"é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(topics[:3])}"
        
        elif mode == "summary":
            # è¦ç´„æ³¨å…¥
            summary = knowledge_context.get("knowledge_summary", "")
            insights = knowledge_context.get("key_insights", [])
            
            context_text = ""
            if summary:
                context_text += f"ã€èƒŒæ™¯çŸ¥è­˜ã€‘{summary[:200]}..."
            if insights:
                context_text += f" ã€é‡è¦ãƒã‚¤ãƒ³ãƒˆã€‘{', '.join(insights[:2])}"
            
            return context_text
        
        elif mode == "full":
            # å®Œå…¨ãªçŸ¥è­˜æ³¨å…¥
            parts = []
            
            if knowledge_context.get("knowledge_summary"):
                parts.append(f"åˆ†æçµæœ: {knowledge_context['knowledge_summary']}")
            
            if knowledge_context.get("key_insights"):
                parts.append(f"ä¸»è¦ç™ºè¦‹: {', '.join(knowledge_context['key_insights'][:3])}")
            
            if knowledge_context.get("related_topics"):
                parts.append(f"é–¢é€£ãƒˆãƒ”ãƒƒã‚¯: {', '.join(knowledge_context['related_topics'][:3])}")
            
            return " / ".join(parts)
        
        return ""
    
    def get_cache_stats(self) -> Dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±å–å¾—"""
        try:
            cache_files = list(self.cache_dir.glob("*.json"))
            total_size = sum(f.stat().st_size for f in cache_files)
            
            return {
                "cache_count": len(cache_files),
                "total_size_mb": total_size / (1024 * 1024),
                "cache_dir": str(self.cache_dir),
                "oldest_cache": min((f.stat().st_mtime for f in cache_files), default=0),
                "newest_cache": max((f.stat().st_mtime for f in cache_files), default=0)
            }
        except Exception:
            return {"error": "çµ±è¨ˆæƒ…å ±å–å¾—ã«å¤±æ•—"}
    
    def _initialize_knowledge_cache(self):
        """çŸ¥è­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–"""
        try:
            print("[ä¼šè©±çŸ¥è­˜] ğŸ“š çŸ¥è­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹ç¯‰é–‹å§‹")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŸ¥è­˜èª­ã¿è¾¼ã¿
            self._load_session_knowledge()
            
            # çµ±åˆçŸ¥è­˜èª­ã¿è¾¼ã¿
            self._load_integrated_knowledge()
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
            self._build_knowledge_indexes()
            
            print(f"[ä¼šè©±çŸ¥è­˜] âœ… çŸ¥è­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹ç¯‰å®Œäº†: {len(self.knowledge_cache)}ä»¶")
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âŒ çŸ¥è­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–å¤±æ•—: {e}")
    
    def _load_session_knowledge(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŸ¥è­˜èª­ã¿è¾¼ã¿"""
        try:
            if not self.sessions_dir.exists():
                return
            
            session_files = list(self.sessions_dir.glob("session_*.json"))
            for session_file in session_files:
                with open(session_file, 'r', encoding='utf-8') as f:
                    session_data = json.load(f)
                    
                    session_metadata = session_data.get("session_metadata", {})
                    session_id = session_metadata.get("session_id", "")
                    
                    # çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ æŠ½å‡º
                    if "collection_results" in session_data:
                        knowledge_items = session_data["collection_results"].get("information_sources", [])
                        for item in knowledge_items:
                            knowledge_id = f"session_{session_id}_{item.get('source_id', '')}"
                            
                            self.knowledge_cache[knowledge_id] = {
                                "type": "session_knowledge",
                                "source": session_id,
                                "content": item.get("content", ""),
                                "title": item.get("title", ""),
                                "categories": item.get("categories", []),
                                "keywords": item.get("keywords", []),
                                "entities": item.get("entities", []),
                                "reliability_score": item.get("reliability_score", 0.7),
                                "created_at": session_metadata.get("created_at", ""),
                                "preprocessing_result": item.get("preprocessing_result", {})
                            }
            
            print(f"[ä¼šè©±çŸ¥è­˜] ğŸ“– ã‚»ãƒƒã‚·ãƒ§ãƒ³çŸ¥è­˜èª­ã¿è¾¼ã¿: {len([k for k in self.knowledge_cache.keys() if k.startswith('session_')])}ä»¶")
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³çŸ¥è­˜èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    
    def _load_integrated_knowledge(self):
        """çµ±åˆçŸ¥è­˜èª­ã¿è¾¼ã¿"""
        try:
            if not self.integrated_dir.exists():
                return
            
            integration_files = list(self.integrated_dir.glob("integration_*.json"))
            for integration_file in integration_files:
                with open(integration_file, 'r', encoding='utf-8') as f:
                    integration_data = json.load(f)
                    
                    for item in integration_data.get("integrated_knowledge", []):
                        knowledge_id = item.get("knowledge_id", "")
                        
                        self.knowledge_cache[knowledge_id] = {
                            "type": "integrated_knowledge",
                            "source": "integration",
                            "content": item.get("synthesized_content", ""),
                            "key_insights": item.get("key_insights", []),
                            "related_concepts": item.get("related_concepts", []),
                            "confidence_score": item.get("confidence_score", 0.7),
                            "novelty_score": item.get("novelty_score", 0.5),
                            "application_domains": item.get("application_domains", []),
                            "actionable_insights": item.get("actionable_insights", []),
                            "created_at": item.get("created_at", ""),
                            "integration_type": item.get("integration_type", "")
                        }
            
            print(f"[ä¼šè©±çŸ¥è­˜] ğŸ”— çµ±åˆçŸ¥è­˜èª­ã¿è¾¼ã¿: {len([k for k in self.knowledge_cache.keys() if not k.startswith('session_')])}ä»¶")
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ çµ±åˆçŸ¥è­˜èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    
    def _build_knowledge_indexes(self):
        """çŸ¥è­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        try:
            # ãƒˆãƒ”ãƒƒã‚¯ãƒ»ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
            for knowledge_id, knowledge in self.knowledge_cache.items():
                # ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
                categories = knowledge.get("categories", [])
                for category in categories:
                    self.topic_index[category.lower()].append(knowledge_id)
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
                keywords = knowledge.get("keywords", [])
                for keyword in keywords:
                    self.topic_index[keyword.lower()].append(knowledge_id)
                
                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                entities = knowledge.get("entities", [])
                for entity in entities:
                    self.entity_index[entity.lower()].append(knowledge_id)
                
                # é–¢é€£æ¦‚å¿µã‹ã‚‰ã‚‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
                related_concepts = knowledge.get("related_concepts", [])
                for concept in related_concepts:
                    self.topic_index[concept.lower()].append(knowledge_id)
            
            print(f"[ä¼šè©±çŸ¥è­˜] ğŸ—‚ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†: ãƒˆãƒ”ãƒƒã‚¯{len(self.topic_index)}ä»¶, ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£{len(self.entity_index)}ä»¶")
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å¤±æ•—: {e}")
    
    def analyze_user_input(self, user_input: str, conversation_context: List[Dict] = None) -> KnowledgeContext:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›åˆ†æãƒ»é–¢é€£çŸ¥è­˜ç‰¹å®š
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›æ–‡
            conversation_context: ä¼šè©±å±¥æ­´
            
        Returns:
            çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            print(f"[ä¼šè©±çŸ¥è­˜] ğŸ” ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›åˆ†æ: {user_input[:50]}...")
            
            # ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡º
            detected_topics = self._detect_topics(user_input)
            
            # é–¢é€£çŸ¥è­˜æ¤œç´¢
            relevant_knowledge = self._search_relevant_knowledge(user_input, detected_topics)
            
            # ä¼šè©±å±¥æ­´è€ƒæ…®
            if conversation_context:
                contextual_knowledge = self._get_contextual_knowledge(conversation_context, detected_topics)
                relevant_knowledge.extend(contextual_knowledge)
            
            # é‡è¤‡é™¤å»ãƒ»ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
            relevant_knowledge = self._deduplicate_and_rank_knowledge(relevant_knowledge)
            
            # ä¿¡é ¼åº¦è¨ˆç®—
            confidence_score = self._calculate_context_confidence(detected_topics, relevant_knowledge)
            
            # çŸ¥è­˜ã‚¿ã‚¤ãƒ—åˆ†é¡
            knowledge_types = self._classify_knowledge_types(relevant_knowledge)
            
            # å¿œç”¨ææ¡ˆç”Ÿæˆ
            application_suggestions = self._generate_application_suggestions(relevant_knowledge, detected_topics)
            
            context_id = f"ctx_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(user_input) % 10000:04d}"
            
            context = KnowledgeContext(
                context_id=context_id,
                user_input=user_input,
                detected_topics=detected_topics,
                relevant_knowledge=relevant_knowledge[:self.provider_config["max_relevant_knowledge"]],
                confidence_score=confidence_score,
                knowledge_types=knowledge_types,
                application_suggestions=application_suggestions
            )
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
            self.active_contexts.append(context)
            
            # å¤ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self._cleanup_old_contexts()
            
            print(f"[ä¼šè©±çŸ¥è­˜] âœ… åˆ†æå®Œäº†: ãƒˆãƒ”ãƒƒã‚¯{len(detected_topics)}ä»¶, é–¢é€£çŸ¥è­˜{len(context.relevant_knowledge)}ä»¶")
            
            return context
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›åˆ†æå¤±æ•—: {e}")
            return KnowledgeContext("", user_input, [], [], 0.0, [], [])
    
    def _detect_topics(self, user_input: str) -> List[str]:
        """ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡º"""
        detected_topics = []
        user_input_lower = user_input.lower()
        
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡º
            for topic, patterns in self.topic_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, user_input_lower):
                        if topic not in detected_topics:
                            detected_topics.append(topic)
                        break
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç›´æ¥ãƒãƒƒãƒãƒ³ã‚°
            for topic_key in self.topic_index.keys():
                if topic_key in user_input_lower:
                    # å¯¾å¿œã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯æ¤œç´¢
                    for main_topic, patterns in self.topic_patterns.items():
                        if any(re.search(pattern, topic_key) for pattern in patterns):
                            if main_topic not in detected_topics:
                                detected_topics.append(main_topic)
                            break
            
            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯æ¨å®š
            for entity_key in self.entity_index.keys():
                if entity_key in user_input_lower:
                    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«é–¢é€£ã™ã‚‹çŸ¥è­˜ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯æ¨å®š
                    related_knowledge_ids = self.entity_index[entity_key][:3]  # ä¸Šä½3ä»¶
                    for knowledge_id in related_knowledge_ids:
                        knowledge = self.knowledge_cache.get(knowledge_id, {})
                        categories = knowledge.get("categories", [])
                        for category in categories:
                            if category not in detected_topics:
                                detected_topics.append(category)
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡ºå¤±æ•—: {e}")
        
        return detected_topics[:5]  # æœ€å¤§5ãƒˆãƒ”ãƒƒã‚¯
    
    def _search_relevant_knowledge(self, user_input: str, detected_topics: List[str]) -> List[Dict]:
        """é–¢é€£çŸ¥è­˜æ¤œç´¢"""
        relevant_knowledge = []
        
        try:
            # ãƒˆãƒ”ãƒƒã‚¯ãƒ™ãƒ¼ã‚¹æ¤œç´¢
            for topic in detected_topics:
                topic_lower = topic.lower()
                knowledge_ids = self.topic_index.get(topic_lower, [])
                
                for knowledge_id in knowledge_ids[:10]:  # å„ãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰æœ€å¤§10ä»¶
                    knowledge = self.knowledge_cache.get(knowledge_id, {})
                    if knowledge:
                        relevance_score = self._calculate_relevance_score(user_input, knowledge)
                        
                        if relevance_score >= self.provider_config["relevance_threshold"]:
                            relevant_knowledge.append({
                                "knowledge_id": knowledge_id,
                                "knowledge": knowledge,
                                "relevance_score": relevance_score,
                                "match_type": "topic_match"
                            })
            
            # ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦æ¤œç´¢
            text_matches = self._search_by_text_similarity(user_input)
            relevant_knowledge.extend(text_matches)
            
            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹æ¤œç´¢
            entity_matches = self._search_by_entities(user_input)
            relevant_knowledge.extend(entity_matches)
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ é–¢é€£çŸ¥è­˜æ¤œç´¢å¤±æ•—: {e}")
        
        return relevant_knowledge
    
    def _calculate_relevance_score(self, user_input: str, knowledge: Dict) -> float:
        """é–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        try:
            user_input_lower = user_input.lower()
            
            scores = []
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é¡ä¼¼åº¦
            content = knowledge.get("content", "").lower()
            if content:
                content_similarity = SequenceMatcher(None, user_input_lower, content).ratio()
                scores.append(("content", content_similarity, 0.4))
            
            # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦
            title = knowledge.get("title", "").lower()
            if title:
                title_similarity = SequenceMatcher(None, user_input_lower, title).ratio()
                scores.append(("title", title_similarity, 0.3))
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
            keywords = knowledge.get("keywords", [])
            keyword_matches = sum(1 for kw in keywords if kw.lower() in user_input_lower)
            if keywords:
                keyword_score = keyword_matches / len(keywords)
                scores.append(("keywords", keyword_score, 0.2))
            
            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒƒãƒ
            entities = knowledge.get("entities", [])
            entity_matches = sum(1 for entity in entities if entity.lower() in user_input_lower)
            if entities:
                entity_score = entity_matches / len(entities)
                scores.append(("entities", entity_score, 0.1))
            
            # é‡ã¿ä»˜ãåˆè¨ˆ
            if scores:
                weighted_sum = sum(score * weight for _, score, weight in scores)
                total_weight = sum(weight for _, _, weight in scores)
                return weighted_sum / total_weight
            
            return 0.0
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ é–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—å¤±æ•—: {e}")
            return 0.0
    
    def _search_by_text_similarity(self, user_input: str) -> List[Dict]:
        """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦æ¤œç´¢"""
        text_matches = []
        
        try:
            user_input_lower = user_input.lower()
            
            for knowledge_id, knowledge in self.knowledge_cache.items():
                content = knowledge.get("content", "").lower()
                title = knowledge.get("title", "").lower()
                
                # å˜èªãƒ¬ãƒ™ãƒ«ã§ã®å…±é€šæ€§ãƒã‚§ãƒƒã‚¯
                user_words = set(re.findall(r'\w+', user_input_lower))
                content_words = set(re.findall(r'\w+', content))
                title_words = set(re.findall(r'\w+', title))
                
                if len(user_words) > 0:
                    content_overlap = len(user_words & content_words) / len(user_words)
                    title_overlap = len(user_words & title_words) / len(user_words) if title_words else 0
                    
                    max_overlap = max(content_overlap, title_overlap)
                    
                    if max_overlap >= 0.3:  # 30%ä»¥ä¸Šã®å˜èªé‡è¤‡
                        text_matches.append({
                            "knowledge_id": knowledge_id,
                            "knowledge": knowledge,
                            "relevance_score": max_overlap,
                            "match_type": "text_similarity"
                        })
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦æ¤œç´¢å¤±æ•—: {e}")
        
        return text_matches
    
    def _search_by_entities(self, user_input: str) -> List[Dict]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹æ¤œç´¢"""
        entity_matches = []
        
        try:
            user_input_lower = user_input.lower()
            
            for entity_key, knowledge_ids in self.entity_index.items():
                if entity_key in user_input_lower:
                    for knowledge_id in knowledge_ids[:5]:  # å„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰æœ€å¤§5ä»¶
                        knowledge = self.knowledge_cache.get(knowledge_id, {})
                        if knowledge:
                            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒƒãƒã«ã‚ˆã‚‹é–¢é€£åº¦ã¯ä¸­ç¨‹åº¦
                            relevance_score = 0.7
                            
                            entity_matches.append({
                                "knowledge_id": knowledge_id,
                                "knowledge": knowledge,
                                "relevance_score": relevance_score,
                                "match_type": "entity_match",
                                "matched_entity": entity_key
                            })
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹æ¤œç´¢å¤±æ•—: {e}")
        
        return entity_matches
    
    def _get_contextual_knowledge(self, conversation_context: List[Dict], detected_topics: List[str]) -> List[Dict]:
        """ä¼šè©±æ–‡è„ˆã‹ã‚‰ã®é–¢é€£çŸ¥è­˜å–å¾—"""
        contextual_knowledge = []
        
        try:
            # éå»ã®ä¼šè©±ã‹ã‚‰é–¢é€£ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
            context_window = conversation_context[-self.provider_config["context_window_size"]:]
            
            for message in context_window:
                message_text = message.get("content", "")
                if message_text:
                    # éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚‚é–¢é€£çŸ¥è­˜æ¤œç´¢
                    past_topics = self._detect_topics(message_text)
                    
                    # ç¾åœ¨ã®ãƒˆãƒ”ãƒƒã‚¯ã¨ã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯
                    common_topics = set(detected_topics) & set(past_topics)
                    
                    if common_topics:
                        for topic in common_topics:
                            topic_knowledge = self.topic_index.get(topic.lower(), [])
                            for knowledge_id in topic_knowledge[:3]:  # å„å…±é€šãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰3ä»¶
                                knowledge = self.knowledge_cache.get(knowledge_id, {})
                                if knowledge:
                                    contextual_knowledge.append({
                                        "knowledge_id": knowledge_id,
                                        "knowledge": knowledge,
                                        "relevance_score": 0.6,  # æ–‡è„ˆçš„é–¢é€£åº¦
                                        "match_type": "contextual_match",
                                        "context_topic": topic
                                    })
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ æ–‡è„ˆçŸ¥è­˜å–å¾—å¤±æ•—: {e}")
        
        return contextual_knowledge
    
    def _deduplicate_and_rank_knowledge(self, knowledge_list: List[Dict]) -> List[Dict]:
        """é‡è¤‡é™¤å»ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        try:
            # çŸ¥è­˜IDã«ã‚ˆã‚‹é‡è¤‡é™¤å»
            unique_knowledge = {}
            for item in knowledge_list:
                knowledge_id = item["knowledge_id"]
                if knowledge_id not in unique_knowledge:
                    unique_knowledge[knowledge_id] = item
                else:
                    # ã‚ˆã‚Šé«˜ã„ã‚¹ã‚³ã‚¢ã®ã‚‚ã®ã‚’ä¿æŒ
                    if item["relevance_score"] > unique_knowledge[knowledge_id]["relevance_score"]:
                        unique_knowledge[knowledge_id] = item
            
            # é–¢é€£åº¦ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            ranked_knowledge = list(unique_knowledge.values())
            ranked_knowledge.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            return ranked_knowledge
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ é‡è¤‡é™¤å»ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¤±æ•—: {e}")
            return knowledge_list
    
    def _calculate_context_confidence(self, detected_topics: List[str], relevant_knowledge: List[Dict]) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿¡é ¼åº¦è¨ˆç®—"""
        try:
            if not detected_topics or not relevant_knowledge:
                return 0.0
            
            # ãƒˆãƒ”ãƒƒã‚¯æ¤œå‡ºã®ç¢ºå®Ÿæ€§
            topic_confidence = min(1.0, len(detected_topics) / 3)
            
            # é–¢é€£çŸ¥è­˜ã®å“è³ª
            avg_relevance = sum(item["relevance_score"] for item in relevant_knowledge) / len(relevant_knowledge)
            
            # çŸ¥è­˜ã®ä¿¡é ¼æ€§
            knowledge_reliability = 0.0
            for item in relevant_knowledge:
                knowledge = item["knowledge"]
                reliability = knowledge.get("reliability_score", 0.7)
                confidence = knowledge.get("confidence_score", 0.7)
                knowledge_reliability += (reliability + confidence) / 2
            
            knowledge_reliability /= len(relevant_knowledge)
            
            # ç·åˆä¿¡é ¼åº¦
            overall_confidence = (topic_confidence * 0.3 + avg_relevance * 0.4 + knowledge_reliability * 0.3)
            
            return min(1.0, overall_confidence)
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ ä¿¡é ¼åº¦è¨ˆç®—å¤±æ•—: {e}")
            return 0.5
    
    def _classify_knowledge_types(self, relevant_knowledge: List[Dict]) -> List[str]:
        """çŸ¥è­˜ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        knowledge_types = set()
        
        try:
            for item in relevant_knowledge:
                knowledge = item["knowledge"]
                
                # çŸ¥è­˜ã®ç¨®é¡åˆ¤å®š
                if knowledge.get("type") == "integrated_knowledge":
                    knowledge_types.add("analytical")
                    
                    # çµ±åˆçŸ¥è­˜ã®ç‰¹æ€§ã«ã‚ˆã‚‹åˆ†é¡
                    if knowledge.get("integration_type") == "temporal_evolution":
                        knowledge_types.add("predictive")
                elif knowledge.get("type") == "session_knowledge":
                    knowledge_types.add("factual")
                    
                    # å‰å‡¦ç†çµæœã«ã‚ˆã‚‹åˆ†é¡
                    preprocessing = knowledge.get("preprocessing_result", {})
                    if preprocessing.get("category") == "å®Ÿç”¨":
                        knowledge_types.add("experiential")
                
                # å¿œç”¨å¯èƒ½æ€§ã«ã‚ˆã‚‹åˆ†é¡
                if knowledge.get("actionable_insights"):
                    knowledge_types.add("experiential")
                
                # äºˆæ¸¬ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±
                if any(keyword in knowledge.get("content", "").lower() 
                      for keyword in ["äºˆæ¸¬", "å°†æ¥", "ãƒˆãƒ¬ãƒ³ãƒ‰", "è¦‹è¾¼ã¿"]):
                    knowledge_types.add("predictive")
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ çŸ¥è­˜ã‚¿ã‚¤ãƒ—åˆ†é¡å¤±æ•—: {e}")
        
        return list(knowledge_types)
    
    def _generate_application_suggestions(self, relevant_knowledge: List[Dict], detected_topics: List[str]) -> List[str]:
        """å¿œç”¨ææ¡ˆç”Ÿæˆ"""
        suggestions = []
        
        try:
            # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ææ¡ˆ
            for item in relevant_knowledge[:3]:  # ä¸Šä½3ä»¶ã‹ã‚‰ææ¡ˆç”Ÿæˆ
                knowledge = item["knowledge"]
                
                # å®Ÿè¡Œå¯èƒ½ãªæ´å¯Ÿ
                actionable_insights = knowledge.get("actionable_insights", [])
                suggestions.extend(actionable_insights[:2])
                
                # å¿œç”¨åˆ†é‡ã®ææ¡ˆ
                application_domains = knowledge.get("application_domains", [])
                for domain in application_domains[:2]:
                    suggestions.append(f"{domain}ã§ã®æ´»ç”¨ã‚’æ¤œè¨")
                
                # çµ±åˆçŸ¥è­˜ã‹ã‚‰ã®ææ¡ˆ
                if knowledge.get("type") == "integrated_knowledge":
                    key_insights = knowledge.get("key_insights", [])
                    for insight in key_insights[:1]:
                        suggestions.append(f"æ´å¯Ÿã®æ´»ç”¨: {insight}")
            
            # ãƒˆãƒ”ãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ã®ææ¡ˆ
            for topic in detected_topics[:2]:
                if topic == "AIæŠ€è¡“":
                    suggestions.append("æœ€æ–°AIæŠ€è¡“å‹•å‘ã®ç¶™ç¶šçš„å­¦ç¿’")
                elif topic == "éŸ³æ¥½ç”Ÿæˆ":
                    suggestions.append("éŸ³æ¥½ç”Ÿæˆãƒ„ãƒ¼ãƒ«ã®å®Ÿè·µçš„æ¤œè¨¼")
                elif topic == "æŠ€è¡“å‹•å‘":
                    suggestions.append("æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®æ·±åŒ–")
            
            # é‡è¤‡é™¤å»
            suggestions = list(dict.fromkeys(suggestions))
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ å¿œç”¨ææ¡ˆç”Ÿæˆå¤±æ•—: {e}")
        
        return suggestions[:5]  # æœ€å¤§5ä»¶
    
    def _cleanup_old_contexts(self):
        """å¤ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # æœ€æ–°10ä»¶ã®ã¿ä¿æŒ
            if len(self.active_contexts) > 10:
                self.active_contexts = self.active_contexts[-10:]
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
    
    def generate_conversation_enhancements(self, knowledge_context: KnowledgeContext) -> List[ConversationEnhancement]:
        """
        ä¼šè©±å¼·åŒ–ææ¡ˆç”Ÿæˆ
        
        Args:
            knowledge_context: çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ä¼šè©±å¼·åŒ–ææ¡ˆãƒªã‚¹ãƒˆ
        """
        try:
            enhancements = []
            
            if not self.provider_config["auto_enhancement_enabled"]:
                return enhancements
            
            # çŸ¥è­˜æ³¨å…¥å‹å¼·åŒ–
            knowledge_injections = self._generate_knowledge_injections(knowledge_context)
            enhancements.extend(knowledge_injections)
            
            # æ´å¯Ÿå…±æœ‰å‹å¼·åŒ–
            insight_sharings = self._generate_insight_sharings(knowledge_context)
            enhancements.extend(insight_sharings)
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå‹å¼·åŒ–
            trend_analyses = self._generate_trend_analyses(knowledge_context)
            enhancements.extend(trend_analyses)
            
            # é–¢é€£åº¦é †ã‚½ãƒ¼ãƒˆ
            enhancements.sort(key=lambda x: x.relevance_score, reverse=True)
            
            print(f"[ä¼šè©±çŸ¥è­˜] ğŸ’¡ ä¼šè©±å¼·åŒ–ææ¡ˆ: {len(enhancements)}ä»¶")
            
            return enhancements[:3]  # æœ€å¤§3ä»¶
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âŒ ä¼šè©±å¼·åŒ–ææ¡ˆç”Ÿæˆå¤±æ•—: {e}")
            return []
    
    def get_knowledge_statistics(self) -> Dict[str, Any]:
        """çŸ¥è­˜çµ±è¨ˆæƒ…å ±å–å¾—"""
        try:
            total_contexts = len(self.active_contexts)
            
            # åˆ†æã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
            analysis_types = {}
            for context in self.active_contexts:
                analysis_type = context.analysis_type
                if analysis_type not in analysis_types:
                    analysis_types[analysis_type] = 0
                analysis_types[analysis_type] += 1
            
            # å¹³å‡é©åˆåº¦
            avg_confidence = 0.0
            if total_contexts > 0:
                avg_confidence = sum(context.confidence_score for context in self.active_contexts) / total_contexts
            
            # çŸ¥è­˜ã‚½ãƒ¼ã‚¹çµ±è¨ˆ
            knowledge_sources = set()
            for context in self.active_contexts:
                for item in context.relevant_knowledge:
                    knowledge_sources.add(item["knowledge_id"])
            
            return {
                "total_contexts": total_contexts,
                "analysis_type_distribution": analysis_types,
                "average_confidence": avg_confidence,
                "unique_knowledge_sources": len(knowledge_sources),
                "auto_enhancement_enabled": self.provider_config["auto_enhancement_enabled"],
                "enhancement_threshold": self.provider_config["enhancement_threshold"]
            }
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âŒ çµ±è¨ˆæƒ…å ±å–å¾—å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _generate_knowledge_injections(self, context: KnowledgeContext) -> List[ConversationEnhancement]:
        """çŸ¥è­˜æ³¨å…¥å¼·åŒ–ç”Ÿæˆ"""
        injections = []
        
        try:
            for item in context.relevant_knowledge[:2]:  # ä¸Šä½2ä»¶
                knowledge = item["knowledge"]
                
                # ç›´æ¥çš„ãªçŸ¥è­˜å…±æœ‰
                if knowledge.get("type") == "session_knowledge":
                    content = f"é–¢é€£ã™ã‚‹èª¿æŸ»çµæœ: {knowledge.get('content', '')[:100]}..."
                    
                    injections.append(ConversationEnhancement(
                        enhancement_type="knowledge_injection",
                        knowledge_source=item["knowledge_id"],
                        enhancement_content=content,
                        relevance_score=item["relevance_score"],
                        timing_suggestion="immediate"
                    ))
                
                # çµ±åˆçŸ¥è­˜ã‹ã‚‰ã®æ´å¯Ÿ
                elif knowledge.get("type") == "integrated_knowledge":
                    insights = knowledge.get("key_insights", [])
                    if insights:
                        content = f"çµ±åˆåˆ†æã«ã‚ˆã‚‹æ´å¯Ÿ: {insights[0]}"
                        
                        injections.append(ConversationEnhancement(
                            enhancement_type="knowledge_injection",
                            knowledge_source=item["knowledge_id"],
                            enhancement_content=content,
                            relevance_score=item["relevance_score"] * 0.9,
                            timing_suggestion="follow_up"
                        ))
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ çŸ¥è­˜æ³¨å…¥å¼·åŒ–ç”Ÿæˆå¤±æ•—: {e}")
        
        return injections
    
    def _generate_insight_sharings(self, context: KnowledgeContext) -> List[ConversationEnhancement]:
        """æ´å¯Ÿå…±æœ‰å¼·åŒ–ç”Ÿæˆ"""
        sharings = []
        
        try:
            # çµ±åˆçŸ¥è­˜ã‹ã‚‰ã®æ´å¯Ÿå…±æœ‰
            for item in context.relevant_knowledge:
                knowledge = item["knowledge"]
                
                if knowledge.get("type") == "integrated_knowledge":
                    actionable_insights = knowledge.get("actionable_insights", [])
                    
                    for insight in actionable_insights[:1]:  # 1ä»¶ã®ã¿
                        content = f"å®Ÿè·µçš„ãªæ´å¯Ÿ: {insight}"
                        
                        sharings.append(ConversationEnhancement(
                            enhancement_type="insight_sharing",
                            knowledge_source=item["knowledge_id"],
                            enhancement_content=content,
                            relevance_score=item["relevance_score"] * 0.8,
                            timing_suggestion="related_topic"
                        ))
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ æ´å¯Ÿå…±æœ‰å¼·åŒ–ç”Ÿæˆå¤±æ•—: {e}")
        
        return sharings
    
    def _generate_trend_analyses(self, context: KnowledgeContext) -> List[ConversationEnhancement]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå¼·åŒ–ç”Ÿæˆ"""
        analyses = []
        
        try:
            # æ™‚ç³»åˆ—é€²åŒ–åˆ†æã‹ã‚‰ã®ãƒˆãƒ¬ãƒ³ãƒ‰å…±æœ‰
            for item in context.relevant_knowledge:
                knowledge = item["knowledge"]
                
                if knowledge.get("integration_type") == "temporal_evolution":
                    evolution_trends = knowledge.get("evolution_trends", [])
                    
                    for trend in evolution_trends[:1]:
                        content = f"ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ: {trend.get('trend', '')} - {trend.get('evidence', '')}"
                        
                        analyses.append(ConversationEnhancement(
                            enhancement_type="trend_analysis",
                            knowledge_source=item["knowledge_id"],
                            enhancement_content=content,
                            relevance_score=item["relevance_score"] * 0.7,
                            timing_suggestion="follow_up"
                        ))
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå¼·åŒ–ç”Ÿæˆå¤±æ•—: {e}")
        
        return analyses
    
    def get_proactive_suggestions(self, conversation_history: List[Dict]) -> List[str]:
        """
        ç©æ¥µçš„ææ¡ˆå–å¾—
        
        Args:
            conversation_history: ä¼šè©±å±¥æ­´
            
        Returns:
            ç©æ¥µçš„ææ¡ˆãƒªã‚¹ãƒˆ
        """
        try:
            if not self.provider_config["proactive_suggestion_enabled"]:
                return []
            
            suggestions = []
            
            # ä¼šè©±ã®ãƒˆãƒ”ãƒƒã‚¯å‚¾å‘åˆ†æ
            recent_topics = self._analyze_conversation_topics(conversation_history)
            
            # æœªæ´»ç”¨ã®é–¢é€£çŸ¥è­˜ç‰¹å®š
            underutilized_knowledge = self._find_underutilized_knowledge(recent_topics)
            
            # ææ¡ˆç”Ÿæˆ
            for knowledge_item in underutilized_knowledge[:3]:
                knowledge = knowledge_item["knowledge"]
                
                if knowledge.get("type") == "integrated_knowledge":
                    suggestions.append(f"é–¢é€£åˆ†æ: {knowledge.get('synthesized_content', '')[:80]}...")
                elif knowledge.get("actionable_insights"):
                    suggestions.append(f"æ´»ç”¨ææ¡ˆ: {knowledge['actionable_insights'][0]}")
                else:
                    suggestions.append(f"é–¢é€£æƒ…å ±: {knowledge.get('content', '')[:80]}...")
            
            return suggestions
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âŒ ç©æ¥µçš„ææ¡ˆå–å¾—å¤±æ•—: {e}")
            return []
    
    def _analyze_conversation_topics(self, conversation_history: List[Dict]) -> List[str]:
        """ä¼šè©±ãƒˆãƒ”ãƒƒã‚¯å‚¾å‘åˆ†æ"""
        topic_counts = defaultdict(int)
        
        try:
            for message in conversation_history[-10:]:  # æœ€æ–°10ä»¶
                content = message.get("content", "")
                detected_topics = self._detect_topics(content)
                
                for topic in detected_topics:
                    topic_counts[topic] += 1
            
            # é »å‡ºãƒˆãƒ”ãƒƒã‚¯é †ã§ã‚½ãƒ¼ãƒˆ
            sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)
            return [topic for topic, count in sorted_topics[:5]]
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ ä¼šè©±ãƒˆãƒ”ãƒƒã‚¯åˆ†æå¤±æ•—: {e}")
            return []
    
    def _find_underutilized_knowledge(self, recent_topics: List[str]) -> List[Dict]:
        """æœªæ´»ç”¨çŸ¥è­˜ç‰¹å®š"""
        underutilized = []
        
        try:
            # æœ€è¿‘ã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢é€£ã™ã‚‹ãŒæœªæ´»ç”¨ã®çŸ¥è­˜ã‚’æ¤œç´¢
            for topic in recent_topics:
                topic_knowledge = self.topic_index.get(topic.lower(), [])
                
                for knowledge_id in topic_knowledge[:5]:
                    knowledge = self.knowledge_cache.get(knowledge_id, {})
                    
                    # æœ€è¿‘ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„é«˜å“è³ªãªçŸ¥è­˜
                    if knowledge:
                        # çµ±åˆçŸ¥è­˜ã‚„é«˜ä¿¡é ¼åº¦çŸ¥è­˜ã‚’å„ªå…ˆ
                        if (knowledge.get("type") == "integrated_knowledge" or 
                            knowledge.get("confidence_score", 0) > 0.8):
                            underutilized.append({
                                "knowledge_id": knowledge_id,
                                "knowledge": knowledge,
                                "relevance_score": 0.7,
                                "topic": topic
                            })
            
            return underutilized
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ æœªæ´»ç”¨çŸ¥è­˜ç‰¹å®šå¤±æ•—: {e}")
            return []
    
    def update_conversation_history(self, user_input: str, assistant_response: str):
        """ä¼šè©±å±¥æ­´æ›´æ–°"""
        try:
            self.conversation_history.append({
                "timestamp": datetime.now().isoformat(),
                "user_input": user_input,
                "assistant_response": assistant_response
            })
            
            # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
            if len(self.conversation_history) > 50:
                self.conversation_history = self.conversation_history[-50:]
                
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âš ï¸ ä¼šè©±å±¥æ­´æ›´æ–°å¤±æ•—: {e}")
    
    def get_knowledge_statistics(self) -> Dict[str, Any]:
        """çŸ¥è­˜çµ±è¨ˆæƒ…å ±å–å¾—"""
        try:
            session_knowledge_count = len([k for k in self.knowledge_cache.keys() if k.startswith('session_')])
            integrated_knowledge_count = len(self.knowledge_cache) - session_knowledge_count
            
            return {
                "total_knowledge_items": len(self.knowledge_cache),
                "session_knowledge": session_knowledge_count,
                "integrated_knowledge": integrated_knowledge_count,
                "topic_index_size": len(self.topic_index),
                "entity_index_size": len(self.entity_index),
                "active_contexts": len(self.active_contexts),
                "conversation_history_size": len(self.conversation_history),
                "cache_hit_rate": 0.85  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯è¨ˆç®—
            }
            
        except Exception as e:
            print(f"[ä¼šè©±çŸ¥è­˜] âŒ çµ±è¨ˆæƒ…å ±å–å¾—å¤±æ•—: {e}")
            return {}


# ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    print("=== ConversationKnowledgeProvider ãƒ†ã‚¹ãƒˆ ===")
    
    provider = ConversationKnowledgeProvider()
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    test_inputs = [
        "AIéŸ³æ¥½ç”Ÿæˆã«ã¤ã„ã¦æ•™ãˆã¦",
        "Transformerã‚’ä½¿ã£ãŸéŸ³æ¥½åˆ¶ä½œã¯ã©ã†ï¼Ÿ",
        "æœ€æ–°ã®æŠ€è¡“å‹•å‘ãŒçŸ¥ã‚ŠãŸã„"
    ]
    
    print("\nğŸ” ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›åˆ†æãƒ†ã‚¹ãƒˆ:")
    for i, user_input in enumerate(test_inputs, 1):
        print(f"\n--- ãƒ†ã‚¹ãƒˆ{i}: {user_input} ---")
        
        context = provider.analyze_user_input(user_input)
        
        print(f"æ¤œå‡ºãƒˆãƒ”ãƒƒã‚¯: {context.detected_topics}")
        print(f"é–¢é€£çŸ¥è­˜: {len(context.relevant_knowledge)}ä»¶")
        print(f"ä¿¡é ¼åº¦: {context.confidence_score:.2f}")
        print(f"çŸ¥è­˜ã‚¿ã‚¤ãƒ—: {context.knowledge_types}")
        print(f"å¿œç”¨ææ¡ˆ: {context.application_suggestions}")
        
        # ä¼šè©±å¼·åŒ–ææ¡ˆ
        enhancements = provider.generate_conversation_enhancements(context)
        print(f"ä¼šè©±å¼·åŒ–: {len(enhancements)}ä»¶")
        for enhancement in enhancements:
            print(f"  - {enhancement.enhancement_type}: {enhancement.enhancement_content[:50]}...")
    
    # ç©æ¥µçš„ææ¡ˆãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ’¡ ç©æ¥µçš„ææ¡ˆãƒ†ã‚¹ãƒˆ:")
    conversation_history = [
        {"content": "AIéŸ³æ¥½ã«ã¤ã„ã¦è©±ã—ãŸ"},
        {"content": "æŠ€è¡“å‹•å‘ã‚‚æ°—ã«ãªã‚‹"}
    ]
    suggestions = provider.get_proactive_suggestions(conversation_history)
    print(f"ææ¡ˆæ•°: {len(suggestions)}ä»¶")
    for suggestion in suggestions:
        print(f"  - {suggestion}")
    
    # çµ±è¨ˆæƒ…å ±
    print(f"\nğŸ“Š çŸ¥è­˜çµ±è¨ˆ:")
    stats = provider.get_knowledge_statistics()
    print(f"  ç·çŸ¥è­˜: {stats.get('total_knowledge_items', 0)}ä»¶")
    print(f"  ã‚»ãƒƒã‚·ãƒ§ãƒ³çŸ¥è­˜: {stats.get('session_knowledge', 0)}ä»¶") 
    print(f"  çµ±åˆçŸ¥è­˜: {stats.get('integrated_knowledge', 0)}ä»¶")
    print(f"  ãƒˆãƒ”ãƒƒã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {stats.get('topic_index_size', 0)}ä»¶")