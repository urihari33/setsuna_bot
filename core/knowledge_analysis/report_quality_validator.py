#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ReportQualityValidator - ãƒ¬ãƒãƒ¼ãƒˆå“è³ªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã®å“è³ªãƒ»ä¸€è²«æ€§ãƒ»å®Œæ•´æ€§ã‚’è‡ªå‹•æ¤œè¨¼
"""

import json
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum

class ValidationSeverity(Enum):
    """æ¤œè¨¼å•é¡Œã®é‡è¦åº¦ãƒ¬ãƒ™ãƒ«"""
    INFO = "info"
    WARNING = "warning" 
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class ValidationIssue:
    """æ¤œè¨¼ã§ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ"""
    severity: ValidationSeverity
    category: str
    field: str
    message: str
    actual_value: Any = None
    expected_value: Any = None
    suggestion: str = ""

@dataclass
class ValidationReport:
    """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ"""
    validation_timestamp: str
    overall_score: float  # 0.0-1.0
    total_issues: int
    issues_by_severity: Dict[str, int] = field(default_factory=dict)
    issues: List[ValidationIssue] = field(default_factory=list)
    quality_metrics: Dict[str, float] = field(default_factory=dict)
    validation_summary: str = ""
    recommendations: List[str] = field(default_factory=list)

class ReportQualityValidator:
    """ãƒ¬ãƒãƒ¼ãƒˆå“è³ªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.validation_history = []
        
        # æ¤œè¨¼åŸºæº–å®šç¾©
        self.validation_criteria = {
            "required_fields": [
                "report_id", "timestamp", "user_prompt", "search_count",
                "analysis_summary", "key_insights", "categories", 
                "related_topics", "data_quality", "cost", "processing_time"
            ],
            "field_types": {
                "report_id": (int, "æ•´æ•°"),
                "timestamp": (str, "æ–‡å­—åˆ—ï¼ˆISOå½¢å¼ï¼‰"),
                "user_prompt": (str, "æ–‡å­—åˆ—"),
                "search_count": (int, "æ•´æ•°"),
                "analysis_summary": (str, "æ–‡å­—åˆ—"),
                "key_insights": (list, "ãƒªã‚¹ãƒˆ"),
                "categories": (dict, "è¾æ›¸"),
                "related_topics": (list, "ãƒªã‚¹ãƒˆ"),
                "data_quality": (float, "å°æ•°ï¼ˆ0.0-1.0ï¼‰"),
                "cost": (float, "å°æ•°"),
                "processing_time": (str, "æ–‡å­—åˆ—")
            },
            "value_ranges": {
                "data_quality": (0.0, 1.0),
                "cost": (0.0, 100.0),  # ä¸Šé™100ãƒ‰ãƒ«
                "search_count": (0, 1000)  # ä¸Šé™1000ä»¶
            },
            "content_quality": {
                "analysis_summary_min_length": 50,
                "analysis_summary_max_length": 10000,
                "key_insights_min_count": 1,
                "key_insights_max_count": 10,
                "related_topics_max_count": 8
            }
        }
    
    def validate_report(self, report: Dict) -> ValidationReport:
        """ãƒ¬ãƒãƒ¼ãƒˆå…¨ä½“ã®å“è³ªæ¤œè¨¼"""
        validation_start = datetime.now()
        issues = []
        
        # 1. æ§‹é€ æ¤œè¨¼
        structural_issues = self._validate_structure(report)
        issues.extend(structural_issues)
        
        # 2. ãƒ‡ãƒ¼ã‚¿å‹æ¤œè¨¼
        type_issues = self._validate_data_types(report)
        issues.extend(type_issues)
        
        # 3. å€¤ç¯„å›²æ¤œè¨¼
        range_issues = self._validate_value_ranges(report)
        issues.extend(range_issues)
        
        # 4. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªæ¤œè¨¼
        content_issues = self._validate_content_quality(report)
        issues.extend(content_issues)
        
        # 5. ä¸€è²«æ€§æ¤œè¨¼
        consistency_issues = self._validate_consistency(report)
        issues.extend(consistency_issues)
        
        # 6. æ¤œè¨¼çµæœé›†è¨ˆ
        validation_report = self._generate_validation_report(
            validation_start, issues, report
        )
        
        # å±¥æ­´ã«è¿½åŠ 
        self.validation_history.append(validation_report)
        
        return validation_report
    
    def _validate_structure(self, report: Dict) -> List[ValidationIssue]:
        """æ§‹é€ æ¤œè¨¼ - å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª"""
        issues = []
        
        for required_field in self.validation_criteria["required_fields"]:
            if required_field not in report:
                issues.append(ValidationIssue(
                    severity=ValidationSeverity.CRITICAL,
                    category="structure",
                    field=required_field,
                    message=f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ '{required_field}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“",
                    suggestion=f"'{required_field}' ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„"
                ))
            elif report[required_field] is None:
                issues.append(ValidationIssue(
                    severity=ValidationSeverity.ERROR,
                    category="structure", 
                    field=required_field,
                    message=f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ '{required_field}' ãŒNullã§ã™",
                    actual_value=None,
                    suggestion=f"'{required_field}' ã«é©åˆ‡ãªå€¤ã‚’è¨­å®šã—ã¦ãã ã•ã„"
                ))
        
        return issues
    
    def _validate_data_types(self, report: Dict) -> List[ValidationIssue]:
        """ãƒ‡ãƒ¼ã‚¿å‹æ¤œè¨¼"""
        issues = []
        
        for field, (expected_type, type_name) in self.validation_criteria["field_types"].items():
            if field in report and report[field] is not None:
                actual_value = report[field]
                if not isinstance(actual_value, expected_type):
                    # ã‚³ã‚¢ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å‹ã‚¨ãƒ©ãƒ¼ã¯é‡å¤§ãªå•é¡Œã¨ã™ã‚‹
                    severity = ValidationSeverity.CRITICAL if field in ["report_id", "timestamp", "data_quality"] else ValidationSeverity.ERROR
                    
                    issues.append(ValidationIssue(
                        severity=severity,
                        category="data_type",
                        field=field,
                        message=f"'{field}' ã®ãƒ‡ãƒ¼ã‚¿å‹ãŒä¸æ­£ã§ã™",
                        actual_value=f"{type(actual_value).__name__}: {actual_value}",
                        expected_value=type_name,
                        suggestion=f"'{field}' ã‚’ {type_name} å‹ã«å¤‰æ›´ã—ã¦ãã ã•ã„"
                    ))
        
        return issues
    
    def _validate_value_ranges(self, report: Dict) -> List[ValidationIssue]:
        """å€¤ç¯„å›²æ¤œè¨¼"""
        issues = []
        
        for field, (min_val, max_val) in self.validation_criteria["value_ranges"].items():
            if field in report and report[field] is not None:
                actual_value = report[field]
                
                # æ•°å€¤å‹ãƒã‚§ãƒƒã‚¯
                if isinstance(actual_value, (int, float)):
                    if actual_value < min_val:
                        # è² ã®å€¤ã‚„data_qualityã®ç¯„å›²å¤–ã¯é‡å¤§ãªå•é¡Œ
                        severity = ValidationSeverity.CRITICAL if field == "data_quality" or actual_value < 0 else ValidationSeverity.WARNING
                        issues.append(ValidationIssue(
                            severity=severity,
                            category="value_range",
                            field=field,
                            message=f"'{field}' ã®å€¤ãŒæœ€å°å€¤ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™",
                            actual_value=actual_value,
                            expected_value=f">= {min_val}",
                            suggestion=f"'{field}' ã‚’ {min_val} ä»¥ä¸Šã«è¨­å®šã—ã¦ãã ã•ã„"
                        ))
                    elif actual_value > max_val:
                        # æ¥µç«¯ã«å¤§ããªå€¤ã¯é‡å¤§ãªå•é¡Œ
                        severity = ValidationSeverity.CRITICAL if actual_value > max_val * 10 else ValidationSeverity.WARNING
                        issues.append(ValidationIssue(
                            severity=severity,
                            category="value_range", 
                            field=field,
                            message=f"'{field}' ã®å€¤ãŒæœ€å¤§å€¤ã‚’ä¸Šå›ã£ã¦ã„ã¾ã™",
                            actual_value=actual_value,
                            expected_value=f"<= {max_val}",
                            suggestion=f"'{field}' ã‚’ {max_val} ä»¥ä¸‹ã«è¨­å®šã—ã¦ãã ã•ã„"
                        ))
        
        return issues
    
    def _validate_content_quality(self, report: Dict) -> List[ValidationIssue]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªæ¤œè¨¼"""
        issues = []
        criteria = self.validation_criteria["content_quality"]
        
        # analysis_summary é•·ã•ãƒã‚§ãƒƒã‚¯
        if "analysis_summary" in report:
            summary = report["analysis_summary"]
            if isinstance(summary, str):
                length = len(summary)
                if length < criteria["analysis_summary_min_length"]:
                    issues.append(ValidationIssue(
                        severity=ValidationSeverity.WARNING,
                        category="content_quality",
                        field="analysis_summary",
                        message="åˆ†æè¦ç´„ãŒçŸ­ã™ãã¾ã™",
                        actual_value=f"{length}æ–‡å­—",
                        expected_value=f">= {criteria['analysis_summary_min_length']}æ–‡å­—",
                        suggestion="ã‚ˆã‚Šè©³ç´°ãªåˆ†æè¦ç´„ã‚’æä¾›ã—ã¦ãã ã•ã„"
                    ))
                elif length > criteria["analysis_summary_max_length"]:
                    issues.append(ValidationIssue(
                        severity=ValidationSeverity.INFO,
                        category="content_quality",
                        field="analysis_summary",
                        message="åˆ†æè¦ç´„ãŒé•·ã™ãã¾ã™",
                        actual_value=f"{length}æ–‡å­—",
                        expected_value=f"<= {criteria['analysis_summary_max_length']}æ–‡å­—",
                        suggestion="åˆ†æè¦ç´„ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
                    ))
                
                # ç©ºç™½æ–‡å­—ã®ã¿ãƒã‚§ãƒƒã‚¯
                if summary.strip() == "":
                    issues.append(ValidationIssue(
                        severity=ValidationSeverity.ERROR,
                        category="content_quality",
                        field="analysis_summary",
                        message="åˆ†æè¦ç´„ãŒç©ºç™½ã§ã™",
                        suggestion="æ„å‘³ã®ã‚ã‚‹åˆ†æè¦ç´„ã‚’æä¾›ã—ã¦ãã ã•ã„"
                    ))
        
        # key_insights å“è³ªãƒã‚§ãƒƒã‚¯
        if "key_insights" in report:
            insights = report["key_insights"]
            if isinstance(insights, list):
                insight_count = len(insights)
                
                if insight_count < criteria["key_insights_min_count"]:
                    issues.append(ValidationIssue(
                        severity=ValidationSeverity.WARNING,
                        category="content_quality",
                        field="key_insights",
                        message="ä¸»è¦ç™ºè¦‹äº‹é …ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                        actual_value=f"{insight_count}ä»¶",
                        expected_value=f">= {criteria['key_insights_min_count']}ä»¶",
                        suggestion="ã‚ˆã‚Šå¤šãã®ä¸»è¦ç™ºè¦‹äº‹é …ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„"
                    ))
                elif insight_count > criteria["key_insights_max_count"]:
                    issues.append(ValidationIssue(
                        severity=ValidationSeverity.INFO,
                        category="content_quality",
                        field="key_insights",
                        message="ä¸»è¦ç™ºè¦‹äº‹é …ãŒå¤šã™ãã¾ã™",
                        actual_value=f"{insight_count}ä»¶",
                        expected_value=f"<= {criteria['key_insights_max_count']}ä»¶",
                        suggestion="æœ€ã‚‚é‡è¦ãªç™ºè¦‹äº‹é …ã«çµã‚Šè¾¼ã‚“ã§ãã ã•ã„"
                    ))
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                unique_insights = set(insights)
                if len(unique_insights) < len(insights):
                    duplicate_count = len(insights) - len(unique_insights)
                    issues.append(ValidationIssue(
                        severity=ValidationSeverity.WARNING,
                        category="content_quality",
                        field="key_insights",
                        message="ä¸»è¦ç™ºè¦‹äº‹é …ã«é‡è¤‡ãŒã‚ã‚Šã¾ã™",
                        actual_value=f"{duplicate_count}ä»¶ã®é‡è¤‡",
                        suggestion="é‡è¤‡ã™ã‚‹ç™ºè¦‹äº‹é …ã‚’é™¤å»ã—ã¦ãã ã•ã„"
                    ))
                
                # ç©ºã®ç™ºè¦‹äº‹é …ãƒã‚§ãƒƒã‚¯
                empty_insights = [i for i, insight in enumerate(insights) if not str(insight).strip()]
                if empty_insights:
                    issues.append(ValidationIssue(
                        severity=ValidationSeverity.WARNING,
                        category="content_quality",
                        field="key_insights",
                        message="ç©ºã®ç™ºè¦‹äº‹é …ãŒã‚ã‚Šã¾ã™",
                        actual_value=f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {empty_insights}",
                        suggestion="ç©ºã®ç™ºè¦‹äº‹é …ã‚’é™¤å»ã—ã¦ãã ã•ã„"
                    ))
        
        # related_topics æ•°é‡ãƒã‚§ãƒƒã‚¯
        if "related_topics" in report:
            topics = report["related_topics"]
            if isinstance(topics, list):
                topic_count = len(topics)
                if topic_count > criteria["related_topics_max_count"]:
                    issues.append(ValidationIssue(
                        severity=ValidationSeverity.INFO,
                        category="content_quality",
                        field="related_topics",
                        message="é–¢é€£ãƒˆãƒ”ãƒƒã‚¯æ•°ãŒå¤šã™ãã¾ã™",
                        actual_value=f"{topic_count}ä»¶",
                        expected_value=f"<= {criteria['related_topics_max_count']}ä»¶",
                        suggestion="æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒˆãƒ”ãƒƒã‚¯ã«çµã‚Šè¾¼ã‚“ã§ãã ã•ã„"
                    ))
        
        return issues
    
    def _validate_consistency(self, report: Dict) -> List[ValidationIssue]:
        """ä¸€è²«æ€§æ¤œè¨¼"""
        issues = []
        
        # search_count vs detailed_data.search_results ã®æ•´åˆæ€§
        if ("search_count" in report and "detailed_data" in report and 
            isinstance(report["detailed_data"], dict)):
            
            reported_count = report.get("search_count", 0)
            search_results = report["detailed_data"].get("search_results", [])
            actual_count = len(search_results) if isinstance(search_results, list) else 0
            
            if reported_count != actual_count:
                issues.append(ValidationIssue(
                    severity=ValidationSeverity.ERROR,
                    category="consistency",
                    field="search_count",
                    message="æ¤œç´¢ä»¶æ•°ã¨å®Ÿéš›ã®æ¤œç´¢çµæœæ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“",
                    actual_value=f"å ±å‘Š: {reported_count}, å®Ÿéš›: {actual_count}",
                    suggestion="search_count ã‚’å®Ÿéš›ã®æ¤œç´¢çµæœæ•°ã«åˆã‚ã›ã¦ãã ã•ã„"
                ))
        
        # data_quality vs search_count ã®å¦¥å½“æ€§
        if "data_quality" in report and "search_count" in report:
            quality = report.get("data_quality", 0.0)
            search_count = report.get("search_count", 0)
            
            # æ¤œç´¢çµæœãŒãªã„ã®ã«å“è³ªã‚¹ã‚³ã‚¢ãŒé«˜ã„å ´åˆ
            if search_count == 0 and quality > 0.1:
                issues.append(ValidationIssue(
                    severity=ValidationSeverity.WARNING,
                    category="consistency",
                    field="data_quality",
                    message="æ¤œç´¢çµæœãŒãªã„ã®ã«å“è³ªã‚¹ã‚³ã‚¢ãŒé«˜ã™ãã¾ã™",
                    actual_value=f"å“è³ª: {quality}, æ¤œç´¢æ•°: {search_count}",
                    suggestion="æ¤œç´¢çµæœãŒãªã„å ´åˆã¯å“è³ªã‚¹ã‚³ã‚¢ã‚’ä½ãè¨­å®šã—ã¦ãã ã•ã„"
                ))
            
            # æ¤œç´¢çµæœãŒå¤šã„ã®ã«å“è³ªã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆ
            elif search_count > 50 and quality < 0.3:
                issues.append(ValidationIssue(
                    severity=ValidationSeverity.INFO,
                    category="consistency",
                    field="data_quality",
                    message="æ¤œç´¢çµæœãŒå¤šã„ã®ã«å“è³ªã‚¹ã‚³ã‚¢ãŒä½ã„ã§ã™",
                    actual_value=f"å“è³ª: {quality}, æ¤œç´¢æ•°: {search_count}",
                    suggestion="æ¤œç´¢çµæœã®å“è³ªè¨ˆç®—ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
                ))
        
        # timestamp ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œè¨¼
        if "timestamp" in report:
            timestamp = report["timestamp"]
            if isinstance(timestamp, str):
                try:
                    datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                except ValueError:
                    issues.append(ValidationIssue(
                        severity=ValidationSeverity.WARNING,
                        category="consistency",
                        field="timestamp",
                        message="ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å½¢å¼ãŒä¸æ­£ã§ã™",
                        actual_value=timestamp,
                        expected_value="ISO 8601å½¢å¼",
                        suggestion="æ­£ã—ã„ISO 8601å½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„"
                    ))
        
        return issues
    
    def _generate_validation_report(self, start_time: datetime, issues: List[ValidationIssue], report: Dict) -> ValidationReport:
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        validation_end = datetime.now()
        
        # é‡è¦åº¦åˆ¥é›†è¨ˆ
        issues_by_severity = {
            "critical": 0,
            "error": 0, 
            "warning": 0,
            "info": 0
        }
        
        for issue in issues:
            issues_by_severity[issue.severity.value] += 1
        
        # å…¨ä½“å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆé‡è¦åº¦ã«åŸºã¥ãæ¸›ç‚¹æ–¹å¼ï¼‰
        base_score = 1.0
        score_penalties = {
            "critical": 0.3,
            "error": 0.1,
            "warning": 0.05,
            "info": 0.01
        }
        
        for severity, count in issues_by_severity.items():
            base_score -= count * score_penalties[severity]
        
        overall_score = max(0.0, min(1.0, base_score))
        
        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        quality_metrics = self._calculate_quality_metrics(report, issues)
        
        # æ¤œè¨¼ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        total_issues = len(issues)
        critical_issues = issues_by_severity["critical"]
        error_issues = issues_by_severity["error"]
        
        if critical_issues > 0:
            summary = f"é‡å¤§ãªå•é¡ŒãŒ{critical_issues}ä»¶æ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ã§ã™ã€‚"
        elif error_issues > 0:
            summary = f"ã‚¨ãƒ©ãƒ¼ãŒ{error_issues}ä»¶æ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ä¿®æ­£ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
        elif total_issues > 0:
            summary = f"è»½å¾®ãªå•é¡ŒãŒ{total_issues}ä»¶æ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å“è³ªå‘ä¸Šã®ãŸã‚ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        else:
            summary = "æ¤œè¨¼å®Œäº†ï¼šå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = self._generate_recommendations(issues, quality_metrics)
        
        return ValidationReport(
            validation_timestamp=validation_end.isoformat(),
            overall_score=overall_score,
            total_issues=total_issues,
            issues_by_severity=issues_by_severity,
            issues=issues,
            quality_metrics=quality_metrics,
            validation_summary=summary,
            recommendations=recommendations
        )
    
    def _calculate_quality_metrics(self, report: Dict, issues: List[ValidationIssue]) -> Dict[str, float]:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        metrics = {}
        
        # æ§‹é€ å®Œæ•´æ€§ã‚¹ã‚³ã‚¢
        required_fields = self.validation_criteria["required_fields"]
        present_fields = sum(1 for field in required_fields if field in report)
        metrics["structural_completeness"] = present_fields / len(required_fields)
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å……å®Ÿåº¦ã‚¹ã‚³ã‚¢
        content_score = 0.0
        if "analysis_summary" in report:
            summary_length = len(str(report["analysis_summary"]))
            content_score += min(1.0, summary_length / 500)  # 500æ–‡å­—ã§æº€ç‚¹
        
        if "key_insights" in report and isinstance(report["key_insights"], list):
            insights_count = len(report["key_insights"])
            content_score += min(1.0, insights_count / 5)  # 5å€‹ã§æº€ç‚¹
        
        metrics["content_richness"] = content_score / 2  # 2é …ç›®ã®å¹³å‡
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªä¿¡é ¼åº¦
        if "data_quality" in report:
            reported_quality = report["data_quality"]
            search_count = report.get("search_count", 0)
            
            # æ¤œç´¢æ•°ã¨å“è³ªã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            expected_quality = min(0.95, 0.5 + (search_count / 200))
            quality_deviation = abs(reported_quality - expected_quality)
            metrics["quality_reliability"] = max(0.0, 1.0 - quality_deviation * 2)
        else:
            metrics["quality_reliability"] = 0.0
        
        # ã‚¨ãƒ©ãƒ¼å¯†åº¦ï¼ˆå•é¡Œæ•°ã®é€†æ•°ï¼‰
        total_fields = len(report)
        error_density = len(issues) / total_fields if total_fields > 0 else 1.0
        metrics["error_density"] = max(0.0, 1.0 - error_density)
        
        return metrics
    
    def _generate_recommendations(self, issues: List[ValidationIssue], metrics: Dict[str, float]) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # é‡è¦åº¦ã®é«˜ã„å•é¡Œã¸ã®å¯¾å¿œ
        critical_issues = [issue for issue in issues if issue.severity == ValidationSeverity.CRITICAL]
        if critical_issues:
            recommendations.append("é‡å¤§ãªå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            for issue in critical_issues[:3]:  # æœ€å¤§3ä»¶
                recommendations.append(f"- {issue.message} ({issue.field})")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åŸºæº–ã®æ¨å¥¨äº‹é …
        if metrics.get("structural_completeness", 1.0) < 0.9:
            recommendations.append("å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ãƒ¬ãƒãƒ¼ãƒˆæ§‹é€ ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        if metrics.get("content_richness", 1.0) < 0.5:
            recommendations.append("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªåˆ†æçµæœã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        
        if metrics.get("quality_reliability", 1.0) < 0.7:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å“è³ªè©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        if metrics.get("error_density", 1.0) < 0.8:
            recommendations.append("å¤šãã®æ¤œè¨¼å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã®è¦‹ç›´ã—ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        
        # ä¸€èˆ¬çš„ãªæ¨å¥¨äº‹é …
        warning_issues = [issue for issue in issues if issue.severity == ValidationSeverity.WARNING]
        if len(warning_issues) > 5:
            recommendations.append("è­¦å‘ŠãŒå¤šæ•°æ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å“è³ªå‘ä¸Šã®ãŸã‚æ®µéšçš„ãªæ”¹å–„ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        
        return recommendations
    
    def get_validation_summary(self) -> Dict:
        """æ¤œè¨¼å±¥æ­´ã®è¦ç´„çµ±è¨ˆ"""
        if not self.validation_history:
            return {"message": "æ¤œè¨¼å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        recent_validations = self.validation_history[-10:]  # æœ€æ–°10ä»¶
        
        avg_score = sum(v.overall_score for v in recent_validations) / len(recent_validations)
        total_issues = sum(v.total_issues for v in recent_validations)
        
        return {
            "total_validations": len(self.validation_history),
            "recent_validations": len(recent_validations),
            "average_quality_score": avg_score,
            "total_issues_found": total_issues,
            "latest_validation": recent_validations[-1].validation_timestamp,
            "trend": "improving" if len(recent_validations) >= 2 and recent_validations[-1].overall_score > recent_validations[-2].overall_score else "stable"
        }
    
    def format_validation_report(self, validation_report: ValidationReport) -> str:
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        lines = []
        
        lines.append("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆå“è³ªæ¤œè¨¼çµæœ")
        lines.append("=" * 50)
        lines.append(f"ğŸ• æ¤œè¨¼æ™‚åˆ»: {validation_report.validation_timestamp}")
        lines.append(f"ğŸ† ç·åˆã‚¹ã‚³ã‚¢: {validation_report.overall_score:.2f} ({validation_report.overall_score * 100:.1f}%)")
        lines.append(f"ğŸ“‹ æ¤œå‡ºå•é¡Œæ•°: {validation_report.total_issues}ä»¶")
        lines.append("")
        
        # é‡è¦åº¦åˆ¥ã‚µãƒãƒªãƒ¼
        if validation_report.total_issues > 0:
            lines.append("ğŸ“ˆ å•é¡Œå†…è¨³:")
            for severity, count in validation_report.issues_by_severity.items():
                if count > 0:
                    emoji = {"critical": "ğŸ”´", "error": "âŒ", "warning": "âš ï¸", "info": "â„¹ï¸"}[severity]
                    lines.append(f"   {emoji} {severity.upper()}: {count}ä»¶")
            lines.append("")
        
        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        lines.append("ğŸ“Š å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        for metric, value in validation_report.quality_metrics.items():
            lines.append(f"   {metric}: {value:.2f} ({value * 100:.1f}%)")
        lines.append("")
        
        # æ¤œè¨¼ã‚µãƒãƒªãƒ¼
        lines.append("ğŸ“ æ¤œè¨¼ã‚µãƒãƒªãƒ¼:")
        lines.append(f"   {validation_report.validation_summary}")
        lines.append("")
        
        # æ¨å¥¨äº‹é …
        if validation_report.recommendations:
            lines.append("ğŸ’¡ æ¨å¥¨äº‹é …:")
            for rec in validation_report.recommendations:
                lines.append(f"   â€¢ {rec}")
            lines.append("")
        
        # é‡è¦ãªå•é¡Œã®è©³ç´°
        critical_and_error_issues = [
            issue for issue in validation_report.issues 
            if issue.severity in [ValidationSeverity.CRITICAL, ValidationSeverity.ERROR]
        ]
        
        if critical_and_error_issues:
            lines.append("ğŸ” é‡è¦ãªå•é¡Œè©³ç´°:")
            for issue in critical_and_error_issues[:5]:  # æœ€å¤§5ä»¶
                severity_emoji = {"critical": "ğŸ”´", "error": "âŒ"}[issue.severity.value]
                lines.append(f"   {severity_emoji} [{issue.field}] {issue.message}")
                if issue.suggestion:
                    lines.append(f"      ğŸ’¡ {issue.suggestion}")
            lines.append("")
        
        return "\n".join(lines)

if __name__ == "__main__":
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    validator = ReportQualityValidator()
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    test_report = {
        "report_id": 1,
        "timestamp": "2025-07-13T20:30:00.000000",
        "user_prompt": "ã›ã¤ãªã®éŸ³æ¥½æ´»å‹•ã«ã¤ã„ã¦",
        "search_count": 10,
        "analysis_summary": "VTuberã¨ã—ã¦ã®éŸ³æ¥½æ´»å‹•ã«é–¢ã™ã‚‹è©³ç´°ãªåˆ†æçµæœã€‚ç‹¬è‡ªã®æ¥½æ›²åˆ¶ä½œã¨é…ä¿¡æ´»å‹•ã®ç‰¹å¾´ã«ã¤ã„ã¦ã€‚",
        "key_insights": [
            "ã‚ªãƒªã‚¸ãƒŠãƒ«æ¥½æ›²ã®åˆ¶ä½œã«æ³¨åŠ›ã—ã¦ã„ã‚‹",
            "è¦–è´è€…ã¨ã®éŸ³æ¥½ã‚’é€šã˜ãŸäº¤æµã‚’é‡è¦–ã—ã¦ã„ã‚‹",
            "ç‹¬ç‰¹ã®éŸ³æ¥½ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ç¢ºç«‹ã—ã¦ã„ã‚‹"
        ],
        "categories": {
            "music": "éŸ³æ¥½åˆ¶ä½œã«é–¢ã™ã‚‹æƒ…å ±",
            "streaming": "é…ä¿¡æ´»å‹•ã«ã¤ã„ã¦"
        },
        "related_topics": ["VTuberéŸ³æ¥½", "ã‚ªãƒªã‚¸ãƒŠãƒ«æ¥½æ›²", "éŸ³æ¥½é…ä¿¡"],
        "data_quality": 0.75,
        "cost": 0.05,
        "processing_time": "5åˆ†",
        "detailed_data": {
            "search_results": [{} for _ in range(10)]  # 10ä»¶ã®ãƒ€ãƒŸãƒ¼çµæœ
        }
    }
    
    print("ğŸ§ª ReportQualityValidator ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 60)
    
    # æ¤œè¨¼å®Ÿè¡Œ
    validation_result = validator.validate_report(test_report)
    
    # çµæœè¡¨ç¤º
    print(validator.format_validation_report(validation_result))
    
    # å±¥æ­´ã‚µãƒãƒªãƒ¼
    print("ğŸ“Š æ¤œè¨¼å±¥æ­´ã‚µãƒãƒªãƒ¼:")
    summary = validator.get_validation_summary()
    for key, value in summary.items():
        print(f"   {key}: {value}")