#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KnowledgeAnalysisEngine - ãƒ¬ãƒãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹æ®µéšåˆ†æã‚·ã‚¹ãƒ†ãƒ 
ã‚·ãƒ³ãƒ—ãƒ«ã§ç›´æ„Ÿçš„ãªçŸ¥è­˜åé›†ãƒ»åˆ†æã‚¨ãƒ³ã‚¸ãƒ³
"""

import json
import os
import sys
from typing import Dict, List, Optional, Callable
from datetime import datetime
from pathlib import Path
import uuid

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ç¢ºå®Ÿã«ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from core.adaptive_learning.duckduckgo_search_service import DuckDuckGoSearchService
    from core.adaptive_learning.gpt35_analysis_service import GPT35AnalysisService
    from core.adaptive_learning.accurate_cost_calculator import AccurateCostCalculator
    from core.knowledge_analysis.report_quality_validator import ReportQualityValidator
    from core.quality_monitoring.quality_history_manager import QualityHistoryManager
except ImportError:
    try:
        from core.adaptive_learning.duckduckgo_search_service import DuckDuckGoSearchService
        from core.adaptive_learning.gpt35_analysis_service import GPT35AnalysisService
        from core.adaptive_learning.accurate_cost_calculator import AccurateCostCalculator
        from core.knowledge_analysis.report_quality_validator import ReportQualityValidator
        from core.quality_monitoring.quality_history_manager import QualityHistoryManager
    except ImportError:
        print("âš ï¸ æ—¢å­˜ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å†åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬æ©Ÿèƒ½ã®ã¿ã§å®Ÿè£…ã—ã¾ã™ã€‚")
        DuckDuckGoSearchService = None
        GPT35AnalysisService = None
        AccurateCostCalculator = None
        ReportQualityValidator = None
        QualityHistoryManager = None

# ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
try:
    from logging_system import get_logger
    LOGGER_AVAILABLE = True
except ImportError:
    LOGGER_AVAILABLE = False
    print("âš ï¸ StructuredLoggerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬ãƒ­ã‚°ã®ã¿ã§å‹•ä½œã—ã¾ã™ã€‚")

class KnowledgeAnalysisEngine:
    """ãƒ¬ãƒãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹çŸ¥è­˜åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, progress_callback: Optional[Callable] = None):
        """åˆæœŸåŒ–"""
        self.progress_callback = progress_callback
        self.session_id = None
        self.reports = []
        self.total_cost = 0.0
        
        # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        if LOGGER_AVAILABLE:
            self.logger = get_logger()
            self.logger.info("knowledge_analysis", "init", "KnowledgeAnalysisEngineåˆæœŸåŒ–é–‹å§‹")
        else:
            self.logger = None
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.data_dir = Path("D:/setsuna_bot/knowledge_db")
        self.sessions_dir = self.data_dir / "sessions"
        self.summaries_dir = self.data_dir / "summaries"
        self.cache_dir = self.data_dir / "cache"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for dir_path in [self.data_dir, self.sessions_dir, self.summaries_dir, self.cache_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ– - è©³ç´°ãƒ­ã‚°ä»˜ã
        self._initialize_services_with_logging()
        
        # å“è³ªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        if ReportQualityValidator:
            self.quality_validator = ReportQualityValidator()
            if self.logger:
                self.logger.info("knowledge_analysis", "init", "ReportQualityValidatoråˆæœŸåŒ–æˆåŠŸ")
        else:
            self.quality_validator = None
            if self.logger:
                self.logger.warning("knowledge_analysis", "init", "ReportQualityValidatoråˆ©ç”¨ä¸å¯")
        
        # å“è³ªå±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            if QualityHistoryManager:
                self.quality_history_manager = QualityHistoryManager()
                if self.logger:
                    self.logger.info("knowledge_analysis", "init", "QualityHistoryManageråˆæœŸåŒ–æˆåŠŸ")
                print("[å“è³ªå±¥æ­´] âœ… QualityHistoryManagerçµ±åˆæˆåŠŸ")
            else:
                # ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ
                try:
                    from core.quality_monitoring.quality_history_manager import QualityHistoryManager as DirectQHM
                    self.quality_history_manager = DirectQHM()
                    if self.logger:
                        self.logger.info("knowledge_analysis", "init", "QualityHistoryManagerç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
                    print("[å“è³ªå±¥æ­´] âœ… QualityHistoryManagerç›´æ¥çµ±åˆæˆåŠŸ")
                except ImportError:
                    self.quality_history_manager = None
                    if self.logger:
                        self.logger.warning("knowledge_analysis", "init", "QualityHistoryManageråˆ©ç”¨ä¸å¯")
                    print("[å“è³ªå±¥æ­´] âš ï¸ QualityHistoryManageråˆ©ç”¨ä¸å¯")
        except Exception as e:
            self.quality_history_manager = None
            if self.logger:
                self.logger.error("knowledge_analysis", "init", f"QualityHistoryManageråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"[å“è³ªå±¥æ­´] âŒ QualityHistoryManageråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _initialize_services_with_logging(self):
        """ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ– - åŒ…æ‹¬çš„ãƒ­ã‚°è¨˜éŒ²"""
        initialization_log = {
            "stage": "initialization",
            "timestamp": datetime.now().isoformat(),
            "environment_check": {},
            "service_status": {},
            "failed_components": [],
            "fallback_activated": [],
            "critical_issues": []
        }
        
        # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
        openai_key_present = bool(os.environ.get('OPENAI_API_KEY'))
        initialization_log["environment_check"]["openai_api_key"] = "present" if openai_key_present else "missing"
        
        if not openai_key_present:
            initialization_log["critical_issues"].append("OPENAI_API_KEY not found in environment")
        
        # DuckDuckGoæ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
        try:
            if DuckDuckGoSearchService:
                self.search_service = DuckDuckGoSearchService()
                initialization_log["service_status"]["search_service"] = "initialized"
                if self.logger:
                    self.logger.info("knowledge_analysis", "init", "DuckDuckGoSearchServiceåˆæœŸåŒ–æˆåŠŸ")
            else:
                self.search_service = None
                initialization_log["service_status"]["search_service"] = "class_not_available"
                initialization_log["failed_components"].append("DuckDuckGoSearchService")
                
        except Exception as e:
            self.search_service = None
            initialization_log["service_status"]["search_service"] = f"failed: {str(e)}"
            initialization_log["failed_components"].append("DuckDuckGoSearchService")
            if self.logger:
                self.logger.error("knowledge_analysis", "init", f"DuckDuckGoSearchServiceåˆæœŸåŒ–å¤±æ•—: {e}")
        
        # GPTåˆ†æã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
        try:
            if GPT35AnalysisService:
                self.analysis_service = GPT35AnalysisService()
                initialization_log["service_status"]["analysis_service"] = "initialized"
                if self.logger:
                    self.logger.info("knowledge_analysis", "init", "GPT35AnalysisServiceåˆæœŸåŒ–æˆåŠŸ")
            else:
                self.analysis_service = None
                initialization_log["service_status"]["analysis_service"] = "class_not_available"
                initialization_log["failed_components"].append("GPT35AnalysisService")
                
        except Exception as e:
            self.analysis_service = None
            initialization_log["service_status"]["analysis_service"] = f"failed: {str(e)}"
            initialization_log["failed_components"].append("GPT35AnalysisService")
            if self.logger:
                self.logger.error("knowledge_analysis", "init", f"GPT35AnalysisServiceåˆæœŸåŒ–å¤±æ•—: {e}")
        
        # ã‚³ã‚¹ãƒˆè¨ˆç®—ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
        try:
            if AccurateCostCalculator:
                self.cost_calculator = AccurateCostCalculator()
                initialization_log["service_status"]["cost_calculator"] = "initialized"
                if self.logger:
                    self.logger.info("knowledge_analysis", "init", "AccurateCostCalculatoråˆæœŸåŒ–æˆåŠŸ")
            else:
                self.cost_calculator = None
                initialization_log["service_status"]["cost_calculator"] = "class_not_available"
                initialization_log["failed_components"].append("AccurateCostCalculator")
                
        except Exception as e:
            self.cost_calculator = None
            initialization_log["service_status"]["cost_calculator"] = f"failed: {str(e)}"
            initialization_log["failed_components"].append("AccurateCostCalculator")
            if self.logger:
                self.logger.error("knowledge_analysis", "init", f"AccurateCostCalculatoråˆæœŸåŒ–å¤±æ•—: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ã®è¨­å®š
        if not self.search_service:
            initialization_log["fallback_activated"].append("mock_search_results")
        if not self.analysis_service:
            initialization_log["fallback_activated"].append("mock_analysis")
        if not self.cost_calculator:
            initialization_log["fallback_activated"].append("estimated_cost")
        
        # åˆæœŸåŒ–çµæœã®ãƒ­ã‚°è¨˜éŒ²
        if self.logger:
            self.logger.info("knowledge_analysis", "init_complete", "ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–å®Œäº†", initialization_log)
        
        # é‡è¦ãªå•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚å‡ºåŠ›
        if initialization_log["critical_issues"] or len(initialization_log["failed_components"]) > 1:
            print(f"âš ï¸ åˆæœŸåŒ–è­¦å‘Š: {len(initialization_log['failed_components'])}å€‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒå¤±æ•—")
            for issue in initialization_log["critical_issues"]:
                print(f"   ğŸ”´ {issue}")
            for component in initialization_log["failed_components"]:
                print(f"   âŒ {component}")
            print(f"   ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {', '.join(initialization_log['fallback_activated'])}")
    
    def start_new_session(self, topic: str) -> str:
        """æ–°ã—ã„åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_uuid = str(uuid.uuid4())[:8]
        self.session_id = f"{topic.replace(' ', '_')[:20]}_{timestamp}_{session_uuid}"
        
        self.reports = []
        self.total_cost = 0.0
        
        self._update_progress(f"æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹: {topic}", 0)
        
        return self.session_id
    
    def analyze_topic(self, user_prompt: str, search_count: int = 100, use_previous_context: bool = True) -> Dict:
        """ãƒˆãƒ”ãƒƒã‚¯åˆ†æå®Ÿè¡Œ"""
        try:
            self._update_progress("åˆ†æé–‹å§‹...", 5)
            
            # å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆãŒã‚ã‚‹å ´åˆã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¿½åŠ 
            context = ""
            if use_previous_context and self.reports:
                last_report = self.reports[-1]
                context = f"å‰å›ã®åˆ†æçµæœ:\n{last_report['analysis_summary']}\n\né–¢é€£ãƒˆãƒ”ãƒƒã‚¯: {', '.join(last_report.get('related_topics', []))}\n\n"
            
            full_prompt = context + f"æ–°ã—ã„åˆ†æä¾é ¼: {user_prompt}"
            
            # å¤§é‡æ¤œç´¢å®Ÿè¡Œ
            self._update_progress("å¤§é‡æ¤œç´¢å®Ÿè¡Œä¸­...", 10)
            search_results = self._execute_large_scale_search(user_prompt, search_count)
            
            # ãƒãƒƒãƒåˆ†æå®Ÿè¡Œ
            self._update_progress("ãƒ‡ãƒ¼ã‚¿åˆ†æä¸­...", 40)
            analysis_result = self._execute_batch_analysis(search_results, full_prompt)
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self._update_progress("ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...", 80)
            report = self._generate_report(user_prompt, search_results, analysis_result)
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
            self.reports.append(report)
            self._save_session()
            
            self._update_progress("åˆ†æå®Œäº†", 100)
            
            return report
            
        except Exception as e:
            print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return self._generate_error_report(user_prompt, str(e))
    
    def _execute_large_scale_search(self, user_prompt: str, search_count: int) -> List[Dict]:
        """å¤§è¦æ¨¡æ¤œç´¢å®Ÿè¡Œ - è©³ç´°ãƒ­ã‚°ä»˜ã"""
        search_session_log = {
            "stage": "large_scale_search",
            "timestamp": datetime.now().isoformat(),
            "user_prompt": user_prompt,
            "target_count": search_count,
            "queries": [],
            "summary": {
                "total_queries": 0,
                "successful_searches": 0,
                "mock_fallbacks": 0,
                "actual_results": 0,
                "mock_results": 0,
                "total_time": 0
            }
        }
        
        search_start_time = datetime.now()
        
        if not self.search_service:
            search_session_log["summary"]["failed_searches"] = 1
            search_session_log["summary"]["actual_results"] = 0
            if self.logger:
                self.logger.warning("knowledge_analysis", "search", "æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹æœªåˆæœŸåŒ–ã€ç©ºã®çµæœã‚’è¿”ã™", search_session_log)
            return []  # å®Ÿãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆ
        
        try:
            # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ
            search_queries = self._generate_search_queries(user_prompt)
            search_session_log["summary"]["total_queries"] = len(search_queries)
            
            all_results = []
            results_per_query = max(5, search_count // len(search_queries))
            
            for i, query in enumerate(search_queries):
                query_log = {
                    "query": str(query),
                    "query_index": f"{i+1}/{len(search_queries)}",
                    "results_requested": results_per_query,
                    "results_received": 0,
                    "api_response_time": 0,
                    "status": "pending",
                    "error": None,
                    "actual_vs_mock": "unknown"
                }
                
                try:
                    # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                    query_str = str(query)
                    query_start_time = datetime.now()
                    
                    results = self.search_service.search(query_str, max_results=results_per_query)
                    
                    query_end_time = datetime.now()
                    query_log["api_response_time"] = (query_end_time - query_start_time).total_seconds()
                    query_log["results_received"] = len(results)
                    query_log["status"] = "success"
                    
                    # å®Ÿéš›ã®æ¤œç´¢çµæœã‹ã©ã†ã‹ã‚’åˆ¤å®š
                    if results:
                        query_log["actual_vs_mock"] = "actual"
                        search_session_log["summary"]["successful_searches"] += 1
                        search_session_log["summary"]["actual_results"] += len(results)
                    else:
                        query_log["actual_vs_mock"] = "empty"
                        search_session_log["summary"]["failed_searches"] = search_session_log["summary"].get("failed_searches", 0) + 1
                    
                    all_results.extend(results)
                    
                    progress = 10 + (20 * (i + 1) / len(search_queries))
                    self._update_progress(f"æ¤œç´¢ä¸­... ({len(all_results)}/{search_count})", int(progress))
                    
                    if len(all_results) >= search_count:
                        break
                        
                except Exception as e:
                    query_log["status"] = "failed"
                    query_log["error"] = str(e)
                    query_log["actual_vs_mock"] = "failed"
                    query_log["results_received"] = 0
                    
                    if self.logger:
                        self.logger.warning("knowledge_analysis", "search_query", f"æ¤œç´¢å¤±æ•—: {query_str}", data={"error": str(e)})
                    
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä½•ã‚‚è¿½åŠ ã—ãªã„ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
                    search_session_log["summary"]["failed_searches"] = search_session_log["summary"].get("failed_searches", 0) + 1
                    continue
                finally:
                    search_session_log["queries"].append(query_log)
            
            # æ¤œç´¢ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
            search_end_time = datetime.now()
            search_session_log["summary"]["total_time"] = (search_end_time - search_start_time).total_seconds()
            search_session_log["final_results_count"] = len(all_results)
            
            # å“è³ªè©•ä¾¡ã‚’è¿½åŠ 
            search_session_log["quality_assessment"] = self._assess_search_quality(all_results)
            
            # ãƒ­ã‚°è¨˜éŒ²
            if self.logger:
                self.logger.info("knowledge_analysis", "search_complete", "å¤§è¦æ¨¡æ¤œç´¢å®Œäº†", data=search_session_log)
            
            # é‡è¦ãªçµ±è¨ˆã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚å‡ºåŠ›ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
            failed_searches = search_session_log["summary"].get("failed_searches", 0)
            success_rate = (search_session_log["summary"]["successful_searches"] / search_session_log["summary"]["total_queries"]) * 100 if search_session_log["summary"]["total_queries"] > 0 else 0
            print(f"ğŸ” æ¤œç´¢å®Œäº†: {search_session_log['summary']['successful_searches']}/{search_session_log['summary']['total_queries']} ã‚¯ã‚¨ãƒªæˆåŠŸ ({success_rate:.1f}%)")
            print(f"ğŸ“Š çµæœ: å®Ÿéš›{search_session_log['summary']['actual_results']}ä»¶ï¼ˆå¤±æ•—{failed_searches}ä»¶ï¼‰")
            
            return all_results  # å–å¾—ã§ããŸã ã‘è¿”ã™
            
        except Exception as e:
            search_session_log["stage_error"] = str(e)
            search_session_log["summary"]["total_time"] = (datetime.now() - search_start_time).total_seconds()
            
            if self.logger:
                self.logger.error("knowledge_analysis", "search_critical", f"å¤§è¦æ¨¡æ¤œç´¢é‡å¤§ã‚¨ãƒ©ãƒ¼: {e}", data=search_session_log)
            
            print(f"âŒ å¤§è¦æ¨¡æ¤œç´¢ã‚¨ãƒ©ãƒ¼ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
            return []  # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç©ºãƒªã‚¹ãƒˆ
    
    def _is_real_search_result(self, result: Dict) -> bool:
        """æ¤œç´¢çµæœãŒå®Ÿéš›ã®ã‚‚ã®ã‹ãƒ¢ãƒƒã‚¯ã‹ã‚’åˆ¤å®š"""
        # URLã§åˆ¤å®šï¼ˆãƒ¢ãƒƒã‚¯çµæœã¯ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ï¼‰
        url = result.get('url', '')
        mock_domains = [
            'ai-trends.com', 'enterprise-ai.com', 'learning-resources.com',
            'community-trends.com', 'development-setup.com', 'webdev-guide.com',
            'best-practices.dev', 'ml-practical.com', 'industry-ml.com',
            'tech-comprehensive.com', 'implementation-patterns.com'
        ]
        
        # ãƒ¢ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ãƒ¢ãƒƒã‚¯çµæœ
        for domain in mock_domains:
            if domain in url:
                return False
        
        # DuckDuckGoã®å®Ÿéš›ã®çµæœãƒ‘ã‚¿ãƒ¼ãƒ³
        if 'duckduckgo.com' in result.get('source', ''):
            return True
        
        # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸé•·ã„URLã¯ãƒ¢ãƒƒã‚¯ã®å¯èƒ½æ€§ãŒé«˜ã„
        if len(url) > 200 and '%' in url:
            return False
        
        # å®Ÿåœ¨ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³
        real_domains = [
            'wikipedia.org', 'github.com', 'stackoverflow.com', 'arxiv.org',
            'medium.com', 'techcrunch.com', 'wired.com', 'nature.com',
            'ieee.org', 'acm.org', 'springer.com', 'sciencedirect.com'
        ]
        
        for domain in real_domains:
            if domain in url:
                return True
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å®Ÿéš›ã®çµæœã¨åˆ¤å®šï¼ˆä¿å®ˆçš„ï¼‰
        return True
    
    def _assess_search_quality(self, results: List[Dict]) -> Dict:
        """æ¤œç´¢çµæœã®å“è³ªè©•ä¾¡"""
        if not results:
            return {"quality_score": 0.0, "issues": ["no_results"]}
        
        quality_metrics = {
            "total_results": len(results),
            "real_results": 0,
            "mock_results": 0,
            "url_quality": {"valid": 0, "invalid": 0, "mock_domains": 0},
            "content_quality": {"meaningful_snippets": 0, "repetitive": 0},
            "language_detection": {"ja": 0, "en": 0, "other": 0},
            "domain_diversity": set(),
            "issues": []
        }
        
        for result in results:
            # å®Ÿéš› vs ãƒ¢ãƒƒã‚¯åˆ¤å®š
            if self._is_real_search_result(result):
                quality_metrics["real_results"] += 1
            else:
                quality_metrics["mock_results"] += 1
            
            # URLå“è³ªãƒã‚§ãƒƒã‚¯
            url = result.get('url', '')
            if url:
                if url.startswith('http'):
                    quality_metrics["url_quality"]["valid"] += 1
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³å¤šæ§˜æ€§
                    try:
                        from urllib.parse import urlparse
                        domain = urlparse(url).netloc
                        quality_metrics["domain_diversity"].add(domain)
                    except:
                        pass
                else:
                    quality_metrics["url_quality"]["invalid"] += 1
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªãƒã‚§ãƒƒã‚¯
            snippet = result.get('snippet', '')
            if snippet:
                # æ—¥æœ¬èªãƒ»è‹±èªåˆ¤å®šï¼ˆç°¡æ˜“ï¼‰
                if any('\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FFF' for char in snippet):
                    quality_metrics["language_detection"]["ja"] += 1
                elif any('a' <= char.lower() <= 'z' for char in snippet):
                    quality_metrics["language_detection"]["en"] += 1
                else:
                    quality_metrics["language_detection"]["other"] += 1
                
                # æ„å‘³ã®ã‚ã‚‹ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‹ãƒã‚§ãƒƒã‚¯
                if len(snippet) > 50 and 'ã€‚' in snippet:
                    quality_metrics["content_quality"]["meaningful_snippets"] += 1
                elif snippet.count('æŠ€è¡“') > 3 or snippet.count('åˆ†æ') > 3:
                    quality_metrics["content_quality"]["repetitive"] += 1
        
        # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-1ï¼‰
        real_ratio = quality_metrics["real_results"] / quality_metrics["total_results"]
        url_valid_ratio = quality_metrics["url_quality"]["valid"] / quality_metrics["total_results"]
        meaningful_ratio = quality_metrics["content_quality"]["meaningful_snippets"] / quality_metrics["total_results"]
        domain_diversity_score = min(1.0, len(quality_metrics["domain_diversity"]) / 10)
        
        quality_score = (real_ratio * 0.4 + url_valid_ratio * 0.3 + meaningful_ratio * 0.2 + domain_diversity_score * 0.1)
        
        # å•é¡Œã®ç‰¹å®š
        if quality_metrics["mock_results"] > quality_metrics["real_results"]:
            quality_metrics["issues"].append("high_mock_ratio")
        if quality_metrics["url_quality"]["invalid"] > quality_metrics["total_results"] * 0.3:
            quality_metrics["issues"].append("many_invalid_urls")
        if quality_metrics["content_quality"]["repetitive"] > quality_metrics["total_results"] * 0.5:
            quality_metrics["issues"].append("repetitive_content")
        if len(quality_metrics["domain_diversity"]) < 3:
            quality_metrics["issues"].append("low_domain_diversity")
        
        quality_metrics["quality_score"] = quality_score
        quality_metrics["domain_diversity"] = list(quality_metrics["domain_diversity"])  # setã‚’listã«å¤‰æ›
        
        return quality_metrics
    
    def _generate_search_queries(self, user_prompt: str) -> List[str]:
        """æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ãå‹•çš„ç”Ÿæˆ"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ­£è¦åŒ–ã¨åˆ†æ
        prompt_clean = user_prompt.strip()
        words = prompt_clean.split()
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¤åˆ¥
        context = self._detect_prompt_context(prompt_clean)
        
        # åŸºæœ¬ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆ
        queries = []
        
        if context == "person":
            # äººç‰©ã«é–¢ã™ã‚‹æ¤œç´¢ - å›ºæœ‰åè©ã‚’é¿ã‘ã¦ä¸€èˆ¬çš„ã‚«ãƒ†ã‚´ãƒªã§æ¤œç´¢
            person_categories = self._extract_person_categories(prompt_clean)
            prompt_keywords = self._extract_activity_keywords(prompt_clean)
            
            # ã‚«ãƒ†ã‚´ãƒªÃ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã¿åˆã‚ã›ã§ã‚¯ã‚¨ãƒªç”Ÿæˆ
            for category in person_categories:
                for keyword in prompt_keywords:
                    queries.append(f"{category} {keyword}")
            
            # åŸºæœ¬çš„ãªã‚«ãƒ†ã‚´ãƒªå˜ä½“ã‚¯ã‚¨ãƒªã‚‚è¿½åŠ 
            queries.extend(person_categories)
        elif context == "technology":
            # æŠ€è¡“é–¢é€£ã®æ¤œç´¢
            main_topic = self._extract_main_topic(prompt_clean)
            queries.extend([
                f"{main_topic}",
                f"{main_topic} æœ€æ–°å‹•å‘",
                f"{main_topic} æŠ€è¡“",
                f"{main_topic} å®Ÿè£…",
                f"{main_topic} äº‹ä¾‹",
                f"{main_topic} å°å…¥",
                f"{main_topic} èª²é¡Œ",
                f"{main_topic} å°†æ¥æ€§",
                f"{main_topic} æ¯”è¼ƒ",
                f"{main_topic} åŠ¹æœ"
            ])
        elif context == "general":
            # ä¸€èˆ¬çš„ãªãƒˆãƒ”ãƒƒã‚¯
            main_topic = self._extract_main_topic(prompt_clean)
            queries.extend([
                f"{main_topic}",
                f"{main_topic} ã«ã¤ã„ã¦",
                f"{main_topic} è©³ç´°",
                f"{main_topic} æƒ…å ±",
                f"{main_topic} è§£èª¬",
                f"{main_topic} ç‰¹å¾´",
                f"{main_topic} æ¦‚è¦",
                f"{main_topic} åŸºæœ¬",
                f"{main_topic} å¿œç”¨",
                f"{main_topic} æœ€æ–°"
            ])
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãã®ã¾ã¾ä½¿ç”¨
            queries.extend([
                prompt_clean,
                f"{prompt_clean} è©³ç´°",
                f"{prompt_clean} æƒ…å ±",
                f"{prompt_clean} ã«ã¤ã„ã¦",
                f"{prompt_clean} è§£èª¬"
            ])
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’çµ„ã¿åˆã‚ã›
        if len(words) > 1:
            # 2èªçµ„ã¿åˆã‚ã›
            for i in range(len(words) - 1):
                two_word_phrase = " ".join(words[i:i+2])
                queries.append(two_word_phrase)
            
            # 3èªçµ„ã¿åˆã‚ã›ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã„å ´åˆï¼‰
            if len(words) >= 3:
                three_word_phrase = " ".join(words[:3])
                queries.append(three_word_phrase)
        
        # é‡è¤‡é™¤å»ã¨æœ€å¤§20ã‚¯ã‚¨ãƒªã«åˆ¶é™
        unique_queries = list(dict.fromkeys(queries))  # é †åºã‚’ä¿æŒã—ã¤ã¤é‡è¤‡é™¤å»
        return unique_queries[:20]
    
    def _detect_prompt_context(self, prompt: str) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¤åˆ¥"""
        prompt_lower = prompt.lower()
        
        # äººç‰©é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        person_keywords = ["ã›ã¤ãª", "ç‰‡ç„¡", "æ­Œæ‰‹", "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ", "éŸ³æ¥½å®¶", "ä½œæ›²å®¶", "ã‚·ãƒ³ã‚¬ãƒ¼"]
        if any(keyword in prompt_lower for keyword in person_keywords):
            return "person"
        
        # æŠ€è¡“é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰  
        tech_keywords = ["ai", "æŠ€è¡“", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°", "ã‚·ã‚¹ãƒ†ãƒ ", "é–‹ç™º", "api", "æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’"]
        if any(keyword in prompt_lower for keyword in tech_keywords):
            return "technology"
        
        return "general"
    
    def _extract_person_categories(self, prompt: str) -> List[str]:
        """äººç‰©ã®ä¸€èˆ¬çš„ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡ºï¼ˆå›ºæœ‰åè©ã‚’ä½¿ã‚ãªã„ï¼‰"""
        # ã€Œã›ã¤ãªã€é–¢é€£ã®ç‰¹åˆ¥å‡¦ç†ï¼šVTuber/æ­Œæ‰‹/æ˜ åƒã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã¨ã—ã¦ã®ã‚«ãƒ†ã‚´ãƒª
        if "ã›ã¤ãª" in prompt:
            return [
                "VTuber",
                "æ­Œæ‰‹", 
                "æ˜ åƒã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼",
                "ãƒãƒ¼ãƒãƒ£ãƒ«ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ",
                "ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¼éŸ³æ¥½",
                "ã‚ªãƒªã‚¸ãƒŠãƒ«æ¥½æ›²",
                "éŸ³æ¥½åˆ¶ä½œ",
                "å‹•ç”»åˆ¶ä½œ"
            ]
        
        # ãã®ä»–ã®äººç‰©ã®å ´åˆã®ä¸€èˆ¬çš„ã‚«ãƒ†ã‚´ãƒª
        general_categories = [
            "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ",
            "ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼", 
            "éŸ³æ¥½å®¶",
            "æ­Œæ‰‹",
            "ä½œæ›²å®¶",
            "æ˜ åƒåˆ¶ä½œ",
            "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼"
        ]
        
        return general_categories
    
    def _extract_activity_keywords(self, prompt: str) -> List[str]:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰æ´»å‹•ãƒ»åˆ†é‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        prompt_lower = prompt.lower()
        
        # éŸ³æ¥½é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        music_keywords = ["éŸ³æ¥½", "æ¥½æ›²", "æ­Œ", "ä½œæ›²", "ç·¨æ›²", "ãƒœãƒ¼ã‚«ãƒ«", "ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ¼"]
        # æ˜ åƒé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰  
        video_keywords = ["æ˜ åƒ", "å‹•ç”»", "åˆ¶ä½œ", "ç·¨é›†", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„", "ä½œå“"]
        # æ´»å‹•é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        activity_keywords = ["æ´»å‹•", "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«", "çµŒæ­´", "å®Ÿç¸¾", "ä½œå“", "æƒ…å ±"]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®š
        found_keywords = []
        for keyword in music_keywords + video_keywords + activity_keywords:
            if keyword in prompt:
                found_keywords.append(keyword)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        if not found_keywords:
            found_keywords = ["æ´»å‹•", "ä½œå“", "æƒ…å ±", "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«"]
        
        return found_keywords
    
    def _extract_main_topic(self, prompt: str) -> str:
        """ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º"""
        words = prompt.split()
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å»
        stop_words = ["ã«ã¤ã„ã¦", "ã«é–¢ã—ã¦", "ã‚’", "ã®", "ãŒ", "ã¯", "ã§", "ã¨", "ã‚„"]
        filtered_words = [word for word in words if word not in stop_words]
        
        if filtered_words:
            # æœ€åˆã®æ„å‘³ã®ã‚ã‚‹å˜èªã‚’è¿”ã™
            return filtered_words[0]
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return words[0] if words else prompt
    
    
    def _execute_batch_analysis(self, search_results: List[Dict], prompt: str) -> Dict:
        """ãƒãƒƒãƒåˆ†æå®Ÿè¡Œ - è©³ç´°ãƒ­ã‚°ä»˜ã"""
        analysis_session_log = {
            "stage": "batch_analysis",
            "timestamp": datetime.now().isoformat(),
            "input_prompt": prompt,
            "input_data_count": len(search_results),
            "batch_logs": [],
            "summary": {
                "total_batches": 0,
                "successful_batches": 0,
                "failed_batches": 0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cost": 0.0,
                "total_time": 0,
                "analysis_service_available": bool(self.analysis_service)
            }
        }
        
        analysis_start_time = datetime.now()
        
        # æ¤œç´¢çµæœãŒç©ºã®å ´åˆã®å‡¦ç†
        if not search_results:
            analysis_session_log["summary"]["no_data_reason"] = "empty_search_results"
            analysis_session_log["summary"]["total_time"] = (datetime.now() - analysis_start_time).total_seconds()
            
            if self.logger:
                self.logger.warning("knowledge_analysis", "analysis", "æ¤œç´¢çµæœãŒç©ºã®ãŸã‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—", data=analysis_session_log)
            
            return {
                "analysis": "æ¤œç´¢çµæœãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€åˆ†æã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
                "batch_summaries": [],
                "total_cost": 0.0,
                "processed_items": 0,
                "analysis_log": analysis_session_log,
                "empty_data": True
            }
        
        if not self.analysis_service:
            analysis_session_log["fallback_reason"] = "analysis_service_not_available"
            if self.logger:
                self.logger.warning("knowledge_analysis", "analysis", "åˆ†æã‚µãƒ¼ãƒ“ã‚¹æœªåˆæœŸåŒ–ã€åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—", data=analysis_session_log)
            return {
                "analysis": "åˆ†æã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€åˆ†æã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
                "batch_summaries": [],
                "total_cost": 0.0,
                "processed_items": len(search_results),
                "analysis_log": analysis_session_log,
                "service_unavailable": True
            }
        
        try:
            batch_size = 10
            batch_summaries = []
            total_cost = 0.0
            
            analysis_session_log["summary"]["total_batches"] = (len(search_results) + batch_size - 1) // batch_size
            
            # 10ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†
            for i in range(0, len(search_results), batch_size):
                batch = search_results[i:i + batch_size]
                batch_number = i // batch_size + 1
                
                batch_log = {
                    "batch_number": batch_number,
                    "batch_size": len(batch),
                    "start_time": datetime.now().isoformat(),
                    "input_token_count": 0,
                    "output_token_count": 0,
                    "api_response_time": 0,
                    "cost": 0.0,
                    "status": "pending",
                    "error": None
                }
                
                try:
                    # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¦‚ç®—ï¼ˆãƒãƒƒãƒå…¨ä½“ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚µã‚¤ã‚ºï¼‰
                    batch_content_size = sum(len(result.get('title', '') + result.get('snippet', '')) for result in batch)
                    batch_log["input_token_count"] = batch_content_size // 4  # æ¦‚ç®—
                    
                    # GPTåˆ†æå®Ÿè¡Œ - æ™‚é–“æ¸¬å®šï¼ˆæ¤œç´¢çµæœã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æï¼‰
                    batch_start_time = datetime.now()
                    batch_analysis = self.analysis_service.analyze_search_results(batch, prompt)
                    batch_end_time = datetime.now()
                    
                    batch_log["api_response_time"] = (batch_end_time - batch_start_time).total_seconds()
                    batch_log["output_token_count"] = len(batch_analysis.get('analysis', '')) // 4  # æ¦‚ç®—
                    batch_log["cost"] = batch_analysis.get('cost', 0.0)
                    batch_log["status"] = "success"
                    
                    batch_summaries.append(batch_analysis['analysis'])
                    total_cost += batch_analysis['cost']
                    
                    # ã‚µãƒãƒªãƒ¼çµ±è¨ˆæ›´æ–°
                    analysis_session_log["summary"]["successful_batches"] += 1
                    analysis_session_log["summary"]["total_input_tokens"] += batch_log["input_token_count"]
                    analysis_session_log["summary"]["total_output_tokens"] += batch_log["output_token_count"]
                    analysis_session_log["summary"]["total_cost"] += batch_log["cost"]
                    
                    if self.logger:
                        self.logger.info("knowledge_analysis", "batch_success", f"ãƒãƒƒãƒ{batch_number}åˆ†ææˆåŠŸ", data=batch_log)
                    
                except Exception as batch_error:
                    batch_log["status"] = "failed"
                    batch_log["error"] = str(batch_error)
                    analysis_session_log["summary"]["failed_batches"] += 1
                    
                    if self.logger:
                        self.logger.error("knowledge_analysis", "batch_failed", f"ãƒãƒƒãƒ{batch_number}åˆ†æå¤±æ•—", data=batch_log)
                    
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ¢ãƒƒã‚¯è¦ç´„ã‚’ç”Ÿæˆ
                    mock_summary = f"ãƒãƒƒãƒ{batch_number}ã®è¦ç´„ï¼ˆåˆ†æã‚¨ãƒ©ãƒ¼ã®ãŸã‚ç°¡æ˜“ç”Ÿæˆï¼‰: {len(batch)}ä»¶ã®æ¤œç´¢çµæœã‹ã‚‰æŠ€è¡“å‹•å‘ã€å¸‚å ´åˆ†æã€å®Ÿç”¨æ€§ã«é–¢ã™ã‚‹æƒ…å ±ã‚’å«ã‚€ã€‚"
                    batch_summaries.append(mock_summary)
                
                finally:
                    batch_log["end_time"] = datetime.now().isoformat()
                    analysis_session_log["batch_logs"].append(batch_log)
                
                progress = 40 + (30 * (i + batch_size) / len(search_results))
                self._update_progress(f"ãƒãƒƒãƒåˆ†æä¸­... ({batch_number}/{analysis_session_log['summary']['total_batches']})", int(progress))
            
            # å…¨ä½“çµ±åˆåˆ†æ - è©³ç´°ãƒ­ã‚°ä»˜ã
            integration_log = {
                "stage": "final_integration",
                "start_time": datetime.now().isoformat(),
                "input_batches": len(batch_summaries),
                "input_token_count": 0,
                "output_token_count": 0,
                "api_response_time": 0,
                "cost": 0.0,
                "status": "pending",
                "error": None
            }
            
            try:
                integration_prompt = f"""
ä»¥ä¸‹ã®ãƒãƒƒãƒè¦ç´„ã‚’çµ±åˆåˆ†æã—ã¦ãã ã•ã„ï¼š

{chr(10).join([f"ãƒãƒƒãƒ{i+1}: {summary}" for i, summary in enumerate(batch_summaries)])}

å…ƒã®è³ªå•: {prompt}

ä»¥ä¸‹ã®å½¢å¼ã§çµ±åˆåˆ†æçµæœã‚’æä¾›ï¼š
1. ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ï¼ˆ2-3è¡Œï¼‰
2. ä¸»è¦ç™ºè¦‹äº‹é …ï¼ˆ5-7å€‹ï¼‰
3. ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æï¼š
   - æŠ€è¡“é¢
   - å¸‚å ´ãƒ»ãƒ“ã‚¸ãƒã‚¹é¢
   - å®Ÿç”¨æ€§ãƒ»å¿œç”¨äº‹ä¾‹
   - èª²é¡Œãƒ»ãƒªã‚¹ã‚¯
4. é–¢é€£èª¿æŸ»ææ¡ˆï¼ˆ3-5å€‹ï¼‰
5. ä¿¡é ¼åº¦è©•ä¾¡ï¼ˆ1-10ï¼‰
"""
                
                integration_log["input_token_count"] = len(integration_prompt) // 4  # æ¦‚ç®—
                
                # çµ±åˆåˆ†æå®Ÿè¡Œ - æ™‚é–“æ¸¬å®šï¼ˆè¦ç´„çµ±åˆåˆ†æï¼‰
                integration_start_time = datetime.now()
                # çµ±åˆåˆ†æã§ã¯è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢çµæœã¨ã—ã¦æ‰±ã„åˆ†æ
                summary_results = [{
                    'title': f'ãƒãƒƒãƒ{i+1}è¦ç´„',
                    'snippet': summary,
                    'url': f'internal://batch_{i+1}',
                    'source': 'Analysis Summary'
                } for i, summary in enumerate(batch_summaries)]
                final_analysis = self.analysis_service.analyze_search_results(summary_results, prompt)
                integration_end_time = datetime.now()
                
                integration_log["api_response_time"] = (integration_end_time - integration_start_time).total_seconds()
                integration_log["output_token_count"] = len(final_analysis.get('analysis', '')) // 4  # æ¦‚ç®—
                integration_log["cost"] = final_analysis.get('cost', 0.0)
                integration_log["status"] = "success"
                
                total_cost += final_analysis['cost']
                
                # ã‚µãƒãƒªãƒ¼çµ±è¨ˆæ›´æ–°
                analysis_session_log["summary"]["total_input_tokens"] += integration_log["input_token_count"]
                analysis_session_log["summary"]["total_output_tokens"] += integration_log["output_token_count"]
                analysis_session_log["summary"]["total_cost"] += integration_log["cost"]
                
                if self.logger:
                    self.logger.info("knowledge_analysis", "integration_success", "çµ±åˆåˆ†ææˆåŠŸ", data=integration_log)
                    
            except Exception as integration_error:
                integration_log["status"] = "failed"
                integration_log["error"] = str(integration_error)
                
                if self.logger:
                    self.logger.error("knowledge_analysis", "integration_failed", f"çµ±åˆåˆ†æå¤±æ•—: {integration_error}", data=integration_log)
                
                # çµ±åˆåˆ†æå¤±æ•—æ™‚ã¯ãƒãƒƒãƒè¦ç´„ã‚’ãã®ã¾ã¾ä½¿ç”¨
                final_analysis = {"analysis": "\n\n".join([f"ãƒãƒƒãƒ{i+1}è¦ç´„:\n{summary}" for i, summary in enumerate(batch_summaries)])}
                
            finally:
                integration_log["end_time"] = datetime.now().isoformat()
                analysis_session_log["integration_log"] = integration_log
            
            # åˆ†æã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†
            analysis_end_time = datetime.now()
            analysis_session_log["summary"]["total_time"] = (analysis_end_time - analysis_start_time).total_seconds()
            
            self.total_cost += total_cost
            
            # æœ€çµ‚ãƒ­ã‚°è¨˜éŒ²
            if self.logger:
                self.logger.info("knowledge_analysis", "analysis_complete", "ãƒãƒƒãƒåˆ†æå®Œäº†", data=analysis_session_log)
            
            # é‡è¦ãªçµ±è¨ˆã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚å‡ºåŠ›
            success_rate = (analysis_session_log["summary"]["successful_batches"] / analysis_session_log["summary"]["total_batches"]) * 100 if analysis_session_log["summary"]["total_batches"] > 0 else 0
            print(f"ğŸ§  åˆ†æå®Œäº†: {analysis_session_log['summary']['successful_batches']}/{analysis_session_log['summary']['total_batches']} ãƒãƒƒãƒæˆåŠŸ ({success_rate:.1f}%)")
            print(f"ğŸ’° ã‚³ã‚¹ãƒˆ: ${analysis_session_log['summary']['total_cost']:.6f} | â±ï¸ æ™‚é–“: {analysis_session_log['summary']['total_time']:.1f}ç§’")
            
            return {
                "analysis": final_analysis['analysis'],
                "batch_summaries": batch_summaries,
                "total_cost": total_cost,
                "processed_items": len(search_results),
                "analysis_log": analysis_session_log  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ­ã‚°ã‚‚å«ã‚ã‚‹
            }
            
        except Exception as e:
            analysis_session_log["stage_error"] = str(e)
            analysis_session_log["summary"]["total_time"] = (datetime.now() - analysis_start_time).total_seconds()
            
            if self.logger:
                self.logger.error("knowledge_analysis", "analysis_critical", f"ãƒãƒƒãƒåˆ†æé‡å¤§ã‚¨ãƒ©ãƒ¼: {e}", data=analysis_session_log)
            
            print(f"âŒ ãƒãƒƒãƒåˆ†æã‚¨ãƒ©ãƒ¼ã€åˆ†æã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
            return {
                "analysis": f"åˆ†æå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "batch_summaries": [],
                "total_cost": 0.0,
                "processed_items": len(search_results),
                "analysis_log": analysis_session_log,
                "error": True
            }
    
    
    def _generate_report(self, user_prompt: str, search_results: List[Dict], analysis_result: Dict) -> Dict:
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆç©ºãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        report_id = len(self.reports) + 1
        timestamp = datetime.now().isoformat()
        
        # ç©ºãƒ‡ãƒ¼ã‚¿ã®å ´åˆã®ç‰¹åˆ¥ãªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if not search_results or analysis_result.get("empty_data"):
            empty_report = {
                "report_id": report_id,
                "timestamp": timestamp,
                "user_prompt": user_prompt,
                "search_count": 0,
                "analysis_summary": "æ¤œç´¢çµæœãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€åˆ†æã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã™ã‚‹ã‹ã€æ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚",
                "key_insights": [
                    "å®Ÿéš›ã®Webæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ",
                    "ddgsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å‹•ä½œã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "æ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™"
                ],
                "categories": {},
                "related_topics": [],
                "data_quality": 0.0,
                "cost": analysis_result.get("total_cost", 0.0),
                "processing_time": "å³åº§ã«å®Œäº†",
                "detailed_data": {
                    "search_results": [],
                    "batch_summaries": [],
                    "no_data_reason": "search_failed"
                },
                "empty_data_report": True
            }
            
            # ç©ºãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒãƒ¼ãƒˆã«ã‚‚å“è³ªæ¤œè¨¼ã‚’é©ç”¨
            if self.quality_validator:
                try:
                    validation_report = self.quality_validator.validate_report(empty_report)
                    empty_report["validation_report"] = {
                        "validation_timestamp": validation_report.validation_timestamp,
                        "overall_score": validation_report.overall_score,
                        "total_issues": validation_report.total_issues,
                        "issues_by_severity": validation_report.issues_by_severity,
                        "quality_metrics": validation_report.quality_metrics,
                        "validation_summary": validation_report.validation_summary,
                        "recommendations": validation_report.recommendations
                    }
                    
                    print(f"âœ… ç©ºãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒãƒ¼ãƒˆå“è³ªæ¤œè¨¼: ã‚¹ã‚³ã‚¢ {validation_report.overall_score:.2f}")
                    if self.logger:
                        self.logger.info("knowledge_analysis", "validation_empty_data", 
                                        f"ç©ºãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒãƒ¼ãƒˆæ¤œè¨¼å®Œäº†: ã‚¹ã‚³ã‚¢ {validation_report.overall_score:.2f}",
                                        data={"report_id": report_id, "score": validation_report.overall_score})
                        
                except Exception as validation_error:
                    print(f"âš ï¸ ç©ºãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒãƒ¼ãƒˆæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {validation_error}")
                    if self.logger:
                        self.logger.error("knowledge_analysis", "validation_empty_failed", 
                                        f"ç©ºãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒãƒ¼ãƒˆæ¤œè¨¼å¤±æ•—: {validation_error}",
                                        data={"report_id": report_id, "error": str(validation_error)})
            
            return empty_report
        
        # åˆ†æçµæœã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        analysis_text = analysis_result.get("analysis", "")
        
        # ã‚­ãƒ¼ã‚¤ãƒ³ã‚µã‚¤ãƒˆæŠ½å‡ºï¼ˆç°¡æ˜“ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰
        key_insights = []
        lines = analysis_text.split('\n')
        for line in lines:
            if any(marker in line for marker in ['1.', '2.', '3.', '4.', '5.', '6.', '7.', '-']):
                insight = line.strip()
                if len(insight) > 10 and not insight.startswith('#'):
                    key_insights.append(insight)
        
        # é–¢é€£ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
        related_topics = []
        if "é–¢é€£èª¿æŸ»" in analysis_text or "ææ¡ˆ" in analysis_text:
            for line in lines:
                if any(marker in line for marker in ['1.', '2.', '3.', '4.', '5.']):
                    if "èª¿æŸ»" in line or "åˆ†æ" in line:
                        topic = line.strip().replace('1.', '').replace('2.', '').replace('3.', '').replace('4.', '').replace('5.', '').strip()
                        if len(topic) > 5:
                            related_topics.append(topic)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ†ææŠ½å‡º
        categories = {}
        current_category = None
        for line in lines:
            if "æŠ€è¡“é¢" in line:
                current_category = "technology"
                categories[current_category] = ""
            elif "å¸‚å ´" in line or "ãƒ“ã‚¸ãƒã‚¹" in line:
                current_category = "market"
                categories[current_category] = ""
            elif "å®Ÿç”¨" in line or "å¿œç”¨" in line:
                current_category = "applications"
                categories[current_category] = ""
            elif "èª²é¡Œ" in line or "ãƒªã‚¹ã‚¯" in line:
                current_category = "challenges"
                categories[current_category] = ""
            elif current_category and line.strip() and not line.startswith('#'):
                categories[current_category] += line.strip() + "\n"
        
        report = {
            "report_id": report_id,
            "timestamp": timestamp,
            "user_prompt": user_prompt,
            "search_count": len(search_results),
            "analysis_summary": analysis_text,
            "key_insights": key_insights[:7],  # æœ€å¤§7å€‹
            "categories": categories,
            "related_topics": related_topics[:5],  # æœ€å¤§5å€‹
            "data_quality": min(0.95, 0.5 + (len(search_results) / 200)),  # æ¤œç´¢æ•°ã«åŸºã¥ãå“è³ªã‚¹ã‚³ã‚¢
            "cost": analysis_result.get("total_cost", 0.0),
            "processing_time": "10-15åˆ†",
            "detailed_data": {
                "search_results": search_results,
                "batch_summaries": analysis_result.get("batch_summaries", [])
            }
        }
        
        # å“è³ªæ¤œè¨¼å®Ÿè¡Œ
        if self.quality_validator:
            try:
                validation_report = self.quality_validator.validate_report(report)
                report["validation_report"] = {
                    "validation_timestamp": validation_report.validation_timestamp,
                    "overall_score": validation_report.overall_score,
                    "total_issues": validation_report.total_issues,
                    "issues_by_severity": validation_report.issues_by_severity,
                    "quality_metrics": validation_report.quality_metrics,
                    "validation_summary": validation_report.validation_summary,
                    "recommendations": validation_report.recommendations
                }
                
                # é‡è¦ãªå•é¡ŒãŒã‚ã‚‹å ´åˆã¯ãƒ­ã‚°å‡ºåŠ›
                critical_issues = validation_report.issues_by_severity.get("critical", 0)
                error_issues = validation_report.issues_by_severity.get("error", 0)
                
                if critical_issues > 0:
                    print(f"ğŸ”´ ãƒ¬ãƒãƒ¼ãƒˆå“è³ªæ¤œè¨¼: é‡å¤§ãªå•é¡Œ {critical_issues}ä»¶æ¤œå‡º")
                    if self.logger:
                        self.logger.error("knowledge_analysis", "validation_critical", 
                                        f"é‡å¤§ãªæ¤œè¨¼å•é¡Œ {critical_issues}ä»¶", 
                                        data={"report_id": report_id, "issues": critical_issues})
                elif error_issues > 0:
                    print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆå“è³ªæ¤œè¨¼: ã‚¨ãƒ©ãƒ¼ {error_issues}ä»¶æ¤œå‡º")
                    if self.logger:
                        self.logger.warning("knowledge_analysis", "validation_error", 
                                          f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ {error_issues}ä»¶", 
                                          data={"report_id": report_id, "issues": error_issues})
                else:
                    print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆå“è³ªæ¤œè¨¼: å“è³ªã‚¹ã‚³ã‚¢ {validation_report.overall_score:.2f}")
                    if self.logger:
                        self.logger.info("knowledge_analysis", "validation_success", 
                                        f"æ¤œè¨¼å®Œäº†: ã‚¹ã‚³ã‚¢ {validation_report.overall_score:.2f}",
                                        data={"report_id": report_id, "score": validation_report.overall_score})
                        
            except Exception as validation_error:
                print(f"âš ï¸ å“è³ªæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {validation_error}")
                if self.logger:
                    self.logger.error("knowledge_analysis", "validation_failed", 
                                    f"å“è³ªæ¤œè¨¼å¤±æ•—: {validation_error}",
                                    data={"report_id": report_id, "error": str(validation_error)})
                
                # æ¤œè¨¼å¤±æ•—æ™‚ã¯åŸºæœ¬çš„ãªæ¤œè¨¼æƒ…å ±ã®ã¿è¿½åŠ 
                report["validation_report"] = {
                    "validation_timestamp": datetime.now().isoformat(),
                    "overall_score": 0.5,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢
                    "total_issues": 0,
                    "issues_by_severity": {},
                    "quality_metrics": {},
                    "validation_summary": f"å“è³ªæ¤œè¨¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {validation_error}",
                    "recommendations": ["å“è³ªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã‚’ç¢ºèªã—ã¦ãã ã•ã„"]
                }
        
        # å“è³ªå±¥æ­´ã¸ã®è¨˜éŒ²
        if self.quality_history_manager and report.get("validation_report"):
            try:
                # ValidationReportã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å†æ§‹ç¯‰
                from core.knowledge_analysis.report_quality_validator import ValidationReport, ValidationSeverity, ValidationIssue
                
                validation_data = report["validation_report"]
                validation_report = ValidationReport(
                    validation_timestamp=validation_data["validation_timestamp"],
                    overall_score=validation_data["overall_score"],
                    total_issues=validation_data["total_issues"],
                    issues_by_severity=validation_data["issues_by_severity"],
                    quality_metrics=validation_data["quality_metrics"],
                    validation_summary=validation_data["validation_summary"],
                    recommendations=validation_data["recommendations"],
                    issues=[]  # ç°¡ç•¥åŒ–
                )
                
                # å‡¦ç†æ™‚é–“è¨ˆç®—ï¼ˆãƒ€ãƒŸãƒ¼å€¤ã¨ã—ã¦è¨­å®šï¼‰
                processing_time = analysis_result.get("analysis_log", {}).get("summary", {}).get("total_time", 0.0)
                search_count = len(search_results)
                cost = report.get("cost", 0.0)
                data_quality = report.get("data_quality", 0.0)
                
                # å“è³ªå±¥æ­´è¨˜éŒ²
                record_id = self.quality_history_manager.record_validation_result(
                    validation_report=validation_report,
                    processing_time=processing_time,
                    search_count=search_count,
                    cost=cost,
                    data_quality=data_quality
                )
                
                if record_id:
                    print(f"ğŸ“Š å“è³ªå±¥æ­´è¨˜éŒ²: ID={record_id}")
                    if self.logger:
                        self.logger.info("knowledge_analysis", "quality_history_recorded", 
                                        f"å“è³ªå±¥æ­´è¨˜éŒ²å®Œäº†: {record_id}",
                                        data={"report_id": report_id, "record_id": record_id})
                
            except Exception as history_error:
                print(f"âš ï¸ å“è³ªå±¥æ­´è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {history_error}")
                if self.logger:
                    self.logger.error("knowledge_analysis", "quality_history_failed", 
                                    f"å“è³ªå±¥æ­´è¨˜éŒ²å¤±æ•—: {history_error}",
                                    data={"report_id": report_id, "error": str(history_error)})
        
        return report
    
    def _generate_error_report(self, user_prompt: str, error_message: str) -> Dict:
        """ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        return {
            "report_id": len(self.reports) + 1,
            "timestamp": datetime.now().isoformat(),
            "user_prompt": user_prompt,
            "error": True,
            "error_message": error_message,
            "analysis_summary": f"åˆ†æå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_message}",
            "key_insights": ["åˆ†æã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„"],
            "categories": {},
            "related_topics": [],
            "data_quality": 0.0,
            "cost": 0.0
        }
    
    def get_session_summary(self) -> Dict:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¦ç´„å–å¾—"""
        if not self.reports:
            return {"message": "ã¾ã ãƒ¬ãƒãƒ¼ãƒˆãŒã‚ã‚Šã¾ã›ã‚“"}
        
        return {
            "session_id": self.session_id,
            "total_reports": len(self.reports),
            "total_cost": self.total_cost,
            "reports": self.reports,
            "latest_topics": [report.get("related_topics", []) for report in self.reports[-3:]],
            "session_created": self.reports[0]["timestamp"] if self.reports else None
        }
    
    def _save_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜"""
        if not self.session_id:
            return
        
        try:
            session_data = {
                "session_id": self.session_id,
                "reports": self.reports,
                "total_cost": self.total_cost,
                "created_at": self.reports[0]["timestamp"] if self.reports else datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat()
            }
            
            session_file = self.sessions_dir / f"{self.session_id}.json"
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜: {session_file}")
            
        except Exception as e:
            print(f"âš ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _update_progress(self, message: str, progress: int):
        """é€²æ—æ›´æ–°"""
        if self.progress_callback:
            self.progress_callback(message, progress)
        else:
            print(f"[{progress:3d}%] {message}")
    
    def get_quality_validation_summary(self) -> Dict:
        """å“è³ªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if not self.quality_validator:
            return {"message": "å“è³ªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
        
        return self.quality_validator.get_validation_summary()
    
    def format_validation_report(self, report_id: int) -> str:
        """æŒ‡å®šãƒ¬ãƒãƒ¼ãƒˆã®æ¤œè¨¼çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not self.quality_validator:
            return "å“è³ªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¤œç´¢
        target_report = None
        for report in self.reports:
            if report.get("report_id") == report_id:
                target_report = report
                break
        
        if not target_report:
            return f"ãƒ¬ãƒãƒ¼ãƒˆID {report_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        
        validation_data = target_report.get("validation_report")
        if not validation_data:
            return f"ãƒ¬ãƒãƒ¼ãƒˆID {report_id} ã«ã¯æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
    
    def get_quality_statistics(self, days: int = 30) -> Dict:
        """å“è³ªçµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.quality_history_manager:
            return {"error": "å“è³ªå±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
        
        try:
            return self.quality_history_manager.get_quality_statistics(days)
        except Exception as e:
            return {"error": f"å“è³ªçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"}
    
    def get_quality_trend_analysis(self, days: int = 7) -> Dict:
        """å“è³ªå‚¾å‘åˆ†æå–å¾—"""
        if not self.quality_history_manager:
            return {"error": "å“è³ªå±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
        
        try:
            trend_analysis = self.quality_history_manager.get_quality_trend_analysis(days)
            
            # QualityTrendAnalysisã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã«å¤‰æ›
            return {
                "period_days": trend_analysis.period_days,
                "trend": trend_analysis.trend.value if hasattr(trend_analysis.trend, 'value') else str(trend_analysis.trend),
                "avg_score": trend_analysis.avg_score,
                "score_change": trend_analysis.score_change,
                "volatility": trend_analysis.volatility,
                "issue_trend": trend_analysis.issue_trend,
                "recommendations": trend_analysis.recommendations
            }
        except Exception as e:
            return {"error": f"å“è³ªå‚¾å‘åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"}
    
    def get_recent_quality_alerts(self, hours: int = 24) -> List[Dict]:
        """æœ€è¿‘ã®å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆå–å¾—"""
        if not self.quality_history_manager:
            return [{"error": "å“è³ªå±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}]
        
        try:
            alerts = self.quality_history_manager.get_recent_alerts(hours)
            
            # QualityAlertã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã«å¤‰æ›
            alert_dicts = []
            for alert in alerts:
                alert_dict = {
                    "alert_id": alert.alert_id,
                    "timestamp": alert.timestamp,
                    "level": alert.level.value if hasattr(alert.level, 'value') else str(alert.level),
                    "message": alert.message,
                    "metrics": alert.metrics,
                    "threshold_violated": alert.threshold_violated,
                    "suggested_action": alert.suggested_action
                }
                alert_dicts.append(alert_dict)
            
            return alert_dicts
        except Exception as e:
            return [{"error": f"å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"}]
    
    def cleanup_quality_history(self, keep_days: int = 90):
        """å“è³ªå±¥æ­´ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not self.quality_history_manager:
            print("å“è³ªå±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
        
        try:
            self.quality_history_manager.cleanup_old_records(keep_days)
            print(f"âœ… å“è³ªå±¥æ­´ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {keep_days}æ—¥ã‚ˆã‚Šå¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤")
        except Exception as e:
            print(f"âš ï¸ å“è³ªå±¥æ­´ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def print_quality_summary(self):
        """å“è³ªæƒ…å ±ã®è¦ç´„è¡¨ç¤º"""
        if not self.quality_history_manager:
            print("å“è³ªå±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
        
        try:
            # çµ±è¨ˆæƒ…å ±
            print("\nğŸ“Š å“è³ªçµ±è¨ˆæƒ…å ± (éå»30æ—¥)")
            print("=" * 50)
            stats = self.get_quality_statistics(30)
            if "error" not in stats:
                print(f"ç·è¨˜éŒ²æ•°: {stats.get('total_records', 0)}")
                print(f"å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {stats.get('average_score', 0):.3f}")
                print(f"ã‚¹ã‚³ã‚¢ç¯„å›²: {stats.get('score_range', [0, 0])[0]:.3f} - {stats.get('score_range', [0, 0])[1]:.3f}")
                print(f"ç·å•é¡Œæ•°: {stats.get('total_issues', 0)}")
                print(f"é‡å¤§å•é¡Œæ•°: {stats.get('critical_issues', 0)}")
                print(f"ç·ã‚³ã‚¹ãƒˆ: ${stats.get('total_cost', 0):.6f}")
                print(f"å¹³å‡å‡¦ç†æ™‚é–“: {stats.get('avg_processing_time', 0):.2f}ç§’")
                print(f"ã‚¢ãƒ©ãƒ¼ãƒˆæ•°: {stats.get('alert_counts', {})}")
            else:
                print(f"çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {stats['error']}")
            
            # å‚¾å‘åˆ†æ
            print("\nğŸ“ˆ å“è³ªå‚¾å‘åˆ†æ (éå»7æ—¥)")
            print("=" * 50)
            trend = self.get_quality_trend_analysis(7)
            if "error" not in trend:
                print(f"å‚¾å‘: {trend.get('trend', 'unknown')}")
                print(f"å¹³å‡ã‚¹ã‚³ã‚¢: {trend.get('avg_score', 0):.3f}")
                print(f"ã‚¹ã‚³ã‚¢å¤‰åŒ–: {trend.get('score_change', 0):+.3f}")
                print(f"å¤‰å‹•æ€§: {trend.get('volatility', 0):.3f}")
                print(f"æ¨å¥¨äº‹é …:")
                for recommendation in trend.get('recommendations', []):
                    print(f"  - {recommendation}")
            else:
                print(f"å‚¾å‘åˆ†æå–å¾—ã‚¨ãƒ©ãƒ¼: {trend['error']}")
            
            # æœ€è¿‘ã®ã‚¢ãƒ©ãƒ¼ãƒˆ
            print("\nğŸš¨ æœ€è¿‘ã®ã‚¢ãƒ©ãƒ¼ãƒˆ (éå»24æ™‚é–“)")
            print("=" * 50)
            alerts = self.get_recent_quality_alerts(24)
            if alerts and "error" not in alerts[0]:
                if alerts:
                    for alert in alerts[:5]:  # æœ€æ–°5ä»¶ã¾ã§è¡¨ç¤º
                        print(f"[{alert['level'].upper()}] {alert['message']}")
                        print(f"  æ™‚åˆ»: {alert['timestamp'][:19]}")
                        print(f"  å¯¾å¿œ: {alert['suggested_action']}")
                        print()
                else:
                    print("ã‚¢ãƒ©ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“")
            else:
                print(f"ã‚¢ãƒ©ãƒ¼ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {alerts[0].get('error', 'unknown error') if alerts else 'no data'}")
                
        except Exception as e:
            print(f"å“è³ªæƒ…å ±è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
        
        # ValidationReport ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å†æ§‹ç¯‰ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        try:
            from core.knowledge_analysis.report_quality_validator import ValidationReport
            validation_report = ValidationReport(
                validation_timestamp=validation_data["validation_timestamp"],
                overall_score=validation_data["overall_score"],
                total_issues=validation_data["total_issues"],
                issues_by_severity=validation_data["issues_by_severity"],
                issues=[],  # è©³ç´°ãªå•é¡Œãƒªã‚¹ãƒˆã¯çœç•¥
                quality_metrics=validation_data["quality_metrics"],
                validation_summary=validation_data["validation_summary"],
                recommendations=validation_data["recommendations"]
            )
            
            return self.quality_validator.format_validation_report(validation_report)
        except Exception as e:
            return f"æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}"

if __name__ == "__main__":
    def progress_callback(message, progress):
        print(f"[{progress:3d}%] {message}")
    
    try:
        engine = KnowledgeAnalysisEngine(progress_callback)
        
        print("ğŸ§ª KnowledgeAnalysisEngine - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        
        # æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹
        session_id = engine.start_new_session("AIæŠ€è¡“å‹•å‘")
        print(f"ğŸ“ ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹: {session_id}")
        
        # åˆå›åˆ†æ
        report1 = engine.analyze_topic("AIæŠ€è¡“ã®æœ€æ–°å‹•å‘ã«ã¤ã„ã¦åŒ…æ‹¬çš„ã«èª¿ã¹ãŸã„", search_count=50)
        print(f"âœ… åˆå›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
        print(f"ä¸»è¦ç™ºè¦‹: {len(report1.get('key_insights', []))}å€‹")
        print(f"ã‚³ã‚¹ãƒˆ: ${report1.get('cost', 0):.6f}")
        
        # ç¶™ç¶šåˆ†æ
        report2 = engine.analyze_topic("å‰å›ã®åˆ†æã§è¨€åŠã•ã‚ŒãŸAIæŠ€è¡“ã®ä¼æ¥­å°å…¥ã«ã¤ã„ã¦è©³ã—ã", search_count=30)
        print(f"âœ… ç¶™ç¶šãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¦ç´„
        summary = engine.get_session_summary()
        print(f"ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³è¦ç´„:")
        print(f"  ç·ãƒ¬ãƒãƒ¼ãƒˆæ•°: {summary['total_reports']}")
        print(f"  ç·ã‚³ã‚¹ãƒˆ: ${summary['total_cost']:.6f}")
        
        print("\nğŸ‰ ãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()