#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼šè©±å±¥æ­´åˆ†æã‚·ã‚¹ãƒ†ãƒ  - Phase 2Då®Ÿè£…
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»å¥½ã¿ãƒ»èˆˆå‘³å¤‰é·ã‚’æ·±åº¦åˆ†æ
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from datetime import datetime, timedelta
import re
import hashlib
from statistics import mean, median, stdev

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Windowsãƒ‘ã‚¹è¨­å®š
if os.name == 'nt':
    DATA_DIR = Path("D:/setsuna_bot/data")
    ANALYSIS_CACHE_DIR = Path("D:/setsuna_bot/conversation_analysis_cache")
else:
    DATA_DIR = Path("/mnt/d/setsuna_bot/data")
    ANALYSIS_CACHE_DIR = Path("/mnt/d/setsuna_bot/conversation_analysis_cache")

ANALYSIS_CACHE_DIR.mkdir(parents=True, exist_ok=True)

@dataclass
class ConversationPattern:
    """ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    pattern_id: str
    pattern_type: str  # "temporal", "topic_sequence", "emotional_flow", "question_style"
    frequency: int
    pattern_description: str
    example_sequences: List[str]
    confidence_score: float
    discovered_at: str

@dataclass
class UserBehaviorProfile:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    profile_id: str
    conversation_style: Dict[str, float]  # "exploratory", "focused", "casual", "analytical"
    topic_preferences: Dict[str, float]   # ãƒˆãƒ”ãƒƒã‚¯åˆ¥å¥½ã¿åº¦
    engagement_patterns: Dict[str, Any]   # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³
    temporal_activity: Dict[str, int]     # æ™‚é–“å¸¯åˆ¥æ´»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
    learning_progression: Dict[str, List[float]]  # çŸ¥è­˜ç¿’å¾—ã®é€²æ­©
    interaction_complexity: float         # å¯¾è©±ã®è¤‡é›‘ã•ãƒ¬ãƒ™ãƒ«
    last_updated: str

@dataclass
class TopicEvolution:
    """ãƒˆãƒ”ãƒƒã‚¯é€²åŒ–ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    topic: str
    timeline: List[Dict[str, Any]]  # æ™‚ç³»åˆ—ã§ã®è¨€åŠãƒ»é–¢å¿ƒå¤‰åŒ–
    interest_trajectory: List[float]  # èˆˆå‘³ãƒ¬ãƒ™ãƒ«ã®è»Œè·¡
    context_associations: Dict[str, float]  # ä»–ãƒˆãƒ”ãƒƒã‚¯ã¨ã®é–¢é€£æ€§
    learning_milestones: List[str]    # å­¦ç¿’ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³
    prediction_trend: str             # "increasing", "stable", "decreasing", "cyclical"

class ConversationHistoryAnalyzer:
    """ä¼šè©±å±¥æ­´åˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.multi_turn_conversations_path = DATA_DIR / "multi_turn_conversations.json"
        self.video_conversation_history_path = DATA_DIR / "video_conversation_history.json"
        self.user_preferences_path = DATA_DIR / "user_preferences.json"
        self.activity_sessions_dir = DATA_DIR / "activity_knowledge" / "sessions"
        
        # åˆ†æçµæœä¿å­˜ãƒ‘ã‚¹
        self.conversation_patterns_path = ANALYSIS_CACHE_DIR / "conversation_patterns.json"
        self.user_behavior_profile_path = ANALYSIS_CACHE_DIR / "user_behavior_profile.json"
        self.topic_evolution_path = ANALYSIS_CACHE_DIR / "topic_evolution.json"
        self.analysis_history_path = ANALYSIS_CACHE_DIR / "analysis_history.json"
        
        # ãƒ‡ãƒ¼ã‚¿
        self.multi_turn_data = {}
        self.video_conversation_data = {}
        self.user_preferences = {}
        self.activity_sessions = []
        
        # åˆ†æçµæœ
        self.conversation_patterns = {}
        self.user_behavior_profile = None
        self.topic_evolution = {}
        self.analysis_history = []
        
        # åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.min_pattern_frequency = 3  # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºæœ€å°é »åº¦
        self.topic_keywords = self._build_topic_keywords()
        self.emotional_indicators = self._build_emotional_indicators()
        self.complexity_indicators = self._build_complexity_indicators()
        
        # çµ±è¨ˆæƒ…å ±
        self.analysis_statistics = {
            "total_conversations": 0,
            "total_turns": 0,
            "unique_topics": 0,
            "patterns_discovered": 0,
            "analysis_coverage": 0.0,
            "last_analysis": None
        }
        
        self._load_conversation_data()
        self._load_existing_analysis()
        
        print("[ä¼šè©±å±¥æ­´åˆ†æ] âœ… ä¼šè©±å±¥æ­´åˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _build_topic_keywords(self) -> Dict[str, List[str]]:
        """ãƒˆãƒ”ãƒƒã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸æ§‹ç¯‰"""
        return {
            "éŸ³æ¥½": ["éŸ³æ¥½", "æ›²", "æ­Œ", "ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯", "ã‚µã‚¦ãƒ³ãƒ‰", "æ¥½æ›²", "ãƒ¡ãƒ­ãƒ‡ã‚£", "ãƒªã‚ºãƒ "],
            "å‹•ç”»": ["å‹•ç”»", "æ˜ åƒ", "ãƒ“ãƒ‡ã‚ª", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„", "ãƒãƒ£ãƒ³ãƒãƒ«", "é…ä¿¡", "ã‚¹ãƒˆãƒªãƒ¼ãƒ "],
            "ã‚¢ãƒ‹ãƒ¡": ["ã‚¢ãƒ‹ãƒ¡", "ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³", "æ¼«ç”»", "ãƒãƒ³ã‚¬", "äºŒæ¬¡å…ƒ", "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼"],
            "ã‚²ãƒ¼ãƒ ": ["ã‚²ãƒ¼ãƒ ", "ãƒ—ãƒ¬ã‚¤", "å®Ÿæ³", "ã‚²ãƒ¼ãƒŸãƒ³ã‚°", "ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼", "ã‚¹ãƒˆãƒªãƒ¼ãƒãƒ¼"],
            "æŠ€è¡“": ["æŠ€è¡“", "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°", "ã‚³ãƒ¼ãƒ‰", "é–‹ç™º", "AI", "ã‚·ã‚¹ãƒ†ãƒ "],
            "å‰µä½œ": ["å‰µä½œ", "åˆ¶ä½œ", "ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–", "ã‚¢ãƒ¼ãƒˆ", "ãƒ‡ã‚¶ã‚¤ãƒ³", "ä½œå“", "è¡¨ç¾"],
            "å­¦ç¿’": ["å­¦ç¿’", "å‹‰å¼·", "æ•™è‚²", "çŸ¥è­˜", "ç†è§£", "ç¿’å¾—", "ã‚¹ã‚­ãƒ«", "èƒ½åŠ›"],
            "å¨¯æ¥½": ["å¨¯æ¥½", "ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ", "æ¥½ã—ã„", "é¢ç™½ã„", "è¶£å‘³", "ãƒ¬ã‚¸ãƒ£ãƒ¼"],
            "æ„Ÿæƒ…": ["æ„Ÿæƒ…", "æ°—æŒã¡", "å¿ƒ", "æ„Ÿå‹•", "èˆˆå¥®", "ãƒªãƒ©ãƒƒã‚¯ã‚¹", "ç™’ã—", "ã‚¹ãƒˆãƒ¬ã‚¹"],
            "ç¤¾ä¼š": ["ç¤¾ä¼š", "ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£", "äººé–“é–¢ä¿‚", "äº¤æµ", "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "å‹é”"]
        }
    
    def _build_emotional_indicators(self) -> Dict[str, List[str]]:
        """æ„Ÿæƒ…æŒ‡æ¨™æ§‹ç¯‰"""
        return {
            "positive": ["å¬‰ã—ã„", "æ¥½ã—ã„", "é¢ç™½ã„", "ç´ æ™´ã‚‰ã—ã„", "æœ€é«˜", "è‰¯ã„", "å¥½ã", "æ°—ã«å…¥ã£ãŸ"],
            "negative": ["æ‚²ã—ã„", "ã¤ã¾ã‚‰ãªã„", "å«Œã„", "è‹¦æ‰‹", "æ®‹å¿µ", "ãŒã£ã‹ã‚Š", "ã„ã¾ã„ã¡"],
            "excited": ["ã‚ãã‚ã", "ãƒ†ãƒ³ã‚·ãƒ§ãƒ³", "èˆˆå¥®", "ç†±ã„", "ç››ã‚Šä¸ŠãŒã‚‹", "ã‚¨ã‚­ã‚µã‚¤ãƒˆ"],
            "curious": ["æ°—ã«ãªã‚‹", "èˆˆå‘³æ·±ã„", "é¢ç™½ãã†", "çŸ¥ã‚ŠãŸã„", "ã©ã‚“ãª", "ãªãœ"],
            "calm": ["è½ã¡ç€ã", "ç™’ã—", "ç©ã‚„ã‹", "ãƒªãƒ©ãƒƒã‚¯ã‚¹", "ã®ã‚“ã³ã‚Š", "ã‚†ã£ãã‚Š"],
            "confused": ["åˆ†ã‹ã‚‰ãªã„", "é›£ã—ã„", "è¤‡é›‘", "æ··ä¹±", "ç†è§£ã§ããªã„", "ã‚ˆãåˆ†ã‹ã‚‰ãªã„"]
        }
    
    def _build_complexity_indicators(self) -> Dict[str, List[str]]:
        """è¤‡é›‘ã•æŒ‡æ¨™æ§‹ç¯‰"""
        return {
            "simple": ["ç°¡å˜", "åˆ†ã‹ã‚Šã‚„ã™ã„", "åŸºæœ¬", "åˆæ­©", "å…¥é–€", "ã‚·ãƒ³ãƒ—ãƒ«"],
            "moderate": ["æ™®é€š", "ä¸€èˆ¬çš„", "æ¨™æº–", "ä¸­ç´š", "é©åº¦", "ãƒãƒ©ãƒ³ã‚¹"],
            "complex": ["è¤‡é›‘", "è©³ç´°", "é«˜åº¦", "å°‚é–€", "ä¸Šç´š", "ãƒãƒ‹ã‚¢ãƒƒã‚¯", "æ·±ã„"]
        }
    
    def _load_conversation_data(self):
        """ä¼šè©±ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰"""
        # ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ãƒ‡ãƒ¼ã‚¿
        try:
            if self.multi_turn_conversations_path.exists():
                with open(self.multi_turn_conversations_path, 'r', encoding='utf-8') as f:
                    self.multi_turn_data = json.load(f)
                print(f"[ä¼šè©±å±¥æ­´åˆ†æ] ğŸ“Š ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰")
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å‹•ç”»ä¼šè©±å±¥æ­´
        try:
            if self.video_conversation_history_path.exists():
                with open(self.video_conversation_history_path, 'r', encoding='utf-8') as f:
                    self.video_conversation_data = json.load(f)
                print(f"[ä¼šè©±å±¥æ­´åˆ†æ] ğŸ“Š å‹•ç”»ä¼šè©±å±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰")
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ å‹•ç”»ä¼šè©±å±¥æ­´ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å¥½ã¿
        try:
            if self.user_preferences_path.exists():
                with open(self.user_preferences_path, 'r', encoding='utf-8') as f:
                    self.user_preferences = json.load(f)
                print(f"[ä¼šè©±å±¥æ­´åˆ†æ] ğŸ“Š ãƒ¦ãƒ¼ã‚¶ãƒ¼å¥½ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰")
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼å¥½ã¿ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚»ãƒƒã‚·ãƒ§ãƒ³
        try:
            if self.activity_sessions_dir.exists():
                for session_file in self.activity_sessions_dir.glob("*.json"):
                    try:
                        with open(session_file, 'r', encoding='utf-8') as f:
                            session_data = json.load(f)
                            self.activity_sessions.append(session_data)
                    except:
                        continue
                print(f"[ä¼šè©±å±¥æ­´åˆ†æ] ğŸ“Š {len(self.activity_sessions)}ä»¶ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰")
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _load_existing_analysis(self):
        """æ—¢å­˜åˆ†æçµæœãƒ­ãƒ¼ãƒ‰"""
        try:
            if self.conversation_patterns_path.exists():
                with open(self.conversation_patterns_path, 'r', encoding='utf-8') as f:
                    patterns_data = json.load(f)
                    self.conversation_patterns = {
                        pid: ConversationPattern(**pattern) 
                        for pid, pattern in patterns_data.get("patterns", {}).items()
                    }
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        
        try:
            if self.user_behavior_profile_path.exists():
                with open(self.user_behavior_profile_path, 'r', encoding='utf-8') as f:
                    profile_data = json.load(f)
                    self.user_behavior_profile = UserBehaviorProfile(**profile_data)
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        
        try:
            if self.topic_evolution_path.exists():
                with open(self.topic_evolution_path, 'r', encoding='utf-8') as f:
                    evolution_data = json.load(f)
                    self.topic_evolution = {
                        topic: TopicEvolution(**data) 
                        for topic, data in evolution_data.get("evolutions", {}).items()
                    }
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ ãƒˆãƒ”ãƒƒã‚¯é€²åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
    
    def analyze_conversation_patterns(self) -> Dict[str, ConversationPattern]:
        """ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        print("[ä¼šè©±å±¥æ­´åˆ†æ] ğŸ” ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æä¸­...")
        
        patterns = {}
        
        # ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        patterns.update(self._analyze_turn_patterns())
        
        # ãƒˆãƒ”ãƒƒã‚¯é·ç§»ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns.update(self._analyze_topic_transition_patterns())
        
        # æ„Ÿæƒ…ãƒ•ãƒ­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns.update(self._analyze_emotional_flow_patterns())
        
        # è³ªå•ã‚¹ã‚¿ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns.update(self._analyze_question_style_patterns())
        
        # æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns.update(self._analyze_temporal_patterns())
        
        self.conversation_patterns = patterns
        self._save_conversation_patterns()
        
        print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âœ… {len(patterns)}å€‹ã®ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹")
        return patterns
    
    def _analyze_turn_patterns(self) -> Dict[str, ConversationPattern]:
        """ã‚¿ãƒ¼ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        patterns = {}
        turn_sequences = []
        
        # ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‹ã‚‰ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æŠ½å‡º
        if "current_session" in self.multi_turn_data:
            turns = self.multi_turn_data["current_session"].get("turns", [])
            for i in range(len(turns) - 2):
                sequence = []
                for j in range(3):  # 3ã‚¿ãƒ¼ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
                    if i + j < len(turns):
                        turn = turns[i + j]
                        turn_type = self._classify_turn_type(turn.get("user_input", ""))
                        sequence.append(turn_type)
                if len(sequence) == 3:
                    turn_sequences.append("->".join(sequence))
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        sequence_counts = Counter(turn_sequences)
        
        for sequence, count in sequence_counts.items():
            if count >= self.min_pattern_frequency:
                pattern_id = f"turn_pattern_{hashlib.md5(sequence.encode()).hexdigest()[:8]}"
                patterns[pattern_id] = ConversationPattern(
                    pattern_id=pattern_id,
                    pattern_type="turn_sequence",
                    frequency=count,
                    pattern_description=f"ã‚¿ãƒ¼ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹: {sequence}",
                    example_sequences=[sequence],
                    confidence_score=min(1.0, count / 10),
                    discovered_at=datetime.now().isoformat()
                )
        
        return patterns
    
    def _classify_turn_type(self, user_input: str) -> str:
        """ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        if not user_input:
            return "empty"
        
        # è³ªå•
        if "?" in user_input or any(word in user_input for word in ["ä½•", "ã©ã†", "ãªãœ", "ã„ã¤", "ã©ã“", "èª°"]):
            return "question"
        
        # æ„Ÿæƒ³ãƒ»è©•ä¾¡
        if any(word in user_input for word in ["æ€ã†", "æ„Ÿã˜", "å¥½ã", "å«Œã„", "è‰¯ã„", "æ‚ªã„"]):
            return "opinion"
        
        # è¦æ±‚ãƒ»ä¾é ¼
        if any(word in user_input for word in ["ã—ã¦", "æ•™ãˆã¦", "è¦‹ã›ã¦", "èã‹ã›ã¦", "æ¢ã—ã¦"]):
            return "request"
        
        # æƒ…å ±æä¾›
        if any(word in user_input for word in ["ã§ã™", "ã§ã‚ã‚‹", "ã ã£ãŸ", "ã«ã¤ã„ã¦"]):
            return "information"
        
        # ãã®ä»–
        return "general"
    
    def _analyze_topic_transition_patterns(self) -> Dict[str, ConversationPattern]:
        """ãƒˆãƒ”ãƒƒã‚¯é·ç§»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        patterns = {}
        topic_transitions = []
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã®ãƒˆãƒ”ãƒƒã‚¯é·ç§»æŠ½å‡º
        for session in self.activity_sessions:
            session_topics = []
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã®è©±é¡ŒæŠ½å‡º
            if "context" in session:
                context = session["context"]
                for topic_category, keywords in self.topic_keywords.items():
                    if any(keyword in str(context) for keyword in keywords):
                        session_topics.append(topic_category)
            
            # é·ç§»ãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆ
            for i in range(len(session_topics) - 1):
                transition = f"{session_topics[i]}->{session_topics[i+1]}"
                topic_transitions.append(transition)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦åˆ†æ
        transition_counts = Counter(topic_transitions)
        
        for transition, count in transition_counts.items():
            if count >= self.min_pattern_frequency:
                pattern_id = f"topic_transition_{hashlib.md5(transition.encode()).hexdigest()[:8]}"
                patterns[pattern_id] = ConversationPattern(
                    pattern_id=pattern_id,
                    pattern_type="topic_sequence",
                    frequency=count,
                    pattern_description=f"ãƒˆãƒ”ãƒƒã‚¯é·ç§»: {transition}",
                    example_sequences=[transition],
                    confidence_score=min(1.0, count / 5),
                    discovered_at=datetime.now().isoformat()
                )
        
        return patterns
    
    def _analyze_emotional_flow_patterns(self) -> Dict[str, ConversationPattern]:
        """æ„Ÿæƒ…ãƒ•ãƒ­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        patterns = {}
        emotional_sequences = []
        
        # ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‹ã‚‰æ„Ÿæƒ…ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æŠ½å‡º
        if "current_session" in self.multi_turn_data:
            turns = self.multi_turn_data["current_session"].get("turns", [])
            emotions = []
            
            for turn in turns:
                user_input = turn.get("user_input", "")
                emotion = self._detect_emotion(user_input)
                emotions.append(emotion)
            
            # æ„Ÿæƒ…é·ç§»ãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆ
            for i in range(len(emotions) - 2):
                if i + 2 < len(emotions):
                    sequence = f"{emotions[i]}->{emotions[i+1]}->{emotions[i+2]}"
                    emotional_sequences.append(sequence)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦åˆ†æ
        emotion_counts = Counter(emotional_sequences)
        
        for sequence, count in emotion_counts.items():
            if count >= self.min_pattern_frequency:
                pattern_id = f"emotion_flow_{hashlib.md5(sequence.encode()).hexdigest()[:8]}"
                patterns[pattern_id] = ConversationPattern(
                    pattern_id=pattern_id,
                    pattern_type="emotional_flow",
                    frequency=count,
                    pattern_description=f"æ„Ÿæƒ…ãƒ•ãƒ­ãƒ¼: {sequence}",
                    example_sequences=[sequence],
                    confidence_score=min(1.0, count / 3),
                    discovered_at=datetime.now().isoformat()
                )
        
        return patterns
    
    def _detect_emotion(self, text: str) -> str:
        """æ„Ÿæƒ…æ¤œå‡º"""
        if not text:
            return "neutral"
        
        emotion_scores = defaultdict(int)
        
        for emotion, indicators in self.emotional_indicators.items():
            for indicator in indicators:
                if indicator in text:
                    emotion_scores[emotion] += 1
        
        if emotion_scores:
            return max(emotion_scores.items(), key=lambda x: x[1])[0]
        
        return "neutral"
    
    def _analyze_question_style_patterns(self) -> Dict[str, ConversationPattern]:
        """è³ªå•ã‚¹ã‚¿ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        patterns = {}
        question_styles = []
        
        # ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‹ã‚‰è³ªå•ã‚¹ã‚¿ã‚¤ãƒ«æŠ½å‡º
        if "current_session" in self.multi_turn_data:
            turns = self.multi_turn_data["current_session"].get("turns", [])
            
            for turn in turns:
                user_input = turn.get("user_input", "")
                if "?" in user_input or any(qword in user_input for qword in ["ä½•", "ã©ã†", "ãªãœ"]):
                    style = self._classify_question_style(user_input)
                    question_styles.append(style)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦åˆ†æ
        style_counts = Counter(question_styles)
        
        for style, count in style_counts.items():
            if count >= self.min_pattern_frequency:
                pattern_id = f"question_style_{hashlib.md5(style.encode()).hexdigest()[:8]}"
                patterns[pattern_id] = ConversationPattern(
                    pattern_id=pattern_id,
                    pattern_type="question_style",
                    frequency=count,
                    pattern_description=f"è³ªå•ã‚¹ã‚¿ã‚¤ãƒ«: {style}",
                    example_sequences=[style],
                    confidence_score=min(1.0, count / 5),
                    discovered_at=datetime.now().isoformat()
                )
        
        return patterns
    
    def _classify_question_style(self, text: str) -> str:
        """è³ªå•ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é¡"""
        if "ä½•" in text:
            return "what_question"
        elif "ã©ã†" in text or "ã©ã®ã‚ˆã†" in text:
            return "how_question"
        elif "ãªãœ" in text or "ã©ã†ã—ã¦" in text:
            return "why_question"
        elif "ã„ã¤" in text:
            return "when_question"
        elif "ã©ã“" in text:
            return "where_question"
        elif "èª°" in text:
            return "who_question"
        else:
            return "general_question"
    
    def _analyze_temporal_patterns(self) -> Dict[str, ConversationPattern]:
        """æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        patterns = {}
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“åˆ†æ
        session_times = []
        for session in self.activity_sessions:
            if "timestamp" in session:
                try:
                    timestamp = datetime.fromisoformat(session["timestamp"])
                    hour = timestamp.hour
                    session_times.append(hour)
                except:
                    continue
        
        if session_times:
            # æ™‚é–“å¸¯ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            hour_counts = Counter(session_times)
            most_active_hours = hour_counts.most_common(3)
            
            for hour, count in most_active_hours:
                if count >= 3:
                    time_category = self._get_time_category(hour)
                    pattern_id = f"temporal_{time_category}_{hour}"
                    patterns[pattern_id] = ConversationPattern(
                        pattern_id=pattern_id,
                        pattern_type="temporal",
                        frequency=count,
                        pattern_description=f"{time_category}ã®æ´»å‹• ({hour}æ™‚é ƒ)",
                        example_sequences=[f"{hour}:00"],
                        confidence_score=min(1.0, count / 10),
                        discovered_at=datetime.now().isoformat()
                    )
        
        return patterns
    
    def _get_time_category(self, hour: int) -> str:
        """æ™‚é–“ã‚«ãƒ†ã‚´ãƒªå–å¾—"""
        if 5 <= hour < 12:
            return "morning"
        elif 12 <= hour < 18:
            return "afternoon"
        elif 18 <= hour < 22:
            return "evening"
        else:
            return "night"
    
    def create_user_behavior_profile(self) -> UserBehaviorProfile:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        print("[ä¼šè©±å±¥æ­´åˆ†æ] ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­...")
        
        # ä¼šè©±ã‚¹ã‚¿ã‚¤ãƒ«åˆ†æ
        conversation_style = self._analyze_conversation_style()
        
        # ãƒˆãƒ”ãƒƒã‚¯å¥½ã¿åˆ†æ
        topic_preferences = self._analyze_topic_preferences()
        
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        engagement_patterns = self._analyze_engagement_patterns()
        
        # æ™‚é–“çš„æ´»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        temporal_activity = self._analyze_temporal_activity()
        
        # å­¦ç¿’é€²æ­©åˆ†æ
        learning_progression = self._analyze_learning_progression()
        
        # å¯¾è©±è¤‡é›‘ã•åˆ†æ
        interaction_complexity = self._analyze_interaction_complexity()
        
        profile = UserBehaviorProfile(
            profile_id=f"user_profile_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            conversation_style=conversation_style,
            topic_preferences=topic_preferences,
            engagement_patterns=engagement_patterns,
            temporal_activity=temporal_activity,
            learning_progression=learning_progression,
            interaction_complexity=interaction_complexity,
            last_updated=datetime.now().isoformat()
        )
        
        self.user_behavior_profile = profile
        self._save_user_behavior_profile()
        
        print("[ä¼šè©±å±¥æ­´åˆ†æ] âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†")
        return profile
    
    def _analyze_conversation_style(self) -> Dict[str, float]:
        """ä¼šè©±ã‚¹ã‚¿ã‚¤ãƒ«åˆ†æ"""
        style_scores = {
            "exploratory": 0.0,
            "focused": 0.0,
            "casual": 0.0,
            "analytical": 0.0
        }
        
        total_inputs = 0
        
        # ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±åˆ†æ
        if "current_session" in self.multi_turn_data:
            turns = self.multi_turn_data["current_session"].get("turns", [])
            
            for turn in turns:
                user_input = turn.get("user_input", "")
                if user_input:
                    total_inputs += 1
                    
                    # æ¢ç´¢çš„ã‚¹ã‚¿ã‚¤ãƒ«
                    if any(word in user_input for word in ["ä½•ã‹", "ä»–ã«", "ã‚‚ã£ã¨", "è‰²ã€…", "ã„ã‚ã‚“ãª"]):
                        style_scores["exploratory"] += 1
                    
                    # é›†ä¸­çš„ã‚¹ã‚¿ã‚¤ãƒ«
                    if any(word in user_input for word in ["è©³ã—ã", "å…·ä½“çš„", "æ·±ã", "å°‚é–€", "é›†ä¸­"]):
                        style_scores["focused"] += 1
                    
                    # ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¹ã‚¿ã‚¤ãƒ«
                    if any(word in user_input for word in ["ãªã‚“ã‹", "ã¡ã‚‡ã£ã¨", "ã¾ã‚", "ã‘ã£ã“ã†"]):
                        style_scores["casual"] += 1
                    
                    # åˆ†æçš„ã‚¹ã‚¿ã‚¤ãƒ«
                    if any(word in user_input for word in ["ãªãœ", "ç†ç”±", "åˆ†æ", "æ¯”è¼ƒ", "è©•ä¾¡", "è€ƒå¯Ÿ"]):
                        style_scores["analytical"] += 1
        
        # æ­£è¦åŒ–
        if total_inputs > 0:
            for style in style_scores:
                style_scores[style] = style_scores[style] / total_inputs
        
        return style_scores
    
    def _analyze_topic_preferences(self) -> Dict[str, float]:
        """ãƒˆãƒ”ãƒƒã‚¯å¥½ã¿åˆ†æ"""
        topic_mentions = defaultdict(int)
        topic_positive_mentions = defaultdict(int)
        
        # ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯è¨€åŠæŠ½å‡º
        if "current_session" in self.multi_turn_data:
            turns = self.multi_turn_data["current_session"].get("turns", [])
            
            for turn in turns:
                user_input = turn.get("user_input", "")
                emotion = self._detect_emotion(user_input)
                
                for topic_category, keywords in self.topic_keywords.items():
                    for keyword in keywords:
                        if keyword in user_input:
                            topic_mentions[topic_category] += 1
                            if emotion in ["positive", "excited", "curious"]:
                                topic_positive_mentions[topic_category] += 1
        
        # å¥½ã¿åº¦è¨ˆç®—
        preferences = {}
        for topic in topic_mentions:
            total_mentions = topic_mentions[topic]
            positive_mentions = topic_positive_mentions[topic]
            
            if total_mentions > 0:
                preference_score = positive_mentions / total_mentions
                preferences[topic] = preference_score
        
        return preferences
    
    def _analyze_engagement_patterns(self) -> Dict[str, Any]:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        patterns = {
            "average_session_length": 0,
            "questions_per_session": 0,
            "topic_switches_per_session": 0,
            "response_elaboration": 0.0,
            "follow_up_frequency": 0.0
        }
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³é•·åˆ†æ
        session_lengths = []
        session_questions = []
        
        if "current_session" in self.multi_turn_data:
            turns = self.multi_turn_data["current_session"].get("turns", [])
            session_lengths.append(len(turns))
            
            question_count = 0
            for turn in turns:
                user_input = turn.get("user_input", "")
                if "?" in user_input or any(qword in user_input for qword in ["ä½•", "ã©ã†", "ãªãœ"]):
                    question_count += 1
            session_questions.append(question_count)
        
        # çµ±è¨ˆè¨ˆç®—
        if session_lengths:
            patterns["average_session_length"] = mean(session_lengths)
        if session_questions:
            patterns["questions_per_session"] = mean(session_questions)
        
        return patterns
    
    def _analyze_temporal_activity(self) -> Dict[str, int]:
        """æ™‚é–“çš„æ´»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        activity = defaultdict(int)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“åˆ†æ
        for session in self.activity_sessions:
            if "timestamp" in session:
                try:
                    timestamp = datetime.fromisoformat(session["timestamp"])
                    hour = timestamp.hour
                    time_category = self._get_time_category(hour)
                    activity[time_category] += 1
                except:
                    continue
        
        return dict(activity)
    
    def _analyze_learning_progression(self) -> Dict[str, List[float]]:
        """å­¦ç¿’é€²æ­©åˆ†æ"""
        progression = defaultdict(list)
        
        # ãƒˆãƒ”ãƒƒã‚¯åˆ¥ã®è¤‡é›‘ã•ãƒ¬ãƒ™ãƒ«æ¨ç§»
        for session in self.activity_sessions:
            if "context" in session and "timestamp" in session:
                context = str(session["context"])
                
                for topic_category, keywords in self.topic_keywords.items():
                    if any(keyword in context for keyword in keywords):
                        complexity = self._assess_context_complexity(context)
                        progression[topic_category].append(complexity)
        
        return dict(progression)
    
    def _assess_context_complexity(self, context: str) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¤‡é›‘ã•è©•ä¾¡"""
        complexity_score = 0.0
        
        # å˜èªæ•°ã«ã‚ˆã‚‹è¤‡é›‘ã•
        word_count = len(context.split())
        complexity_score += min(1.0, word_count / 100) * 0.3
        
        # å°‚é–€ç”¨èªã«ã‚ˆã‚‹è¤‡é›‘ã•
        for level, indicators in self.complexity_indicators.items():
            for indicator in indicators:
                if indicator in context:
                    if level == "simple":
                        complexity_score += 0.1
                    elif level == "moderate":
                        complexity_score += 0.5
                    elif level == "complex":
                        complexity_score += 0.9
        
        return min(1.0, complexity_score)
    
    def _analyze_interaction_complexity(self) -> float:
        """å¯¾è©±è¤‡é›‘ã•åˆ†æ"""
        complexity_scores = []
        
        if "current_session" in self.multi_turn_data:
            turns = self.multi_turn_data["current_session"].get("turns", [])
            
            for turn in turns:
                user_input = turn.get("user_input", "")
                if user_input:
                    complexity = self._assess_context_complexity(user_input)
                    complexity_scores.append(complexity)
        
        return mean(complexity_scores) if complexity_scores else 0.5
    
    def analyze_topic_evolution(self) -> Dict[str, TopicEvolution]:
        """ãƒˆãƒ”ãƒƒã‚¯é€²åŒ–åˆ†æ"""
        print("[ä¼šè©±å±¥æ­´åˆ†æ] ğŸ“ˆ ãƒˆãƒ”ãƒƒã‚¯é€²åŒ–ã‚’åˆ†æä¸­...")
        
        evolutions = {}
        
        # ãƒˆãƒ”ãƒƒã‚¯åˆ¥æ™‚ç³»åˆ—åˆ†æ
        for topic_category in self.topic_keywords.keys():
            evolution = self._analyze_single_topic_evolution(topic_category)
            if evolution:
                evolutions[topic_category] = evolution
        
        self.topic_evolution = evolutions
        self._save_topic_evolution()
        
        print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âœ… {len(evolutions)}å€‹ã®ãƒˆãƒ”ãƒƒã‚¯é€²åŒ–ã‚’åˆ†æå®Œäº†")
        return evolutions
    
    def _analyze_single_topic_evolution(self, topic: str) -> Optional[TopicEvolution]:
        """å˜ä¸€ãƒˆãƒ”ãƒƒã‚¯é€²åŒ–åˆ†æ"""
        keywords = self.topic_keywords.get(topic, [])
        if not keywords:
            return None
        
        timeline = []
        interest_trajectory = []
        context_associations = defaultdict(float)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚ç³»åˆ—ã§ã®ãƒˆãƒ”ãƒƒã‚¯è¨€åŠåˆ†æ
        for session in sorted(self.activity_sessions, key=lambda s: s.get("timestamp", "")):
            context = str(session.get("context", ""))
            timestamp = session.get("timestamp", "")
            
            # ãƒˆãƒ”ãƒƒã‚¯è¨€åŠåº¦
            mention_score = 0
            for keyword in keywords:
                if keyword in context:
                    mention_score += 1
            
            if mention_score > 0:
                # èˆˆå‘³ãƒ¬ãƒ™ãƒ«æ¨å®š
                emotion = self._detect_emotion(context)
                if emotion in ["positive", "excited", "curious"]:
                    interest_level = 0.8
                elif emotion in ["neutral"]:
                    interest_level = 0.5
                else:
                    interest_level = 0.3
                
                timeline.append({
                    "timestamp": timestamp,
                    "mention_score": mention_score,
                    "interest_level": interest_level,
                    "context_summary": context[:100] + "..." if len(context) > 100 else context
                })
                
                interest_trajectory.append(interest_level)
                
                # ä»–ãƒˆãƒ”ãƒƒã‚¯ã¨ã®é–¢é€£åˆ†æ
                for other_topic, other_keywords in self.topic_keywords.items():
                    if other_topic != topic:
                        for other_keyword in other_keywords:
                            if other_keyword in context:
                                context_associations[other_topic] += 0.1
        
        if not timeline:
            return None
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬
        prediction_trend = self._predict_topic_trend(interest_trajectory)
        
        # å­¦ç¿’ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³æŠ½å‡º
        milestones = self._extract_learning_milestones(timeline)
        
        return TopicEvolution(
            topic=topic,
            timeline=timeline,
            interest_trajectory=interest_trajectory,
            context_associations=dict(context_associations),
            learning_milestones=milestones,
            prediction_trend=prediction_trend
        )
    
    def _predict_topic_trend(self, trajectory: List[float]) -> str:
        """ãƒˆãƒ”ãƒƒã‚¯ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬"""
        if len(trajectory) < 3:
            return "stable"
        
        # æœ€è¿‘3ãƒã‚¤ãƒ³ãƒˆã®å‚¾å‘
        recent = trajectory[-3:]
        
        if recent[-1] > recent[0] + 0.2:
            return "increasing"
        elif recent[-1] < recent[0] - 0.2:
            return "decreasing"
        elif max(recent) - min(recent) > 0.3:
            return "cyclical"
        else:
            return "stable"
    
    def _extract_learning_milestones(self, timeline: List[Dict]) -> List[str]:
        """å­¦ç¿’ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³æŠ½å‡º"""
        milestones = []
        
        if not timeline:
            return milestones
        
        # æœ€åˆã®è¨€åŠ
        milestones.append(f"åˆå›è¨€åŠ: {timeline[0]['timestamp']}")
        
        # é«˜ã„é–¢å¿ƒã‚’ç¤ºã—ãŸæ™‚ç‚¹
        for event in timeline:
            if event.get("interest_level", 0) > 0.7:
                milestones.append(f"é«˜é–¢å¿ƒè¡¨ç¤º: {event['timestamp']}")
                break
        
        # æœ€è¿‘ã®æ´»å‹•
        if timeline:
            milestones.append(f"æœ€æ–°æ´»å‹•: {timeline[-1]['timestamp']}")
        
        return milestones[:5]  # æœ€å¤§5ã¤
    
    def _save_conversation_patterns(self):
        """ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜"""
        try:
            patterns_data = {
                "patterns": {pid: asdict(pattern) for pid, pattern in self.conversation_patterns.items()},
                "metadata": {
                    "total_patterns": len(self.conversation_patterns),
                    "last_updated": datetime.now().isoformat()
                }
            }
            
            with open(self.conversation_patterns_path, 'w', encoding='utf-8') as f:
                json.dump(patterns_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_user_behavior_profile(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜"""
        try:
            if self.user_behavior_profile:
                with open(self.user_behavior_profile_path, 'w', encoding='utf-8') as f:
                    json.dump(asdict(self.user_behavior_profile), f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_topic_evolution(self):
        """ãƒˆãƒ”ãƒƒã‚¯é€²åŒ–ä¿å­˜"""
        try:
            evolution_data = {
                "evolutions": {topic: asdict(evolution) for topic, evolution in self.topic_evolution.items()},
                "metadata": {
                    "total_topics": len(self.topic_evolution),
                    "last_updated": datetime.now().isoformat()
                }
            }
            
            with open(self.topic_evolution_path, 'w', encoding='utf-8') as f:
                json.dump(evolution_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ ãƒˆãƒ”ãƒƒã‚¯é€²åŒ–ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def perform_comprehensive_analysis(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ"""
        print("[ä¼šè©±å±¥æ­´åˆ†æ] ğŸ”¬ åŒ…æ‹¬çš„åˆ†æã‚’å®Ÿè¡Œä¸­...")
        
        # çµ±è¨ˆæ›´æ–°
        self._update_statistics()
        
        # å„ç¨®åˆ†æå®Ÿè¡Œ
        patterns = self.analyze_conversation_patterns()
        profile = self.create_user_behavior_profile()
        evolutions = self.analyze_topic_evolution()
        
        # åˆ†æå±¥æ­´è¨˜éŒ²
        analysis_record = {
            "analysis_id": f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "timestamp": datetime.now().isoformat(),
            "patterns_count": len(patterns),
            "topics_analyzed": len(evolutions),
            "profile_created": profile is not None,
            "statistics": self.analysis_statistics
        }
        
        self.analysis_history.append(analysis_record)
        self._save_analysis_history()
        
        print("[ä¼šè©±å±¥æ­´åˆ†æ] âœ… åŒ…æ‹¬çš„åˆ†æå®Œäº†")
        
        return {
            "patterns": patterns,
            "profile": profile,
            "evolutions": evolutions,
            "statistics": self.analysis_statistics,
            "analysis_record": analysis_record
        }
    
    def _update_statistics(self):
        """çµ±è¨ˆæƒ…å ±æ›´æ–°"""
        # ä¼šè©±æ•°ã‚«ã‚¦ãƒ³ãƒˆ
        total_conversations = 0
        total_turns = 0
        
        if "current_session" in self.multi_turn_data:
            total_conversations = 1
            total_turns = len(self.multi_turn_data["current_session"].get("turns", []))
        
        total_conversations += len(self.activity_sessions)
        
        # å›ºæœ‰ãƒˆãƒ”ãƒƒã‚¯æ•°
        unique_topics = len(self.topic_evolution)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
        patterns_discovered = len(self.conversation_patterns)
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡
        coverage_rate = min(1.0, total_conversations / 100) if total_conversations > 0 else 0.0
        
        self.analysis_statistics.update({
            "total_conversations": total_conversations,
            "total_turns": total_turns,
            "unique_topics": unique_topics,
            "patterns_discovered": patterns_discovered,
            "analysis_coverage": coverage_rate,
            "last_analysis": datetime.now().isoformat()
        })
    
    def _save_analysis_history(self):
        """åˆ†æå±¥æ­´ä¿å­˜"""
        try:
            history_data = {
                "analysis_history": self.analysis_history,
                "last_updated": datetime.now().isoformat()
            }
            
            with open(self.analysis_history_path, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ä¼šè©±å±¥æ­´åˆ†æ] âš ï¸ åˆ†æå±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_analysis_summary(self) -> Dict[str, Any]:
        """åˆ†æã‚µãƒãƒªãƒ¼å–å¾—"""
        return {
            "statistics": self.analysis_statistics,
            "patterns_overview": {
                "total_patterns": len(self.conversation_patterns),
                "pattern_types": list(set(p.pattern_type for p in self.conversation_patterns.values()))
            },
            "profile_overview": {
                "profile_exists": self.user_behavior_profile is not None,
                "conversation_style": self.user_behavior_profile.conversation_style if self.user_behavior_profile else {},
                "top_topics": dict(list(self.user_behavior_profile.topic_preferences.items())[:5]) if self.user_behavior_profile else {}
            },
            "evolution_overview": {
                "topics_tracked": len(self.topic_evolution),
                "trending_topics": [topic for topic, evo in self.topic_evolution.items() if evo.prediction_trend == "increasing"]
            }
        }