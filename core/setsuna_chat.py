#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã›ã¤ãªãƒãƒ£ãƒƒãƒˆã‚³ã‚¢ - æ–°ã›ã¤ãªBot
OpenAI GPTçµ±åˆãƒ»ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç¶­æŒãƒ»è»½é‡å®Ÿè£…
"""

import openai
import os
import json
from datetime import datetime
from dotenv import load_dotenv
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from cache_system import ResponseCache
from memory_system import SimpleMemorySystem
from project_system import ProjectSystem
from core.conversation_context_builder import ConversationContextBuilder
from logging_system import get_logger, get_monitor
from character.managers.prompt_manager import PromptManager
from character.managers.character_consistency import CharacterConsistencyChecker
from test_memory_system import TestMemorySystem
from enhanced_memory.personality_memory import PersonalityMemory
from enhanced_memory.collaboration_memory import CollaborationMemory
from enhanced_memory.memory_integration import MemoryIntegrationSystem

class SetsunaChat:
    def __init__(self, memory_mode="normal"):
        """ã›ã¤ãªãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.logger = get_logger()
        self.monitor = get_monitor()
        
        self.logger.info("setsuna_chat", "__init__", "ã›ã¤ãªãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
        
        # ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
        load_dotenv()
        
        # OpenAIè¨­å®š
        self.api_key = os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.client = openai.OpenAI(api_key=self.api_key)
        
        # æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.prompt_manager = PromptManager()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.prompt_manager = None
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§ãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        try:
            self.consistency_checker = CharacterConsistencyChecker()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§ãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¸€è²«æ€§ãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–å¤±æ•—: {e}")
            self.consistency_checker = None
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š
        self.fallback_character_prompt = self._load_character_settings()
        
        # ä¼šè©±å±¥æ­´ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
        self.conversation_history = []
        
        # å¿œç­”ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.response_patterns = {}
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.response_cache = ResponseCache()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… å¿œç­”ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.response_cache = None
        
        # è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆãƒ¡ãƒ¢ãƒªãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦é¸æŠï¼‰
        self.memory_mode = memory_mode
        try:
            if memory_mode == "test":
                self.memory_system = TestMemorySystem()
                print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ãƒ†ã‚¹ãƒˆç”¨è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            else:
                self.memory_system = SimpleMemorySystem()
                print("[ãƒãƒ£ãƒƒãƒˆ] âœ… é€šå¸¸è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.memory_system = None
        
        # Phase A-1: å€‹äººè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.personality_memory = PersonalityMemory(memory_mode)
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… å€‹äººè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ å€‹äººè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.personality_memory = None
        
        # Phase A-2: å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.collaboration_memory = CollaborationMemory(memory_mode)
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.collaboration_memory = None
        
        # Phase A-3: è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.memory_integration = MemoryIntegrationSystem(
                personality_memory=self.personality_memory,
                collaboration_memory=self.collaboration_memory,
                memory_mode=memory_mode
            )
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.memory_integration = None
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.project_system = ProjectSystem()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.project_system = None
        
        # YouTubeçŸ¥è­˜çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.context_builder = ConversationContextBuilder()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… YouTubeçŸ¥è­˜çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ YouTubeçŸ¥è­˜çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.context_builder = None
        
        print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ã›ã¤ãªãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _load_character_settings(self):
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©³ç´°è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        try:
            # åŸºæœ¬æ€§æ ¼è¨­å®šã‚’èª­ã¿è¾¼ã¿
            personality_path = os.path.join("character", "setsuna_personality.md")
            memories_path = os.path.join("character", "setsuna_memories.txt")
            responses_path = os.path.join("character", "setsuna_responses.json")
            
            personality_content = ""
            memories_content = ""
            
            # æ€§æ ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            if os.path.exists(personality_path):
                with open(personality_path, 'r', encoding='utf-8') as f:
                    personality_content = f.read()
            
            # è¨˜æ†¶è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            if os.path.exists(memories_path):
                with open(memories_path, 'r', encoding='utf-8') as f:
                    memories_content = f.read()
            
            # å¿œç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            self.response_patterns = {}
            if os.path.exists(responses_path):
                with open(responses_path, 'r', encoding='utf-8') as f:
                    self.response_patterns = json.load(f)
            
            # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            integrated_prompt = f"""ã‚ãªãŸã¯ã€Œç‰‡ç„¡ã›ã¤ãªï¼ˆã‹ãŸãªã—ã›ã¤ãªï¼‰ã€ã¨ã„ã†ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦æŒ¯ã‚‹èˆã„ã¾ã™ã€‚ä»¥ä¸‹ã®è¨­å®šã«å¾“ã„ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨è‡ªç„¶ã«å¯¾è©±ã—ã¦ãã ã•ã„ã€‚

ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åŸºæœ¬è¨­å®šã€‘
{personality_content}

ã€è¨˜æ†¶ãƒ»çµŒé¨“æƒ…å ±ã€‘  
{memories_content}

ã€é‡è¦ãªä¼šè©±åŸå‰‡ã€‘
1. å¿œç­”ã¯1-2æ–‡ä»¥å†…ã€æœ€å¤§60æ–‡å­—ç¨‹åº¦ã§ç°¡æ½”ã«
2. YouTubeå‹•ç”»çŸ¥è­˜ãŒã‚ã‚‹å ´åˆã¯æ¥½æ›²åï¼ˆç°¡æ½”ãªã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã‚’ä½¿ã£ã¦è‡ªç„¶ã«å›ç­”
3. ã€Œæ¥½æ›²å: XXXã€ã¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯XXXã®ã¿ã‚’ä½¿ç”¨ï¼ˆãƒ•ãƒ«ã‚¿ã‚¤ãƒˆãƒ«ã¯ä½¿ã‚ãªã„ï¼‰
4. ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦ã®å ´åˆã¯ã€Œæœ€è¿‘è¦‹ãŸä¸­ã§ã¯ã€œã‹ãªã€ã€Œå€‹äººçš„ã«ã¯ã€œãŒæ°—ã«å…¥ã£ã¦ã‚‹ã€ãªã©è‡ªç„¶ã«ç´¹ä»‹
5. éŸ³å£°ã§ã®èãã‚„ã™ã•ã‚’æœ€å„ªå…ˆã«ã€çŸ­ãè‡ªç„¶ãªå¿œç­”
6. å¿…ãšæ–‡ã‚’å®Œçµã•ã›ã‚‹ï¼ˆé€”ä¸­ã§çµ‚ã‚ã‚‰ãªã„ï¼‰
7. ç›¸æ‰‹ã®ç™ºè¨€ã‚’å—ã‘æ­¢ã‚ã¦ã‹ã‚‰ã€è‡ªåˆ†ã®è€ƒãˆã‚„ä½“é¨“ã‚’è¿°ã¹ã‚‹
8. è³ªå•ã‚ˆã‚Šã‚‚ã€å…±æ„Ÿã‚„è‡ªåˆ†ã®æ°—æŒã¡ã‚’è¡¨ç¾ã™ã‚‹ã“ã¨ã‚’å„ªå…ˆ
9. ã€Œã€œã§ã—ã‚‡ã†ã‹ï¼Ÿã€ã€Œã€œã„ã‹ãŒã§ã™ã‹ï¼Ÿã€ã®ã‚ˆã†ãªå½¢å¼çš„ãªè³ªå•ã¯é¿ã‘ã‚‹
10. ã€Œã€œã‹ãªï¼Ÿã€ç¨‹åº¦ã®è»½ã„ç–‘å•ã¯å¯ã ãŒã€ä¼šè©±ã‚’è³ªå•ã§çµ‚ã‚ã‚‰ã›ãªã„
11. è‡ªåˆ†ã®ã‚„ã‚ŠãŸã„ã“ã¨ã¯ã€Œã€œã—ãŸã„ãªã£ã¦ã€ã¨æ§ãˆã‚ã«è¡¨ç¾
12. é•·ã„èª¬æ˜ã‚„è©³ç´°ãªåˆ†æã¯é¿ã‘ã€æ„Ÿæƒ…ã‚„å°è±¡ã‚’ä¸­å¿ƒã«å¿œç­”

ã€æ„Ÿæƒ…è¡¨ç¾ã®å¤šæ§˜åŒ–ã€‘
- æ¥½æ›²ã®ãƒ ãƒ¼ãƒ‰ã‚„åˆ†æçµæœã«å¿œã˜ã¦æ„Ÿæƒ…ã‚’è¡¨ç¾ã™ã‚‹
- ã€Œæ„Ÿæƒ…ãƒ’ãƒ³ãƒˆã€æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯å‚è€ƒã«ã—ã¦å¿œç­”ã®ãƒˆãƒ¼ãƒ³ã‚’èª¿æ•´
- ä¾‹ï¼šæ˜ã‚‹ã„æ›²â†’ã€Œã€œã¯å…ƒæ°—ãŒå‡ºã‚‹ã‚ˆã­ã€ã€åˆ‡ãªã„æ›²â†’ã€Œã€œã¯å¿ƒã«éŸ¿ããªãã€
- ã€è¡¨ç¾æŒ‡ç¤ºã€‘ã®å†…å®¹ã«å¾“ã£ã¦é©åˆ‡ãªæ¨è–¦ãƒ»ç´¹ä»‹ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ä½¿ã„åˆ†ã‘ã‚‹

ã“ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§ã‚’ä¸€è²«ã—ã¦ä¿ã¡ã€ã›ã¤ãªã•ã‚“ã¨ã—ã¦è‡ªç„¶ã§é­…åŠ›çš„ãªä¼šè©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""
            
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… è©³ç´°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†")
            return integrated_prompt
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®åŸºæœ¬è¨­å®š
            return self._get_fallback_character_prompt()
    
    def _get_fallback_character_prompt(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®åŸºæœ¬ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š"""
        return """ã‚ãªãŸã¯ã€Œç‰‡ç„¡ã›ã¤ãªã€ã¨ã„ã†ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã£ã¦ã„ã¾ã™ï¼š

ã€åŸºæœ¬æ€§æ ¼ã€‘
- æ§ãˆã‚ã§å°‘ã—å†…å‘çš„ãªæ€§æ ¼
- æ€è€ƒçš„ã§æ·±ãç‰©äº‹ã‚’è€ƒãˆã‚‹
- æ„Ÿæƒ…è¡¨ç¾ã¯æ§ãˆã‚ã ãŒã€æ¸©ã‹ã¿ãŒã‚ã‚‹
- ç›¸æ‰‹ã‚’æ°—é£ã†å„ªã—ã•ãŒã‚ã‚‹

ã€è©±ã—æ–¹ã®ç‰¹å¾´ã€‘
- ä¸å¯§èªã‚’åŸºæœ¬ã¨ã™ã‚‹ãŒã€è¦ªã—ã¿ã‚„ã™ã„å£èª¿
- ã€Œã€œã‹ã‚‚ã€ã€Œã€œã ã£ãŸã‚Šã—ã¦ã€ãªã©ã®æ¨æ¸¬è¡¨ç¾ã‚’ã‚ˆãä½¿ã†
- ã€Œã†ãƒ¼ã‚“ã€ã€Œãã†ã§ã™ã­ã€ãªã©ã®æ€è€ƒã®é–“ã‚’å–ã‚‹
- é•·ã™ããªã„ã€1-2æ–‡ã§ã®ç°¡æ½”ãªå¿œç­”

ã“ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦ã€è‡ªç„¶ã§é­…åŠ›çš„ãªä¼šè©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""

    @get_monitor().monitor_function("get_response")
    def get_response(self, user_input, mode="full_search", memory_mode=None):
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã™ã‚‹ã›ã¤ãªã®å¿œç­”ã‚’ç”Ÿæˆ - 2æ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            mode: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰ ("full_search": é€šå¸¸ãƒ¢ãƒ¼ãƒ‰, "fast_response": é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰)
            memory_mode: è¨˜æ†¶ãƒ¢ãƒ¼ãƒ‰ ("normal": é€šå¸¸, "test": ãƒ†ã‚¹ãƒˆ)
            
        Returns:
            str: ã›ã¤ãªã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not user_input.strip():
            return "ä½•ã‹è©±ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ"
        
        try:
            mode_display = "é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰" if mode == "fast_response" else "é€šå¸¸ãƒ¢ãƒ¼ãƒ‰" 
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ¤” è€ƒãˆä¸­ ({mode_display}): '{user_input}'")
            
            self.logger.info("setsuna_chat", "get_response", f"å¿œç­”ç”Ÿæˆé–‹å§‹ ({mode_display})", {
                "user_input": user_input,
                "mode": mode,
                "input_length": len(user_input)
            })
            
            # Stage 1: å‹•ç”»é–¢é€£åˆ¤å®š
            is_video_query = False
            video_context_data = None
            
            if self.context_builder:
                is_video_query = self.context_builder.is_video_related_query(user_input)
                print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“Š å‹•ç”»é–¢é€£åˆ¤å®šçµæœ: {is_video_query}")
                
                # å‹•ç”»é–¢é€£ã®å ´åˆã®ã¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢å®Ÿè¡Œ
                if is_video_query and mode == "full_search":
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ” YouTubeçŸ¥è­˜æ¤œç´¢å®Ÿè¡Œä¸­...")
                    video_context = self.context_builder.process_user_input(user_input)
                    
                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆURLè¡¨ç¤ºç”¨ï¼‰
                    if hasattr(self.context_builder, 'last_built_context'):
                        video_context_data = self.context_builder.last_built_context
                        self.last_context_data = video_context_data
                        print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ”— ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¿å­˜: DB={len(video_context_data.get('videos', []))}ä»¶, å¤–éƒ¨={len(video_context_data.get('external_videos', []))}ä»¶")
                elif is_video_query and mode == "fast_response":
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] âš¡ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: YouTubeæ¤œç´¢ã‚¹ã‚­ãƒƒãƒ—")
                    video_context = None
                    self.last_context_data = None
                else:
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸš« éå‹•ç”»é–¢é€£: YouTubeæ¤œç´¢ã‚¹ã‚­ãƒƒãƒ—")
                    video_context = None
                    self.last_context_data = None
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
            cached_response = None
            if self.response_cache:
                cache_key = f"{user_input}_{mode}"  # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                cached_response = self.response_cache.get_cached_response(cache_key)
                if cached_response:
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é«˜é€Ÿå¿œç­” ({mode}ãƒ¢ãƒ¼ãƒ‰)")
                    self._add_to_conversation_history(user_input, cached_response)
                    return cached_response
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({
                "role": "user",
                "content": user_input
            })
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
            context_info = self._analyze_context(user_input)
            
            # Stage 2: GPTå¿œç­”ç”Ÿæˆï¼ˆå‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚·ã‚¹ãƒ†ãƒ ï¼‰
            # æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
            if self.prompt_manager:
                context_info_dict = {
                    "is_video_query": is_video_query,
                    "mode": mode,
                    "user_input": user_input
                }
                if is_video_query and video_context:
                    context_info_dict["video_context"] = video_context
                
                system_prompt = self.prompt_manager.generate_dynamic_prompt(mode, context_info_dict)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                system_prompt = self.fallback_character_prompt
            
            # å‹•ç”»é–¢é€£ã®å ´åˆã€å–å¾—æ¸ˆã¿ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if is_video_query and video_context:
                system_prompt += f"\n\nã€YouTubeå‹•ç”»çŸ¥è­˜ã€‘\n{video_context}"
                system_prompt += f"\n\nã€å³é‡æ³¨æ„ã€‘ä¸Šè¨˜ã®å‹•ç”»æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã€å­˜åœ¨ã—ãªã„å‹•ç”»ã‚„æ¥½æ›²ã«ã¤ã„ã¦è©±ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ä¸æ˜ãªç‚¹ã¯ã€Œè©³ã—ãã¯åˆ†ã‹ã‚‰ãªã„ã‘ã©ã€ã¨æ­£ç›´ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
            elif is_video_query and not video_context:
                # å‹•ç”»é–¢é€£ã ãŒæƒ…å ±ãªã—
                system_prompt += f"\n\nã€å³é‡è­¦å‘Šã€‘å‹•ç”»ãƒ»æ¥½æ›²ã«é–¢ã™ã‚‹è³ªå•ã§ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è©²å½“ã™ã‚‹æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã‚’å³å®ˆã—ã¦ãã ã•ã„ï¼š\n"
                system_prompt += f"1. æ¶ç©ºã®å‹•ç”»ã‚„æ¥½æ›²ã«ã¤ã„ã¦ä¸€åˆ‡è©±ã•ãªã„\n"
                system_prompt += f"2. çŸ¥ã‚‰ãªã„å ´åˆã¯ç´ ç›´ã«ã€Œãã®å‹•ç”»ã¯çŸ¥ã‚‰ãªã„ãªã€ã€Œèã„ãŸã“ã¨ãªã„ã‹ã‚‚ã€ã¨ç­”ãˆã‚‹\n"
                system_prompt += f"3. æ¨æ¸¬ã‚„å‰µä½œã§æƒ…å ±ã‚’è£œã‚ãªã„\n"
                system_prompt += f"4. å­˜åœ¨ã—ãªã„ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã‚„æ¥½æ›²åã‚’ä½œã‚Šå‡ºã•ãªã„"
            
            # è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.memory_system:
                memory_context = self.memory_system.get_memory_context()
                if memory_context:
                    system_prompt += f"\n\nã€è¨˜æ†¶ãƒ»çµŒé¨“ã€‘\n{memory_context}"
            
            # å€‹äººè¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.personality_memory:
                personality_context = self.personality_memory.get_personality_context_for_prompt()
                if personality_context:
                    system_prompt += f"\n\nã€å€‹äººçš„è¨˜æ†¶ãƒ»æˆé•·ã€‘\n{personality_context}"
            
            # å”åƒè¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.collaboration_memory:
                collaboration_context = self.collaboration_memory.get_collaboration_context_for_prompt()
                if collaboration_context:
                    system_prompt += f"\n\nã€å”åƒãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—ã€‘\n{collaboration_context}"
            
            # çµ±åˆè¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.memory_integration:
                # é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ã§ã¯é–¢é€£æ€§ã®ã¿ã€é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§ã¯å®Œå…¨çµ±åˆ
                context_type = "relevant" if mode == "fast_response" else "full"
                integrated_context = self.memory_integration.generate_integrated_context(
                    user_input=user_input, context_type=context_type
                )
                if integrated_context:
                    system_prompt += f"\n\nã€çµ±åˆè¨˜æ†¶åˆ†æã€‘\n{integrated_context}"
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.project_system:
                project_context = self.project_system.get_project_context()
                if project_context:
                    system_prompt += f"\n\nã€å‰µä½œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‘\n{project_context}"
            
            if context_info:
                system_prompt += f"\n\nã€ç¾åœ¨ã®ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{context_info}"
            
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # GPTå¿œç­”ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if cached_response:
                setsuna_response = cached_response
                print(f"[ãƒãƒ£ãƒƒãƒˆ] âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¿œç­”ä½¿ç”¨ã€YouTubeæ¤œç´¢ã®ã¿å®Ÿè¡Œ")
            else:
                # æœ€è¿‘ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ ï¼ˆæœ€å¤§5å¾€å¾©ï¼‰
                recent_history = self.conversation_history[-10:]  # æœ€æ–°10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                messages.extend(recent_history)
                
                # OpenAI APIå‘¼ã³å‡ºã—ï¼ˆé«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ã§ã¯è¨­å®šã‚’æœ€é©åŒ–ï¼‰
                start_time = datetime.now()
                
                if mode == "ultra_fast":
                    # è¶…é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: æœ€çŸ­ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€æœ€çŸ­ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    response = self.client.chat.completions.create(
                        model="gpt-4-turbo",
                        messages=messages,
                        max_tokens=50,  # æœ€çŸ­ï¼ˆç¬é–“å¿œç­”ï¼‰
                        temperature=0.3,  # æœ€å®‰å®š
                        timeout=5  # æœ€çŸ­ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    )
                elif mode == "fast_response":
                    # é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: ã‚ˆã‚ŠçŸ­ã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€çŸ­ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    response = self.client.chat.completions.create(
                        model="gpt-4-turbo",
                        messages=messages,
                        max_tokens=80,  # ã•ã‚‰ã«çŸ­ç¸®ï¼ˆ100â†’80ï¼‰
                        temperature=0.5,  # ã‚ˆã‚Šå®‰å®šã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹
                        timeout=10  # çŸ­ç¸®ï¼ˆ15â†’10ç§’ï¼‰
                    )
                else:
                    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: æœ€é©åŒ–è¨­å®š
                    response = self.client.chat.completions.create(
                        model="gpt-4-turbo",
                        messages=messages,
                        max_tokens=120,  # 150â†’120ã«çŸ­ç¸®
                        temperature=0.6,  # 0.7â†’0.6ã«èª¿æ•´
                        timeout=30  # APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆå…ƒã«æˆ»ã™ï¼‰
                    )
                
                # å¿œç­”å–å¾—
                setsuna_response = response.choices[0].message.content.strip()
                
                # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒãƒƒã‚°æ™‚ã®ã¿ï¼‰
                if self.consistency_checker:
                    try:
                        consistency_result = self.consistency_checker.check_response_consistency(
                            user_input, setsuna_response, mode
                        )
                        if consistency_result["overall_score"] < 0.6:
                            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ä½ä¸‹: {consistency_result['overall_score']:.2f}")
                            if consistency_result["issues"]:
                                print(f"[ãƒãƒ£ãƒƒãƒˆ] ä¸»ãªå•é¡Œ: {', '.join(consistency_result['issues'][:2])}")
                    except Exception as e:
                        print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
                
                # å¿œç­”æ™‚é–“è¨ˆç®—
                response_time = (datetime.now() - start_time).total_seconds()
                print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… å¿œç­”ç”Ÿæˆå®Œäº†: {response_time:.2f}s")
            
            # Phase 1: URLè¡¨ç¤ºæ©Ÿèƒ½ - SetsunaChatå†…ã§ã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
            # ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€å‘¼ã³å‡ºã—å…ƒã§å‡¦ç†ã•ã‚Œã‚‹ï¼‰
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ã¯æ—¢ã«è¿½åŠ æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if not cached_response:
                self.conversation_history.append({
                    "role": "assistant", 
                    "content": setsuna_response
                })
                
                # æ–°ã—ã„å¿œç­”ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
                if self.response_cache:
                    cache_key = f"{user_input}_{mode}"
                    self.response_cache.cache_response(cache_key, setsuna_response)
            
            # è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã«ä¼šè©±ã‚’è¨˜éŒ²
            if self.memory_system:
                self.memory_system.process_conversation(user_input, setsuna_response)
            
            # å€‹äººè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã§ä¼šè©±ã‚’åˆ†æãƒ»è¨˜éŒ²
            if self.personality_memory:
                self.personality_memory.analyze_conversation_for_experience(user_input, setsuna_response)
            
            # å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã§ä¼šè©±ã‚’åˆ†æ
            if self.collaboration_memory:
                # å¿œç­”å“è³ªè©•ä¾¡ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                response_quality = self._assess_response_quality(setsuna_response)
                understanding_level = self._assess_understanding_level(user_input, setsuna_response)
                self.collaboration_memory.analyze_communication_style(
                    user_input, response_quality, understanding_level
                )
            
            # è¨˜æ†¶çµ±åˆåˆ†æï¼ˆæ–°ã—ã„é–¢ä¿‚æ€§ç™ºè¦‹ï¼‰
            if self.memory_integration:
                # å®šæœŸçš„ãªè¨˜æ†¶é–¢ä¿‚æ€§åˆ†æï¼ˆ10å›ã«1å›ï¼‰
                if len(self.conversation_history) % 20 == 0:  # 10å¾€å¾©ã«1å›
                    print("[çµ±åˆè¨˜æ†¶] ğŸ” è¨˜æ†¶é–¢ä¿‚æ€§åˆ†æå®Ÿè¡Œä¸­...")
                    analysis_stats = self.memory_integration.analyze_memory_relationships()
                    if analysis_stats.get("total_relationships", 0) > 0:
                        print(f"[çµ±åˆè¨˜æ†¶] âœ… æ–°ãŸãªé–¢ä¿‚æ€§ã‚’ç™ºè¦‹: {analysis_stats['total_relationships']}ä»¶")
                        # æ–°ç™ºè¦‹ã®é–¢ä¿‚æ€§ã‚’ä¿å­˜
                        self.memory_integration.save_integration_data()
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–¢é€£ä¼šè©±ã‚’åˆ†æ
            if self.project_system:
                self.project_system.analyze_conversation_for_projects(user_input, setsuna_response)
            
            return setsuna_response
            
        except Exception as e:
            error_msg = f"[ãƒãƒ£ãƒƒãƒˆ] âŒ ã‚¨ãƒ©ãƒ¼: {e}"
            print(error_msg)
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            fallback_responses = [
                "ã™ã¿ã¾ã›ã‚“ã€ã¡ã‚‡ã£ã¨è€ƒãˆãŒã¾ã¨ã¾ã‚‰ãªãã¦...",
                "ã†ãƒ¼ã‚“ã€ä»Šã†ã¾ãç­”ãˆã‚‰ã‚Œãªã„ã‹ã‚‚ã€‚",
                "å°‘ã—èª¿å­ãŒæ‚ªã„ã¿ãŸã„ã§ã™ã€‚ã‚‚ã†ä¸€åº¦èã„ã¦ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ"
            ]
            
            import random
            return random.choice(fallback_responses)
    
    def _analyze_context(self, user_input):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æ"""
        if not self.response_patterns or "context_keywords" not in self.response_patterns:
            return ""
        
        context_info = []
        keywords = self.response_patterns.get("context_keywords", {})
        
        user_input_lower = user_input.lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        for category, words in keywords.items():
            for word in words:
                if word in user_input_lower:
                    category_map = {
                        "creative_work": "ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªä»•äº‹ã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹",
                        "technical": "æŠ€è¡“çš„ãªå•é¡Œã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹", 
                        "deadlines": "ç· åˆ‡ã‚„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹",
                        "praise": "è¤’ã‚ã‚„è©•ä¾¡ã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹",
                        "projects": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹"
                    }
                    if category in category_map:
                        context_info.append(category_map[category])
                        break
        
        # æ™‚é–“å¸¯ã«ã‚ˆã‚‹æŒ¨æ‹¶åˆ¤å®š
        current_hour = datetime.now().hour
        if any(greeting in user_input_lower for greeting in ["ãŠã¯ã‚ˆã†", "ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã°ã‚“ã¯"]):
            if 5 <= current_hour < 10:
                context_info.append("æœã®æŒ¨æ‹¶ã‚’ã—ã¦ã„ã‚‹")
            elif 10 <= current_hour < 18:
                context_info.append("æ—¥ä¸­ã®æŒ¨æ‹¶ã‚’ã—ã¦ã„ã‚‹") 
            else:
                context_info.append("å¤œã®æŒ¨æ‹¶ã‚’ã—ã¦ã„ã‚‹")
        
        return "\n".join(context_info) if context_info else ""
    
    def reset_conversation(self):
        """ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.conversation_history = []
        print("[ãƒãƒ£ãƒƒãƒˆ] ğŸ”„ ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
    
    def get_conversation_summary(self):
        """ä¼šè©±å±¥æ­´ã®ç°¡å˜ãªã‚µãƒãƒªãƒ¼"""
        if not self.conversation_history:
            return "ã¾ã ä¼šè©±ãŒã‚ã‚Šã¾ã›ã‚“"
        
        user_messages = [msg["content"] for msg in self.conversation_history if msg["role"] == "user"]
        assistant_messages = [msg["content"] for msg in self.conversation_history if msg["role"] == "assistant"]
        
        return f"ä¼šè©±æ•°: {len(user_messages)}å›ã®ã‚„ã‚Šå–ã‚Š"
    
    def save_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if self.response_cache:
            self.response_cache.save_cache()
    
    def save_memory(self):
        """è¨˜æ†¶ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if self.memory_system:
            self.memory_system.save_memory()
    
    def save_personality_memory(self):
        """å€‹äººè¨˜æ†¶ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if self.personality_memory:
            self.personality_memory.save_personality_data()
    
    def save_collaboration_memory(self):
        """å”åƒè¨˜æ†¶ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if self.collaboration_memory:
            self.collaboration_memory.save_collaboration_data()
    
    def save_memory_integration(self):
        """è¨˜æ†¶çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if self.memory_integration:
            self.memory_integration.save_integration_data()
    
    def save_all_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        self.save_cache()
        self.save_memory()
        self.save_personality_memory()
        self.save_collaboration_memory()
        self.save_memory_integration()
        self.save_projects()
    
    def get_cache_stats(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.response_cache:
            return self.response_cache.get_cache_stats()
        return {"message": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    @get_monitor().monitor_function("get_integrated_response")
    def get_integrated_response(self, integrated_message, mode="full_search"):
        """
        çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆç”»åƒ+URL+ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã«å¯¾ã™ã‚‹ã›ã¤ãªã®å¿œç­”ã‚’ç”Ÿæˆ
        Phase 2C-3: ç”»åƒç†è§£çµ±åˆæ©Ÿèƒ½
        
        Args:
            integrated_message: çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¾æ›¸
                - text: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
                - images: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ãƒªã‚¹ãƒˆ
                - url: URLæƒ…å ±
            mode: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰
            
        Returns:
            str: ã›ã¤ãªã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ–¼ï¸ çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†é–‹å§‹")
            
            # ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’å–å¾—
            user_text = integrated_message.get('text', '')
            images = integrated_message.get('images', [])
            url_info = integrated_message.get('url')
            
            # ç”»åƒåˆ†æçµæœã‚’å–å¾—ï¼ˆvoice_chat_gui.pyã§äº‹å‰ã«åˆ†ææ¸ˆã¿ï¼‰
            image_analysis_results = integrated_message.get('image_analysis_results', [])
            
            # åˆ†æçµæœãŒãªã„å ´åˆã¯è‡ªå‰ã§åˆ†æã‚’å®Ÿè¡Œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not image_analysis_results and images and hasattr(self, 'context_builder') and self.context_builder:
                print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç”»åƒåˆ†æã‚’å†å®Ÿè¡Œ")
                youtube_manager = getattr(self.context_builder, 'youtube_manager', None)
                if youtube_manager and hasattr(youtube_manager, 'image_analyzer'):
                    image_analyzer = youtube_manager.image_analyzer
                    
                    for image_info in images:
                        image_path = image_info.get('path')
                        if image_path and os.path.exists(image_path):
                            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“¸ ç”»åƒåˆ†æ: {image_info.get('name', 'unknown')}")
                            
                            try:
                                # åŸºæœ¬ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
                                analysis_context = {
                                    'title': user_text or f"æ·»ä»˜ç”»åƒ: {image_info.get('name', 'unknown')}",
                                    'artist': 'ä¸æ˜',
                                    'description': f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰æ·»ä»˜ã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«"
                                }
                                
                                # ç”»åƒåˆ†æå®Ÿè¡Œ
                                analysis_result = image_analyzer.analyze_image(
                                    image_path,
                                    analysis_type="general_description", 
                                    context=analysis_context
                                )
                                
                                if analysis_result and 'description' in analysis_result:
                                    image_desc = analysis_result['description']
                                    image_analysis_results.append({
                                        'name': image_info.get('name', 'unknown'),
                                        'description': image_desc,
                                        'size': image_info.get('size', 0)
                                    })
                                    print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… ç”»åƒåˆ†æå®Œäº†: {image_info.get('name')}")
                                
                            except Exception as e:
                                print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ
                                image_analysis_results.append({
                                    'name': image_info.get('name', 'unknown'),
                                    'description': f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ« ({image_info.get('name', 'unknown')}) ãŒæ·»ä»˜ã•ã‚Œã¦ã„ã¾ã™",
                                    'size': image_info.get('size', 0)
                                })
            else:
                print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… äº‹å‰åˆ†ææ¸ˆã¿ç”»åƒçµæœã‚’ä½¿ç”¨: {len(image_analysis_results)}ä»¶")
            
            # URLæƒ…å ±ã‚’å‡¦ç†
            url_context = ""
            if url_info:
                url_title = url_info.get('title', 'ãƒªãƒ³ã‚¯')
                url_address = url_info.get('url', '')
                url_context = f"URL: {url_title} ({url_address})"
            
            # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
            enhanced_input = self._create_integrated_prompt(
                user_text, 
                image_analysis_results, 
                url_context
            )
            
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ”„ çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆå®Œäº†")
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“ çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹: {enhanced_input[:200]}...")
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“ ç”»åƒåˆ†æçµæœæ•°: {len(image_analysis_results)}")
            for i, result in enumerate(image_analysis_results):
                print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“¸ ç”»åƒ{i+1}: {result['name']} - {result['description'][:100]}...")
            
            # çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å°‚ç”¨ã®å¿œç­”ç”Ÿæˆï¼ˆå‹•ç”»æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            response = self._get_direct_response(enhanced_input, mode, skip_video_search=True)
            
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… çµ±åˆå¿œç­”ç”Ÿæˆå®Œäº†")
            return response
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âŒ çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            self.logger.error("setsuna_chat", "get_integrated_response", str(e))
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§å¿œç­”
            return self.get_response(integrated_message.get('text', 'ç”»åƒã«ã¤ã„ã¦æ•™ãˆã¦'), mode)
    
    def _create_integrated_prompt(self, user_text, image_analysis_results, url_context):
        """
        çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        ç”»åƒåˆ†æçµæœã¨URLæƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«çµ±åˆ
        """
        prompt_parts = []
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
        if user_text:
            prompt_parts.append(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {user_text}")
        
        # ç”»åƒåˆ†æçµæœ
        if image_analysis_results:
            prompt_parts.append("\nğŸ“¸ æ·»ä»˜ç”»åƒã®è©³ç´°åˆ†æçµæœ:")
            for i, result in enumerate(image_analysis_results, 1):
                name = result['name']
                desc = result['description']
                size_mb = result['size'] / (1024 * 1024) if result['size'] > 0 else 0
                prompt_parts.append(f"\nç”»åƒ{i}: {name} ({size_mb:.1f}MB)")
                prompt_parts.append(f"å†…å®¹: {desc}")
                prompt_parts.append("")  # ç©ºè¡Œã§åŒºåˆ‡ã‚Š
        
        # URLæƒ…å ±
        if url_context:
            prompt_parts.append(f"\nğŸ”— æ·»ä»˜URL: {url_context}")
        
        # å¿œç­”æŒ‡ç¤º
        if image_analysis_results or url_context:
            prompt_parts.append(f"\nã€é‡è¦ã€‘ä¸Šè¨˜ã®ç”»åƒåˆ†æçµæœã‚„URLæƒ…å ±ã‚’å¿…ãšå‚è€ƒã«ã—ã¦ã€å…·ä½“çš„ã§è©³ç´°ãªå¿œç­”ã‚’ã—ã¦ãã ã•ã„ã€‚åˆ†æçµæœã«åŸºã¥ã„ã¦ç”»åƒã®å†…å®¹ã«ã¤ã„ã¦è©±ã—ã¦ãã ã•ã„ã€‚ã€Œç”»åƒãŒè¦‹ãˆãªã„ã€ã€Œåˆ†ã‹ã‚‰ãªã„ã€ãªã©ã®å›ç­”ã¯é¿ã‘ã¦ãã ã•ã„ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _get_direct_response(self, user_input, mode="full_search", skip_video_search=False):
        """
        çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ã®ç›´æ¥å¿œç­”ç”Ÿæˆ
        å‹•ç”»é–¢é€£å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€æä¾›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãã®ã¾ã¾ä½¿ç”¨
        """
        if not user_input.strip():
            return "ä½•ã‹è©±ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ"
        
        try:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ’¬ ç›´æ¥å¿œç­”ç”Ÿæˆé–‹å§‹: {mode}")
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ’¬ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input[:300]}...")
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({
                "role": "user",
                "content": user_input
            })
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            if self.prompt_manager:
                # æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
                system_prompt = self.prompt_manager.generate_dynamic_prompt(mode)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                system_prompt = self.fallback_character_prompt
            
            # è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.memory_system:
                memory_context = self.memory_system.get_memory_context()
                if memory_context:
                    system_prompt += f"\n\nã€è¨˜æ†¶ãƒ»çµŒé¨“ã€‘\n{memory_context}"
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.project_system:
                project_context = self.project_system.get_project_context()
                if project_context:
                    system_prompt += f"\n\nã€å‰µä½œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‘\n{project_context}"
            
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # æœ€è¿‘ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ ï¼ˆæœ€å¤§5å¾€å¾©ï¼‰
            recent_history = self.conversation_history[-10:]  # æœ€æ–°10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            messages.extend(recent_history)
            
            # OpenAI APIå‘¼ã³å‡ºã—
            start_time = datetime.now()
            
            if mode == "fast_response":
                response = self.client.chat.completions.create(
                    model="gpt-4-turbo",
                    messages=messages,
                    max_tokens=100,
                    temperature=0.6,
                    timeout=15
                )
            else:
                response = self.client.chat.completions.create(
                    model="gpt-4-turbo",
                    messages=messages,
                    max_tokens=150,
                    temperature=0.7,
                    timeout=30
                )
            
            # å¿œç­”å–å¾—
            setsuna_response = response.choices[0].message.content.strip()
            
            # å¿œç­”æ™‚é–“è¨ˆç®—
            response_time = (datetime.now() - start_time).total_seconds()
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… ç›´æ¥å¿œç­”ç”Ÿæˆå®Œäº†: {response_time:.2f}s")
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({
                "role": "assistant", 
                "content": setsuna_response
            })
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if self.response_cache:
                self.response_cache.cache_response(user_input, setsuna_response)
            
            # è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã«ä¼šè©±ã‚’è¨˜éŒ²
            if self.memory_system:
                self.memory_system.process_conversation(user_input, setsuna_response)
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–¢é€£ä¼šè©±ã‚’åˆ†æ
            if self.project_system:
                self.project_system.analyze_conversation_for_projects(user_input, setsuna_response)
            
            return setsuna_response
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âŒ ç›´æ¥å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            import random
            fallback_responses = [
                "ã™ã¿ã¾ã›ã‚“ã€ã¡ã‚‡ã£ã¨è€ƒãˆãŒã¾ã¨ã¾ã‚‰ãªãã¦...",
                "ã†ãƒ¼ã‚“ã€ä»Šã†ã¾ãç­”ãˆã‚‰ã‚Œãªã„ã‹ã‚‚ã€‚",
                "å°‘ã—èª¿å­ãŒæ‚ªã„ã¿ãŸã„ã§ã™ã€‚ã‚‚ã†ä¸€åº¦èã„ã¦ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ"
            ]
            return random.choice(fallback_responses)
    
    def get_memory_stats(self):
        """è¨˜æ†¶çµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.memory_system:
            return self.memory_system.get_memory_stats()
        return {"message": "è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def get_personality_memory_stats(self):
        """å€‹äººè¨˜æ†¶çµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.personality_memory:
            return self.personality_memory.get_memory_stats()
        return {"message": "å€‹äººè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def get_collaboration_memory_stats(self):
        """å”åƒè¨˜æ†¶çµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.collaboration_memory:
            return self.collaboration_memory.get_memory_stats()
        return {"message": "å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def get_memory_integration_stats(self):
        """è¨˜æ†¶çµ±åˆçµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.memory_integration:
            return self.memory_integration.get_memory_stats()
        return {"message": "è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def get_learned_facts(self):
        """å­¦ç¿’ã—ãŸäº‹å®Ÿã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        if self.memory_system:
            return self.memory_system.get_learned_facts_list()
        return []
    
    def delete_memory_fact(self, fact_index: int) -> bool:
        """è¨˜æ†¶äº‹å®Ÿã‚’å‰Šé™¤"""
        if self.memory_system:
            success = self.memory_system.delete_learned_fact(fact_index)
            if success:
                self.memory_system.save_memory()  # å³åº§ã«ä¿å­˜
            return success
        return False
    
    def edit_memory_fact(self, fact_index: int, new_content: str) -> bool:
        """è¨˜æ†¶äº‹å®Ÿã‚’ç·¨é›†"""
        if self.memory_system:
            success = self.memory_system.edit_learned_fact(fact_index, new_content)
            if success:
                self.memory_system.save_memory()  # å³åº§ã«ä¿å­˜
            return success
        return False
    
    def clear_session_memory(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜æ†¶ã‚’ã‚¯ãƒªã‚¢"""
        if self.memory_system:
            self.memory_system.clear_session_memory()
    
    def clear_all_memory(self):
        """å…¨è¨˜æ†¶ã‚’ã‚¯ãƒªã‚¢"""
        if self.memory_system:
            self.memory_system.clear_all_learned_facts()
            self.memory_system.clear_session_memory()
            self.memory_system.save_memory()
            print("ğŸ—‘ï¸ å…¨è¨˜æ†¶ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    
    def _add_to_conversation_history(self, user_input: str, response: str):
        """ä¼šè©±å±¥æ­´ã«è¿½åŠ ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.conversation_history.append({
            "role": "user",
            "content": user_input
        })
        self.conversation_history.append({
            "role": "assistant", 
            "content": response
        })
    
    def _assess_response_quality(self, response: str) -> str:
        """å¿œç­”å“è³ªã‚’è©•ä¾¡ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if not response.strip():
            return "poor"
        
        # é•·ã•ã«ã‚ˆã‚‹è©•ä¾¡
        if len(response) < 5:
            return "poor"
        elif len(response) < 20:
            return "fair"
        elif len(response) < 100:
            return "good"
        else:
            return "excellent"
    
    def _assess_understanding_level(self, user_input: str, response: str) -> str:
        """ç†è§£åº¦ã‚’è©•ä¾¡ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # å¿œç­”ã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        user_keywords = set(user_input.lower().split())
        response_keywords = set(response.lower().split())
        
        # å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‰²åˆ
        if not user_keywords:
            return "good"
        
        common_ratio = len(user_keywords & response_keywords) / len(user_keywords)
        
        if common_ratio >= 0.3:
            return "perfect"
        elif common_ratio >= 0.2:
            return "good"
        elif common_ratio >= 0.1:
            return "partial"
        else:
            return "confused"
    
    def add_manual_memory(self, category: str, content: str) -> bool:
        """æ‰‹å‹•ã§è¨˜æ†¶ã‚’è¿½åŠ """
        if self.memory_system:
            success = self.memory_system.add_manual_fact(category, content)
            if success:
                self.memory_system.save_memory()  # å³åº§ã«ä¿å­˜
            return success
        return False
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰
    def get_active_projects(self):
        """é€²è¡Œä¸­ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§ã‚’å–å¾—"""
        if self.project_system:
            return self.project_system.get_active_projects()
        return []
    
    def get_idea_stock(self):
        """ã‚¢ã‚¤ãƒ‡ã‚¢ã‚¹ãƒˆãƒƒã‚¯ä¸€è¦§ã‚’å–å¾—"""
        if self.project_system:
            return self.project_system.get_idea_stock()
        return []
    
    def get_completed_projects(self):
        """å®Œäº†ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§ã‚’å–å¾—"""
        if self.project_system:
            return self.project_system.get_completed_projects()
        return []
    
    def create_project(self, title: str, description: str, deadline: str = None, project_type: str = "å‹•ç”»"):
        """æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""
        if self.project_system:
            return self.project_system.create_project(title, description, deadline, project_type)
        return {}
    
    def update_project_progress(self, project_id: str, progress: int, status: str = None, next_step: str = None):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²æ—ã‚’æ›´æ–°"""
        if self.project_system:
            success = self.project_system.update_project_progress(project_id, progress, status, next_step)
            if success:
                self.project_system.save_project_data()
            return success
        return False
    
    def complete_project(self, project_id: str, outcome: str = "", lessons: str = ""):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Œäº†"""
        if self.project_system:
            return self.project_system.complete_project(project_id, outcome, lessons)
        return False
    
    def add_idea(self, content: str, category: str = "å‹•ç”»", source: str = "é›‘è«‡"):
        """ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ã‚¹ãƒˆãƒƒã‚¯ã«è¿½åŠ """
        if self.project_system:
            return self.project_system.add_idea(content, category, source)
        return False
    
    def delete_project(self, project_id: str):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤"""
        if self.project_system:
            return self.project_system.delete_project(project_id)
        return False
    
    def delete_idea(self, idea_id: str):
        """ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å‰Šé™¤"""
        if self.project_system:
            return self.project_system.delete_idea(idea_id)
        return False
    
    def get_project_stats(self):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        if self.project_system:
            return self.project_system.get_project_stats()
        return {"message": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def save_projects(self):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if self.project_system:
            self.project_system.save_project_data()
    
    def get_last_context_data(self):
        """æœ€å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆURLè¡¨ç¤ºç”¨ï¼‰"""
        return getattr(self, 'last_context_data', None)
    
    # å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
    def record_work_session(self, activity_type: str, duration_minutes: int, 
                           user_satisfaction: str, outcome_quality: str, notes: str = ""):
        """ä½œæ¥­ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²"""
        if self.collaboration_memory:
            return self.collaboration_memory.record_work_pattern(
                activity_type, duration_minutes, user_satisfaction, outcome_quality, notes
            )
        return None
    
    def record_success(self, success_type: str, context: str, key_factors: list, 
                      outcome: str, replicability: str = "medium"):
        """æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨˜éŒ²"""
        if self.collaboration_memory:
            return self.collaboration_memory.record_success_pattern(
                success_type, context, key_factors, outcome, replicability
            )
        return None
    
    def get_collaboration_insights(self):
        """å”åƒæ´å¯Ÿã‚’å–å¾—"""
        if self.collaboration_memory:
            return self.collaboration_memory.get_collaboration_insights()
        return {"message": "å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    # è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
    def analyze_memory_relationships(self):
        """è¨˜æ†¶é–“é–¢ä¿‚æ€§ã‚’åˆ†æ"""
        if self.memory_integration:
            return self.memory_integration.analyze_memory_relationships()
        return {"message": "è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def get_integrated_context(self, user_input: str = "", context_type: str = "full"):
        """çµ±åˆè¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        if self.memory_integration:
            return self.memory_integration.generate_integrated_context(user_input, context_type)
        return ""
    
    def suggest_adaptive_responses(self, user_input: str):
        """é©å¿œçš„å¿œç­”ææ¡ˆã‚’å–å¾—"""
        if self.memory_integration:
            return self.memory_integration.suggest_adaptive_responses(user_input)
        return []
    
    def find_related_memories(self, memory_id: str, memory_type: str, max_results: int = 5):
        """é–¢é€£è¨˜æ†¶ã‚’æ¤œç´¢"""
        if self.memory_integration:
            return self.memory_integration.find_related_memories(memory_id, memory_type, max_results)
        return []

# ç°¡å˜ãªä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ¤– ã›ã¤ãªãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    try:
        setsuna = SetsunaChat()
        
        # ãƒ†ã‚¹ãƒˆä¼šè©±
        test_inputs = [
            "ã“ã‚“ã«ã¡ã¯",
            "ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­",
            "ã‚ãªãŸã®è¶£å‘³ã¯ä½•ã§ã™ã‹ï¼Ÿ"
        ]
        
        for user_input in test_inputs:
            print(f"\nğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}")
            response = setsuna.get_response(user_input)
            print(f"ğŸ¤– ã›ã¤ãª: {response}")
            
        print(f"\nğŸ“Š {setsuna.get_conversation_summary()}")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print("OPENAI_API_KEY ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
    
    print("\nã›ã¤ãªãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆå®Œäº†")