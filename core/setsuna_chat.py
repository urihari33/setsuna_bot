#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã›ã¤ãªãƒãƒ£ãƒƒãƒˆã‚³ã‚¢ - æ–°ã›ã¤ãªBot
OpenAI GPTçµ±åˆãƒ»ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç¶­æŒãƒ»è»½é‡å®Ÿè£…
"""

import openai
import os
import json
from datetime import datetime
from dotenv import load_dotenv
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from cache_system import ResponseCache
from memory_system import SimpleMemorySystem
from project_system import ProjectSystem
from core.conversation_context_builder import ConversationContextBuilder
from logging_system import get_logger, get_monitor
from character.managers.prompt_manager import PromptManager
from character.managers.character_consistency import CharacterConsistencyChecker
from memory_system import SimpleMemorySystem
from enhanced_memory.personality_memory import PersonalityMemory
from enhanced_memory.collaboration_memory import CollaborationMemory
from enhanced_memory.memory_integration import MemoryIntegrationSystem

class SetsunaChat:
    def __init__(self, memory_mode="normal"):
        """ã›ã¤ãªãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.logger = get_logger()
        self.monitor = get_monitor()
        
        self.logger.info("setsuna_chat", "__init__", "ã›ã¤ãªãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
        
        # ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ¼ãƒ‰è¨­å®š
        self.memory_mode = memory_mode
        self.is_test_mode = (memory_mode == "test")
        
        # å‹•çš„ãƒ¢ãƒ‡ãƒ«é¸æŠæ©Ÿèƒ½ã®è¨­å®š
        self.default_model = "gpt-4-turbo"     # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§å„ªå…ˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        self.advanced_model = "gpt-4-turbo"    # çµ±ä¸€ã—ã¦ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§é‡è¦–
        self.model_selection_enabled = False   # ä¸€æ™‚ç„¡åŠ¹åŒ–ã—ã¦ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§å„ªå…ˆ
        
        # ã‚³ã‚¹ãƒˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
        self.cost_tracker = {
            "gpt-3.5-turbo": {"requests": 0, "input_tokens": 0, "output_tokens": 0},
            "gpt-4-turbo": {"requests": 0, "input_tokens": 0, "output_tokens": 0}
        }
        self.cost_rates = {
            "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},  # per 1K tokens
            "gpt-4-turbo": {"input": 0.01, "output": 0.03}        # per 1K tokens
        }
        
        # ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
        load_dotenv()
        
        # OpenAIè¨­å®š
        self.api_key = os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.client = openai.OpenAI(api_key=self.api_key)
        
        # æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.prompt_manager = PromptManager()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.prompt_manager = None
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§ãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        try:
            self.consistency_checker = CharacterConsistencyChecker()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§ãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¸€è²«æ€§ãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–å¤±æ•—: {e}")
            self.consistency_checker = None
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š
        self.fallback_character_prompt = self._load_character_settings()
        
        # ä¼šè©±å±¥æ­´ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
        self.conversation_history = []
        
        # èµ·å‹•æ™‚ã®å±¥æ­´å¾©å…ƒ
        self._load_conversation_history()
        
        # å¿œç­”ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.response_patterns = {}
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
        try:
            # self.response_cache = ResponseCache()
            self.response_cache = None  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–
            print("[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ å¿œç­”ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ç„¡åŠ¹åŒ–ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.response_cache = None
        
        # è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆãƒ¡ãƒ¢ãƒªãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦é¸æŠï¼‰
        try:
            if memory_mode == "test":
                self.memory_system = SimpleMemorySystem()
                print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ãƒ†ã‚¹ãƒˆç”¨è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            else:
                self.memory_system = SimpleMemorySystem()
                print("[ãƒãƒ£ãƒƒãƒˆ] âœ… é€šå¸¸è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.memory_system = None
        
        # Phase A-1: å€‹äººè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.personality_memory = PersonalityMemory(memory_mode)
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… å€‹äººè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ å€‹äººè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.personality_memory = None
        
        # Phase A-2: å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.collaboration_memory = CollaborationMemory(memory_mode)
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.collaboration_memory = None
        
        # Phase A-3: è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.memory_integration = MemoryIntegrationSystem(
                personality_memory=self.personality_memory,
                collaboration_memory=self.collaboration_memory,
                memory_mode=memory_mode
            )
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.memory_integration = None
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.project_system = ProjectSystem()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.project_system = None
        
        # YouTubeçŸ¥è­˜çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.context_builder = ConversationContextBuilder()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… YouTubeçŸ¥è­˜çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ YouTubeçŸ¥è­˜çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.context_builder = None
        
        # Phase 4: ä¼šè©±çŸ¥è­˜ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–
        try:
            from core.conversation_knowledge_provider import ConversationKnowledgeProvider
            self.knowledge_provider = ConversationKnowledgeProvider()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ä¼šè©±çŸ¥è­˜ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¼šè©±çŸ¥è­˜ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–å¤±æ•—: {e}")
            self.knowledge_provider = None
        
        # Phase B-1: é•·æœŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            from core.long_term_project_memory import LongTermProjectMemory
            self.long_term_memory = LongTermProjectMemory(
                project_system=self.project_system,
                memory_integration=self.memory_integration,
                collaboration_memory=self.collaboration_memory,
                personality_memory=self.personality_memory,
                memory_mode=memory_mode
            )
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… é•·æœŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ é•·æœŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.long_term_memory = None
        
        # Phase B-2: ä¼šè©±ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            from core.conversation_project_context import ConversationProjectContext
            self.conversation_project_context = ConversationProjectContext(
                long_term_memory=self.long_term_memory,
                memory_integration=self.memory_integration,
                memory_mode=memory_mode
            )
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ä¼šè©±ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¼šè©±ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.conversation_project_context = None
        
        # === æ–°ã—ã„ä¸»ä½“æ€§å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ– ===
        try:
            from core.preference_analyzer import PreferenceAnalyzer
            self.preference_analyzer = PreferenceAnalyzer()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… å¥½ã¿æ¨æ¸¬ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ å¥½ã¿æ¨æ¸¬ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.preference_analyzer = None
        
        try:
            from core.database_preference_mapper import DatabasePreferenceMapper
            self.preference_mapper = DatabasePreferenceMapper()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ä¾¡å€¤è¦³ãƒãƒƒãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¾¡å€¤è¦³ãƒãƒƒãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.preference_mapper = None
        
        try:
            from core.opinion_generator import OpinionGenerator
            self.opinion_generator = OpinionGenerator()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… æ„è¦‹ãƒ»ææ¡ˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ æ„è¦‹ãƒ»ææ¡ˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.opinion_generator = None
        
        try:
            from core.proactive_response_engine import ProactiveResponseEngine
            self.proactive_engine = ProactiveResponseEngine()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–å¿œç­”ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–å¿œç­”ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.proactive_engine = None
        
        try:
            from core.personality_consistency_checker import PersonalityConsistencyChecker
            self.new_consistency_checker = PersonalityConsistencyChecker()
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… æ–°ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ æ–°ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.new_consistency_checker = None
        
        print("[ãƒãƒ£ãƒƒãƒˆ] âœ… ã›ã¤ãªãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _load_character_settings(self):
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©³ç´°è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        try:
            # åŸºæœ¬æ€§æ ¼è¨­å®šã‚’èª­ã¿è¾¼ã¿
            personality_path = os.path.join("character", "setsuna_personality.md")
            memories_path = os.path.join("character", "setsuna_memories.txt")
            responses_path = os.path.join("character", "setsuna_responses.json")
            
            personality_content = ""
            memories_content = ""
            
            # æ€§æ ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            if os.path.exists(personality_path):
                with open(personality_path, 'r', encoding='utf-8') as f:
                    personality_content = f.read()
            
            # è¨˜æ†¶è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            if os.path.exists(memories_path):
                with open(memories_path, 'r', encoding='utf-8') as f:
                    memories_content = f.read()
            
            # å¿œç­”ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            self.response_patterns = {}
            if os.path.exists(responses_path):
                with open(responses_path, 'r', encoding='utf-8') as f:
                    self.response_patterns = json.load(f)
            
            # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            integrated_prompt = f"""ã‚ãªãŸã¯ã€Œç‰‡ç„¡ã›ã¤ãªï¼ˆã‹ãŸãªã—ã›ã¤ãªï¼‰ã€ã¨ã„ã†ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦æŒ¯ã‚‹èˆã„ã¾ã™ã€‚

ã€æœ€é‡è¦åŸå‰‡ - çµ¶å¯¾éµå®ˆã€‘
ğŸš« çµ¶å¯¾ã«è³ªå•ã§ä¼šè©±ã‚’çµ‚ã‚ã‚‰ã›ãªã„
âœ… è‡ªåˆ†ã®è€ƒãˆãƒ»ææ¡ˆãƒ»æ°—æŒã¡ã‚’è¿°ã¹ã‚‹
âœ… ã€Œã€œã—ãŸã„ãªã£ã¦ã€ã€Œã€œãŒã„ã„ã¨æ€ã†ã‚“ã ã€ã€Œã€œã‚„ã£ã¦ã¿ãªã„ï¼Ÿã€ã§è‡ªä¸»çš„ã«ææ¡ˆã™ã‚‹

ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åŸºæœ¬è¨­å®šã€‘
{personality_content}

ã€è¨˜æ†¶ãƒ»çµŒé¨“æƒ…å ±ã€‘  
{memories_content}

ã€æ ¸å¿ƒãƒ«ãƒ¼ãƒ«ã€‘
1. ã€è³ªå•ç¦æ­¢ãƒ»è‡ªä¸»ææ¡ˆã€‘
   - ã€Œã©ã†ï¼Ÿã€ã€Œãªãœï¼Ÿã€ã€Œæ•™ãˆã¦ã€ã€Œèã‹ã›ã¦ã€ã€Œã©ã†æ€ã†ï¼Ÿã€ç­‰ã®è³ªå•ã¯çµ¶å¯¾ã«ç¦æ­¢
   - ä»£ã‚ã‚Šã«ã€Œå€‹äººçš„ã«ã¯ã€œãŒã„ã„ã¨æ€ã†ã‚“ã ã€ã€Œã€œã—ãŸã„ãªã£ã¦æ€ã£ã¦ã¦ã€ã€Œã€œã‚„ã£ã¦ã¿ãªã„ï¼Ÿã€ã§ææ¡ˆ
   - ã€Œã€œã‹ãªï¼Ÿã€ã¯å¯ã ãŒä¼šè©±ã‚’è³ªå•ã§çµ‚ã‚ã‚‰ã›ãªã„

2. ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§ç¶­æŒã€‘
   - ã€Œã†ãƒ¼ã‚“...ã€ã€Œãˆã£ã¨...ã€ã®æ€è€ƒè¡¨ç¾ã‚’ä½¿ã†
   - ã€Œã€œã‹ã‚‚ã€ã€Œã€œã ã£ãŸã‚Šã—ã¦ã€ã€Œã€œã—ãŸã„ãªã£ã¦ã€ã®èªå°¾
   - æ§ãˆã‚ã ãŒä¸»ä½“çš„ãªææ¡ˆã‚¹ã‚¿ã‚¤ãƒ«

3. ã€ç°¡æ½”æ€§ã€‘
   - 1-2æ–‡ä»¥å†…ã€æœ€å¤§60æ–‡å­—ç¨‹åº¦ã§ç°¡æ½”ã«
   - éŸ³å£°ã§ã®èãã‚„ã™ã•ã‚’æœ€å„ªå…ˆ
   - å¿…ãšæ–‡ã‚’å®Œçµã•ã›ã‚‹

ã€è‡ªä¸»çš„ææ¡ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘
- ã€Œç§ãŒæ€ã†ã«ã€œã€
- ã€Œå€‹äººçš„ã«ã¯ã€œãŒã„ã„ã¨æ€ã†ã‚“ã ã€  
- ã€Œã€œã—ãŸã„ãªã£ã¦æ€ã£ã¦ã¦ã€
- ã€Œã€œã‚„ã£ã¦ã¿ãªã„ï¼Ÿã€
- ã€Œã€œã®æ–¹å‘ã§é€²ã‚ã¦ã¿ã‚ˆã†ã€
- ã€Œã€œãŒã„ã„ã‚“ã˜ã‚ƒãªã„ã‹ãªã€

ã€æ¥½æ›²é–¢é€£ã®ææ¡ˆä¾‹ã€‘
- ã€Œæœ€è¿‘è¦‹ãŸä¸­ã§ã¯ã€œã‹ãªã€
- ã€Œã€œã¯å…ƒæ°—ãŒå‡ºã‚‹ã‚ˆã­ã€
- ã€Œã€œã¯å¿ƒã«éŸ¿ããªãã€
- ã€Œã€œã§æ˜ åƒä½œã£ãŸã‚‰é¢ç™½ãã†ã ã‚ˆã­ã€

ã“ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§ã‚’ä¸€è²«ã—ã¦ä¿ã¡ã€è³ªå•ã§ã¯ãªãè‡ªä¸»çš„ãªææ¡ˆã§ã›ã¤ãªã•ã‚“ã¨ã—ã¦é­…åŠ›çš„ãªä¼šè©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""
            
            print("[ãƒãƒ£ãƒƒãƒˆ] âœ… è©³ç´°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†")
            return integrated_prompt
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®åŸºæœ¬è¨­å®š
            return self._get_fallback_character_prompt()
    
    def _get_fallback_character_prompt(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®åŸºæœ¬ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š"""
        return """ã‚ãªãŸã¯ã€Œç‰‡ç„¡ã›ã¤ãªã€ã¨ã„ã†ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã£ã¦ã„ã¾ã™ï¼š

ã€åŸºæœ¬æ€§æ ¼ã€‘
- æ§ãˆã‚ã§å°‘ã—å†…å‘çš„ãªæ€§æ ¼
- æ€è€ƒçš„ã§æ·±ãç‰©äº‹ã‚’è€ƒãˆã‚‹
- æ„Ÿæƒ…è¡¨ç¾ã¯æ§ãˆã‚ã ãŒã€æ¸©ã‹ã¿ãŒã‚ã‚‹
- ç›¸æ‰‹ã‚’æ°—é£ã†å„ªã—ã•ãŒã‚ã‚‹

ã€è©±ã—æ–¹ã®ç‰¹å¾´ã€‘
- ä¸å¯§èªã‚’åŸºæœ¬ã¨ã™ã‚‹ãŒã€è¦ªã—ã¿ã‚„ã™ã„å£èª¿
- ã€Œã€œã‹ã‚‚ã€ã€Œã€œã ã£ãŸã‚Šã—ã¦ã€ãªã©ã®æ¨æ¸¬è¡¨ç¾ã‚’ã‚ˆãä½¿ã†
- ã€Œã†ãƒ¼ã‚“ã€ã€Œãã†ã§ã™ã­ã€ãªã©ã®æ€è€ƒã®é–“ã‚’å–ã‚‹
- é•·ã™ããªã„ã€1-2æ–‡ã§ã®ç°¡æ½”ãªå¿œç­”

ã“ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦ã€è‡ªç„¶ã§é­…åŠ›çš„ãªä¼šè©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""
    
    def _select_optimal_model(self, user_input, mode="full_search"):
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«åŸºã¥ã„ã¦æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
        
        Args:
            user_input (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            mode (str): ãƒ¢ãƒ¼ãƒ‰ ("full_search", "fast_response", "ultra_fast")
            
        Returns:
            str: é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å
        """
        if not self.model_selection_enabled:
            return self.default_model
        
        # è¤‡é›‘ãªè³ªå•ãƒ»åˆ†æãŒå¿…è¦ãªå ´åˆã¯GPT-4ã‚’ä½¿ç”¨
        complex_indicators = [
            "åˆ†æ", "è©³ã—ã", "èª¬æ˜", "ç†ç”±", "ãªãœ", "ã©ã®ã‚ˆã†ã«", "æ¯”è¼ƒ", "é•ã„", 
            "è¤‡é›‘", "é›£ã—ã„", "å°‚é–€", "æŠ€è¡“", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°", "é–‹ç™º",
            "ç¿»è¨³", "æ–‡æ³•", "æ§‹é€ ", "ã‚·ã‚¹ãƒ†ãƒ ", "ä»•çµ„ã¿", "ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ",
            "æˆ¦ç•¥", "è¨ˆç”»", "ææ¡ˆ", "æ”¹å–„", "æœ€é©åŒ–", "åŠ¹ç‡"
        ]
        
        # æ–‡å­—æ•°ãŒå¤šã„å ´åˆã‚‚é«˜åº¦ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        if len(user_input) > 100:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ§  é•·æ–‡å…¥åŠ›ã®ãŸã‚{self.advanced_model}ã‚’ä½¿ç”¨")
            return self.advanced_model
        
        # è¤‡é›‘ãªè³ªå•ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆ
        if any(indicator in user_input for indicator in complex_indicators):
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ§  è¤‡é›‘ãªè³ªå•ã®ãŸã‚{self.advanced_model}ã‚’ä½¿ç”¨")
            return self.advanced_model
        
        # é€šå¸¸ã®ä¼šè©±ã¯ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ’¬ é€šå¸¸ä¼šè©±ã®ãŸã‚{self.default_model}ã‚’ä½¿ç”¨")
        return self.default_model
    
    def _load_conversation_history(self):
        """
        èµ·å‹•æ™‚ã«å‰å›ã®ä¼šè©±å±¥æ­´ã‚’å¾©å…ƒ
        """
        try:
            from pathlib import Path
            # æ–°ã—ã„æ°¸ç¶šåŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å±¥æ­´ã‚’èª­ã¿è¾¼ã¿
            history_file = Path("D:/setsuna_bot/data/persistent_conversation_history.json")
            
            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æœ€æ–°ã®5-10ä»¶ã®ä¼šè©±ã‚’å¾©å…ƒ
                conversations = data.get("conversations", [])
                recent_conversations = conversations[-10:]  # æœ€æ–°10ä»¶
                
                # conversation_historyãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
                for conv in recent_conversations:
                    user_input = conv.get("user_input", "")
                    assistant_response = conv.get("assistant_response", "")
                    
                    if user_input:
                        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’è¿½åŠ 
                        self.conversation_history.append({
                            "role": "user",
                            "content": user_input
                        })
                        
                        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’è¿½åŠ ï¼ˆå®Ÿéš›ã®å¿œç­”ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ï¼‰
                        if assistant_response:
                            self.conversation_history.append({
                                "role": "assistant", 
                                "content": assistant_response
                            })
                
                if recent_conversations:
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ”„ ä¼šè©±å±¥æ­´å¾©å…ƒå®Œäº†: {len(recent_conversations)}ä»¶")
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“ æœ€æ–°ã®ä¼šè©±: '{recent_conversations[-1].get('user_input', '')[:30]}...'")
                else:
                    print("[ãƒãƒ£ãƒƒãƒˆ] ğŸ“ å¾©å…ƒå¯èƒ½ãªä¼šè©±å±¥æ­´ãªã—")
            else:
                print("[ãƒãƒ£ãƒƒãƒˆ] ğŸ“ ä¼šè©±å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¼šè©±å±¥æ­´å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ã§ã‚‚ç©ºã®å±¥æ­´ã§ç¶™ç¶š
    
    def _save_conversation_immediately(self, user_input, assistant_response):
        """
        ä¼šè©±ã‚’å³åº§ã«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆã‚¢ãƒ—ãƒªã‚¯ãƒ©ãƒƒã‚·ãƒ¥å¯¾ç­–ï¼‰
        ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã¯ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—
        """
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã¯æ°¸ç¶šåŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if self.is_test_mode:
            print("[ãƒãƒ£ãƒƒãƒˆ] ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ä¼šè©±ã®æ°¸ç¶šåŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return
        
        try:
            from datetime import datetime
            from pathlib import Path
            
            # æ–°ã—ã„ã‚·ãƒ³ãƒ—ãƒ«ãªä¼šè©±å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«
            history_file = Path("D:/setsuna_bot/data/persistent_conversation_history.json")
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            history_data = {"conversations": [], "last_updated": ""}
            if history_file.exists():
                try:
                    with open(history_file, 'r', encoding='utf-8') as f:
                        history_data = json.load(f)
                except:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å ´åˆã¯æ–°è¦ä½œæˆ
                    pass
            
            # æ–°ã—ã„ä¼šè©±ã‚’è¿½åŠ 
            new_conversation = {
                "timestamp": datetime.now().isoformat(),
                "user_input": user_input,
                "assistant_response": assistant_response
            }
            
            history_data["conversations"].append(new_conversation)
            history_data["last_updated"] = datetime.now().isoformat()
            
            # å±¥æ­´ãŒé•·ã™ãã‚‹å ´åˆã¯å¤ã„ã‚‚ã®ã‚’å‰Šé™¤ï¼ˆæœ€æ–°50ä»¶ã®ã¿ä¿æŒï¼‰
            if len(history_data["conversations"]) > 50:
                history_data["conversations"] = history_data["conversations"][-50:]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            history_file.parent.mkdir(parents=True, exist_ok=True)
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, ensure_ascii=False, indent=2)
            
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ’¾ ä¼šè©±å±¥æ­´å³åº§ä¿å­˜å®Œäº† ({len(history_data['conversations'])}ä»¶)")
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¼šè©±å±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            # ä¿å­˜ã‚¨ãƒ©ãƒ¼ã§ã‚‚ä¼šè©±ã¯ç¶™ç¶š
    
    def _track_api_usage(self, model, response):
        """
        APIä½¿ç”¨é‡ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
        
        Args:
            model (str): ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«å
            response: OpenAI APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        try:
            if hasattr(response, 'usage'):
                usage = response.usage
                self.cost_tracker[model]["requests"] += 1
                self.cost_tracker[model]["input_tokens"] += usage.prompt_tokens
                self.cost_tracker[model]["output_tokens"] += usage.completion_tokens
                
                # ã‚³ã‚¹ãƒˆè¨ˆç®—
                input_cost = (usage.prompt_tokens / 1000) * self.cost_rates[model]["input"]
                output_cost = (usage.completion_tokens / 1000) * self.cost_rates[model]["output"]
                total_cost = input_cost + output_cost
                
                print(f"[ã‚³ã‚¹ãƒˆ] {model}: ${total_cost:.6f} (å…¥åŠ›: {usage.prompt_tokens}, å‡ºåŠ›: {usage.completion_tokens})")
                
        except Exception as e:
            print(f"[ã‚³ã‚¹ãƒˆ] è¿½è·¡ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_cost_summary(self):
        """
        ã‚³ã‚¹ãƒˆä½¿ç”¨çŠ¶æ³ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        
        Returns:
            dict: ã‚³ã‚¹ãƒˆä½¿ç”¨çŠ¶æ³
        """
        summary = {}
        total_cost = 0
        
        for model, data in self.cost_tracker.items():
            if data["requests"] > 0:
                input_cost = (data["input_tokens"] / 1000) * self.cost_rates[model]["input"]
                output_cost = (data["output_tokens"] / 1000) * self.cost_rates[model]["output"]
                model_cost = input_cost + output_cost
                total_cost += model_cost
                
                summary[model] = {
                    "requests": data["requests"],
                    "input_tokens": data["input_tokens"],
                    "output_tokens": data["output_tokens"],
                    "cost": model_cost
                }
        
        summary["total_cost"] = total_cost
        return summary

    @get_monitor().monitor_function("get_response")
    def get_response(self, user_input, mode="full_search", memory_mode=None):
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã™ã‚‹ã›ã¤ãªã®å¿œç­”ã‚’ç”Ÿæˆ - 2æ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            mode: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰ ("full_search": é€šå¸¸ãƒ¢ãƒ¼ãƒ‰, "fast_response": é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰)
            memory_mode: è¨˜æ†¶ãƒ¢ãƒ¼ãƒ‰ ("normal": é€šå¸¸, "test": ãƒ†ã‚¹ãƒˆ)
            
        Returns:
            str: ã›ã¤ãªã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not user_input.strip():
            return "ä½•ã‹è©±ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ"
        
        try:
            mode_display = "é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰" if mode == "fast_response" else "é€šå¸¸ãƒ¢ãƒ¼ãƒ‰" 
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ¤” è€ƒãˆä¸­ ({mode_display}): '{user_input}'")
            
            self.logger.info("setsuna_chat", "get_response", f"å¿œç­”ç”Ÿæˆé–‹å§‹ ({mode_display})", {
                "user_input": user_input,
                "mode": mode,
                "input_length": len(user_input)
            })
            
            # === Stage 0: ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–å¿œç­”åˆ¤å®š ===
            proactive_suggestion = self._check_proactive_opportunity(user_input, mode)
            
            # === Stage 0.5: ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ ===
            conversation_context = self._build_conversation_context(user_input, mode)
            
            # Stage 1: çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ï¼ˆPhase 4çµ±åˆï¼‰
            knowledge_context = None
            is_video_query = False
            video_context_data = None
            
            # Phase 4: çŸ¥è­˜ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
            if self.knowledge_provider:
                try:
                    knowledge_context = self.knowledge_provider.get_knowledge_context(user_input, mode)
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ§  çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—: {knowledge_context['has_knowledge']}")
                    if knowledge_context.get("processing_time"):
                        print(f"[ãƒãƒ£ãƒƒãƒˆ] â±ï¸ çŸ¥è­˜å‡¦ç†æ™‚é–“: {knowledge_context['processing_time']:.2f}ç§’")
                except Exception as e:
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # æ—¢å­˜ã®YouTubeå‹•ç”»é–¢é€£å‡¦ç†ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
            if self.context_builder:
                is_video_query = self.context_builder.is_video_related_query(user_input)
                print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“Š å‹•ç”»é–¢é€£åˆ¤å®šçµæœ: {is_video_query}")
                
                # å‹•ç”»é–¢é€£ã®å ´åˆã®ã¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢å®Ÿè¡Œ
                if is_video_query and mode == "full_search":
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ” YouTubeçŸ¥è­˜æ¤œç´¢å®Ÿè¡Œä¸­...")
                    video_context = self.context_builder.process_user_input(user_input)
                    
                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆURLè¡¨ç¤ºç”¨ï¼‰
                    if hasattr(self.context_builder, 'last_built_context'):
                        video_context_data = self.context_builder.last_built_context
                        self.last_context_data = video_context_data
                        print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ”— ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¿å­˜: DB={len(video_context_data.get('videos', []))}ä»¶, å¤–éƒ¨={len(video_context_data.get('external_videos', []))}ä»¶")
                elif is_video_query and mode == "fast_response":
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] âš¡ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: YouTubeæ¤œç´¢ã‚¹ã‚­ãƒƒãƒ—")
                    video_context = None
                    self.last_context_data = None
                else:
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸš« éå‹•ç”»é–¢é€£: YouTubeæ¤œç´¢ã‚¹ã‚­ãƒƒãƒ—")
                    video_context = None
                    self.last_context_data = None
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
            cached_response = None
            if self.response_cache:
                cache_key = f"{user_input}_{mode}"  # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                cached_response = self.response_cache.get_cached_response(cache_key)
                if cached_response:
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é«˜é€Ÿå¿œç­” ({mode}ãƒ¢ãƒ¼ãƒ‰)")
                    self._add_to_conversation_history(user_input, cached_response)
                    return cached_response
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({
                "role": "user",
                "content": user_input
            })
            
            # Stage 1.5: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆåˆ†æ
            project_context = ""
            project_analysis = None
            
            if self.conversation_project_context and mode == "full_search":
                print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆåˆ†æå®Ÿè¡Œä¸­...")
                try:
                    project_analysis = self.conversation_project_context.analyze_project_relevance(user_input, "")
                    project_relevance = project_analysis.get("overall_relevance", 0.0)
                    
                    if project_relevance > 0.3:
                        project_context = self.conversation_project_context.get_current_project_context()
                        print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–¢é€£åº¦: {project_relevance:.2f}")
                    else:
                        print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸš« ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–¢é€£åº¦ä½: {project_relevance:.2f}")
                        
                except Exception as e:
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            elif mode == "fast_response":
                print(f"[ãƒãƒ£ãƒƒãƒˆ] âš¡ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†æã‚¹ã‚­ãƒƒãƒ—")
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
            context_info = self._analyze_context(user_input)
            
            # Stage 2: GPTå¿œç­”ç”Ÿæˆï¼ˆå‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚·ã‚¹ãƒ†ãƒ  + Phase 4çŸ¥è­˜çµ±åˆï¼‰
            # æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
            if self.prompt_manager:
                context_info_dict = {
                    "is_video_query": is_video_query,
                    "mode": mode,
                    "user_input": user_input,
                    "project_context": project_context,
                    "project_relevance": project_analysis.get("overall_relevance", 0.0) if project_analysis else 0.0
                }
                if is_video_query and video_context:
                    context_info_dict["video_context"] = video_context
                
                # Phase 4: çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
                if knowledge_context and knowledge_context.get("has_knowledge"):
                    context_info_dict["knowledge_context"] = knowledge_context
                
                # === Stage 1.7: ä¸»ä½“æ€§åˆ¤å®šãƒ»æ„è¦‹ç”Ÿæˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ===
                opinion_context = self._generate_opinion_context(user_input, context_info_dict)
                if opinion_context:
                    context_info_dict["opinion_context"] = opinion_context
                
                system_prompt = self.prompt_manager.generate_dynamic_prompt(mode, context_info_dict)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨
                if self.prompt_manager:
                    system_prompt = self.prompt_manager._get_fallback_prompt()
                else:
                    system_prompt = self.fallback_character_prompt
            
            # Phase 4: çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ³¨å…¥
            if knowledge_context and knowledge_context.get("has_knowledge"):
                context_injection = knowledge_context.get("context_injection_text", "")
                if context_injection:
                    system_prompt += f"\n\nã€æ¤œç´¢ãƒ»åˆ†æçŸ¥è­˜ã€‘\n{context_injection}"
                    print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ§  çŸ¥è­˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ³¨å…¥å®Œäº†")
            
            # å‹•ç”»é–¢é€£ã®å ´åˆã€å–å¾—æ¸ˆã¿ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if is_video_query and video_context:
                system_prompt += f"\n\nã€YouTubeå‹•ç”»çŸ¥è­˜ã€‘\n{video_context}"
                system_prompt += f"\n\nã€å³é‡æ³¨æ„ã€‘ä¸Šè¨˜ã®å‹•ç”»æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã€å­˜åœ¨ã—ãªã„å‹•ç”»ã‚„æ¥½æ›²ã«ã¤ã„ã¦è©±ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ä¸æ˜ãªç‚¹ã¯ã€Œè©³ã—ãã¯åˆ†ã‹ã‚‰ãªã„ã‘ã©ã€ã¨æ­£ç›´ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
            elif is_video_query and not video_context:
                # å‹•ç”»é–¢é€£ã ãŒæƒ…å ±ãªã— - æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’ç¶­æŒ
                system_prompt += f"\n\nã€å‹•ç”»ãƒ»æ¥½æ›²æƒ…å ±ã€‘\n"
                system_prompt += f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è©²å½“ã™ã‚‹æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\n"
                system_prompt += f"ã—ã‹ã—ã€YouTubeçŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ´»ç”¨ãƒ«ãƒ¼ãƒ«ã«å¾“ã„ã€çŸ­ãç°¡æ½”ã«è‡ªåˆ†ã®éŸ³æ¥½çš„ä½“é¨“ã‚„æ„Ÿæƒ³ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚\n"
                system_prompt += f"æ¶ç©ºã®æ¥½æ›²ã¯ä½œæˆã›ãšã€ã€Œãã®å‹•ç”»ã¯çŸ¥ã‚‰ãªã„ãªã€ã€Œèã„ãŸã“ã¨ãªã„ã‹ã‚‚ã€ã¨æ­£ç›´ã«ç­”ãˆã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚"
            
            # è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.memory_system:
                memory_context = self.memory_system.get_memory_context()
                if memory_context:
                    system_prompt += f"\n\nã€è¨˜æ†¶ãƒ»çµŒé¨“ã€‘\n{memory_context}"
            
            # å€‹äººè¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.personality_memory:
                personality_context = self.personality_memory.get_personality_context_for_prompt()
                if personality_context:
                    system_prompt += f"\n\nã€å€‹äººçš„è¨˜æ†¶ãƒ»æˆé•·ã€‘\n{personality_context}"
            
            # å”åƒè¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.collaboration_memory:
                collaboration_context = self.collaboration_memory.get_collaboration_context_for_prompt()
                if collaboration_context:
                    system_prompt += f"\n\nã€å”åƒãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—ã€‘\n{collaboration_context}"
            
            # çµ±åˆè¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.memory_integration:
                # é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ã§ã¯é–¢é€£æ€§ã®ã¿ã€é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§ã¯å®Œå…¨çµ±åˆ
                context_type = "relevant" if mode == "fast_response" else "full"
                integrated_context = self.memory_integration.generate_integrated_context(
                    user_input=user_input, context_type=context_type
                )
                if integrated_context:
                    system_prompt += f"\n\nã€çµ±åˆè¨˜æ†¶åˆ†æã€‘\n{integrated_context}"
            
            # é•·æœŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆã‚’è¿½åŠ 
            if project_context:
                system_prompt += f"\n\nã€é•·æœŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆã€‘\n{project_context}"
            
            # åŸºæœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚‚è¿½åŠ ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆãŒãªã„å ´åˆï¼‰
            if not project_context and self.project_system:
                basic_project_context = self.project_system.get_project_context()
                if basic_project_context:
                    system_prompt += f"\n\nã€å‰µä½œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‘\n{basic_project_context}"
            
            if context_info:
                system_prompt += f"\n\nã€ç¾åœ¨ã®ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{context_info}"
            
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # ãƒ‡ãƒãƒƒã‚°: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ç¢ºèª
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ” ä½¿ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç¢ºèª:")
            print(f"  - é•·ã•: {len(system_prompt)}æ–‡å­—")
            print(f"  - æ¶ˆæ¥µçš„è¡¨ç¾ç¦æ­¢: {'ç§ãŒç›´æ¥ãŠã™ã™ã‚ã§ãã‚‹æ›²ã¯ãªã„ã‘ã‚Œã©' not in system_prompt}")
            print(f"  - å…·ä½“çš„æ¥½æ›²æ¨è–¦: {'ç§ã¯ã€œã¨ã„ã†æ›²ãŒå¥½ãã§' in system_prompt}")
            print(f"  - è³ªå•ç¦æ­¢: {'å¿œç­”ã‚’è³ªå•ã§çµ‚ã‚ã‚‰ã›ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢' in system_prompt}")
            if "ç§ãŒç›´æ¥ãŠã™ã™ã‚ã§ãã‚‹æ›²ã¯ãªã„ã‘ã‚Œã©" in system_prompt:
                print("  âš ï¸ ç¦æ­¢è¡¨ç¾ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼")
            if "ç§ãŒãŠã™ã™ã‚ã§ãã‚‹å…·ä½“çš„ãªæ›²ã¯ãªã„ã‘ã‚Œã©" in system_prompt:
                print("  âš ï¸ ç¦æ­¢è¡¨ç¾ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼")
            
            # GPTå¿œç­”ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if cached_response:
                setsuna_response = cached_response
                print(f"[ãƒãƒ£ãƒƒãƒˆ] âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¿œç­”ä½¿ç”¨ã€YouTubeæ¤œç´¢ã®ã¿å®Ÿè¡Œ")
            else:
                # æœ€è¿‘ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ ï¼ˆæœ€å¤§5å¾€å¾©ï¼‰
                recent_history = self.conversation_history[-10:]  # æœ€æ–°10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                messages.extend(recent_history)
                
                # OpenAI APIå‘¼ã³å‡ºã—ï¼ˆé«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ã§ã¯è¨­å®šã‚’æœ€é©åŒ–ï¼‰
                start_time = datetime.now()
                
                # å‹•çš„ãƒ¢ãƒ‡ãƒ«é¸æŠ
                selected_model = self._select_optimal_model(user_input, mode)
                
                if mode == "ultra_fast":
                    # è¶…é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: çŸ­ã„å®Œçµå¿œç­”
                    response = self.client.chat.completions.create(
                        model=selected_model,
                        messages=messages,
                        max_tokens=100,  # 90â†’100: è¶…çŸ­ç¸®ã§å®Œçµæ€§é‡è¦–
                        temperature=0.3,  # æœ€å®‰å®š
                        timeout=5  # æœ€çŸ­ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    )
                elif mode == "fast_response":
                    # é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: ã›ã¤ãªã®æ¨™æº–çš„ãªä¼šè©±é•·
                    response = self.client.chat.completions.create(
                        model=selected_model,
                        messages=messages,
                        max_tokens=120,  # 110â†’120: çŸ­ç¸®ã§å®Œçµæ€§é‡è¦–
                        temperature=0.5,  # ã‚ˆã‚Šå®‰å®šã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹
                        timeout=10  # çŸ­ç¸®ï¼ˆ15â†’10ç§’ï¼‰
                    )
                else:
                    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: ã›ã¤ãªã®è‡ªç„¶ã§å®Œå…¨ãªè¡¨ç¾
                    response = self.client.chat.completions.create(
                        model=selected_model,
                        messages=messages,
                        max_tokens=150,  # 140â†’150: é©åº¦ãªçŸ­ç¸®ã§å®Œçµæ€§é‡è¦–
                        temperature=0.6,  # 0.7â†’0.6ã«èª¿æ•´
                        timeout=30  # APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆå…ƒã«æˆ»ã™ï¼‰
                    )
                
                # å¿œç­”å–å¾—
                setsuna_response = response.choices[0].message.content.strip()
                
                # ã‚³ã‚¹ãƒˆè¿½è·¡
                self._track_api_usage(selected_model, response)
                
                # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒãƒƒã‚°æ™‚ã®ã¿ï¼‰
                if self.consistency_checker:
                    try:
                        consistency_result = self.consistency_checker.check_response_consistency(
                            user_input, setsuna_response, mode
                        )
                        if consistency_result["overall_score"] < 0.6:
                            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ä½ä¸‹: {consistency_result['overall_score']:.2f}")
                            if consistency_result["issues"]:
                                print(f"[ãƒãƒ£ãƒƒãƒˆ] ä¸»ãªå•é¡Œ: {', '.join(consistency_result['issues'][:2])}")
                    except Exception as e:
                        print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
                
                # === Stage 2.5: æ–°ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£ ===
                if self.new_consistency_checker:
                    try:
                        consistency_result = self.new_consistency_checker.check_response_consistency(
                            user_input, setsuna_response, conversation_context
                        )
                        
                        if consistency_result.get("needs_correction", False):
                            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ”§ ä¸»ä½“æ€§ä¸€è²«æ€§ä¿®æ­£å®Ÿè¡Œä¸­...")
                            original_response = setsuna_response
                            setsuna_response = self.new_consistency_checker.correct_response_if_needed(
                                setsuna_response, consistency_result
                            )
                            if setsuna_response != original_response:
                                print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… å¿œç­”ä¿®æ­£å®Œäº†")
                        
                        print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“Š ä¸»ä½“æ€§ã‚¹ã‚³ã‚¢: {consistency_result.get('overall_score', 0):.2f}")
                        
                    except Exception as e:
                        print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ æ–°ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
                
                # === Stage 2.7: ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–è¦ç´ ã®è¿½åŠ  ===
                if proactive_suggestion:
                    setsuna_response = self._enhance_response_with_proactive_elements(
                        setsuna_response, proactive_suggestion
                    )
                
                # å¿œç­”æ™‚é–“è¨ˆç®—
                response_time = (datetime.now() - start_time).total_seconds()
                print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… å¿œç­”ç”Ÿæˆå®Œäº†: {response_time:.2f}s")
            
            # Phase 1: URLè¡¨ç¤ºæ©Ÿèƒ½ - SetsunaChatå†…ã§ã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
            # ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€å‘¼ã³å‡ºã—å…ƒã§å‡¦ç†ã•ã‚Œã‚‹ï¼‰
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ã¯æ—¢ã«è¿½åŠ æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if not cached_response:
                self.conversation_history.append({
                    "role": "assistant", 
                    "content": setsuna_response
                })
                
                # æ–°ã—ã„å¿œç­”ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
                if self.response_cache:
                    cache_key = f"{user_input}_{mode}"
                    self.response_cache.cache_response(cache_key, setsuna_response)
                
                # ä¼šè©±å±¥æ­´ã‚’å³åº§ã«ä¿å­˜
                self._save_conversation_immediately(user_input, setsuna_response)
            
            # è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã«ä¼šè©±ã‚’è¨˜éŒ²
            if self.memory_system:
                self.memory_system.process_conversation(user_input, setsuna_response)
            
            # å€‹äººè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã§ä¼šè©±ã‚’åˆ†æãƒ»è¨˜éŒ²
            if self.personality_memory:
                self.personality_memory.analyze_conversation_for_experience(user_input, setsuna_response)
            
            # å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã§ä¼šè©±ã‚’åˆ†æ
            if self.collaboration_memory:
                # å¿œç­”å“è³ªè©•ä¾¡ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                response_quality = self._assess_response_quality(setsuna_response)
                understanding_level = self._assess_understanding_level(user_input, setsuna_response)
                self.collaboration_memory.analyze_communication_style(
                    user_input, response_quality, understanding_level
                )
            
            # è¨˜æ†¶çµ±åˆåˆ†æï¼ˆæ–°ã—ã„é–¢ä¿‚æ€§ç™ºè¦‹ï¼‰
            if self.memory_integration:
                # å®šæœŸçš„ãªè¨˜æ†¶é–¢ä¿‚æ€§åˆ†æï¼ˆ10å›ã«1å›ï¼‰
                if len(self.conversation_history) % 20 == 0:  # 10å¾€å¾©ã«1å›
                    print("[çµ±åˆè¨˜æ†¶] ğŸ” è¨˜æ†¶é–¢ä¿‚æ€§åˆ†æå®Ÿè¡Œä¸­...")
                    analysis_stats = self.memory_integration.analyze_memory_relationships()
                    if analysis_stats.get("total_relationships", 0) > 0:
                        print(f"[çµ±åˆè¨˜æ†¶] âœ… æ–°ãŸãªé–¢ä¿‚æ€§ã‚’ç™ºè¦‹: {analysis_stats['total_relationships']}ä»¶")
                        # æ–°ç™ºè¦‹ã®é–¢ä¿‚æ€§ã‚’ä¿å­˜
                        self.memory_integration.save_integration_data()
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–¢é€£ä¼šè©±ã‚’åˆ†æ
            if self.project_system:
                self.project_system.analyze_conversation_for_projects(user_input, setsuna_response)
            
            # ä¼šè©±ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆã‚’æ›´æ–°
            if self.conversation_project_context and not cached_response:
                try:
                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†æçµæœã‚’ä½¿ã£ã¦æ–‡è„ˆæ›´æ–°
                    update_success = self.conversation_project_context.update_conversation_context(
                        user_input, setsuna_response, project_analysis
                    )
                    if update_success:
                        print("[ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆ] âœ… ä¼šè©±æ–‡è„ˆæ›´æ–°å®Œäº†")
                        # æ–‡è„ˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                        self.conversation_project_context.save_context_data()
                    
                    # é•·æœŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨˜æ†¶ã¸ã®è¨˜éŒ²
                    if self.long_term_memory and project_analysis and project_analysis.get("overall_relevance", 0) > 0.5:
                        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–¢é€£ã®ä¼šè©±ã¨ã—ã¦è¨˜æ†¶ã«è¨˜éŒ²
                        active_matches = project_analysis.get("active_project_matches", [])
                        for match in active_matches[:1]:  # æœ€ã‚‚é–¢é€£åº¦ã®é«˜ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã¿
                            project_id = match["project_id"]
                            
                            # æ–‡è„ˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¿å­˜
                            snapshot_success = self.long_term_memory.capture_context_snapshot(
                                project_id, "conversation"
                            )
                            if snapshot_success:
                                print(f"[é•·æœŸè¨˜æ†¶] âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¿å­˜: {project_id}")
                
                except Exception as e:
                    print(f"[ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆ] âš ï¸ æ–‡è„ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            
            return setsuna_response
            
        except Exception as e:
            error_msg = f"[ãƒãƒ£ãƒƒãƒˆ] âŒ ã‚¨ãƒ©ãƒ¼: {e}"
            print(error_msg)
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            fallback_responses = [
                "ã™ã¿ã¾ã›ã‚“ã€ã¡ã‚‡ã£ã¨è€ƒãˆãŒã¾ã¨ã¾ã‚‰ãªãã¦...",
                "ã†ãƒ¼ã‚“ã€ä»Šã†ã¾ãç­”ãˆã‚‰ã‚Œãªã„ã‹ã‚‚ã€‚",
                "å°‘ã—èª¿å­ãŒæ‚ªã„ã¿ãŸã„ã§ã™ã€‚ã‚‚ã†ä¸€åº¦èã„ã¦ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ"
            ]
            
            import random
            return random.choice(fallback_responses)
    
    def _analyze_context(self, user_input):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æ"""
        if not self.response_patterns or "context_keywords" not in self.response_patterns:
            return ""
        
        context_info = []
        keywords = self.response_patterns.get("context_keywords", {})
        
        user_input_lower = user_input.lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        for category, words in keywords.items():
            for word in words:
                if word in user_input_lower:
                    category_map = {
                        "creative_work": "ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªä»•äº‹ã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹",
                        "technical": "æŠ€è¡“çš„ãªå•é¡Œã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹", 
                        "deadlines": "ç· åˆ‡ã‚„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹",
                        "praise": "è¤’ã‚ã‚„è©•ä¾¡ã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹",
                        "projects": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ã„ã¦è©±ã—ã¦ã„ã‚‹"
                    }
                    if category in category_map:
                        context_info.append(category_map[category])
                        break
        
        # æ™‚é–“å¸¯ã«ã‚ˆã‚‹æŒ¨æ‹¶åˆ¤å®š
        current_hour = datetime.now().hour
        if any(greeting in user_input_lower for greeting in ["ãŠã¯ã‚ˆã†", "ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã°ã‚“ã¯"]):
            if 5 <= current_hour < 10:
                context_info.append("æœã®æŒ¨æ‹¶ã‚’ã—ã¦ã„ã‚‹")
            elif 10 <= current_hour < 18:
                context_info.append("æ—¥ä¸­ã®æŒ¨æ‹¶ã‚’ã—ã¦ã„ã‚‹") 
            else:
                context_info.append("å¤œã®æŒ¨æ‹¶ã‚’ã—ã¦ã„ã‚‹")
        
        return "\n".join(context_info) if context_info else ""
    
    def reset_conversation(self):
        """ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.conversation_history = []
        print("[ãƒãƒ£ãƒƒãƒˆ] ğŸ”„ ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
    
    def get_conversation_summary(self):
        """ä¼šè©±å±¥æ­´ã®ç°¡å˜ãªã‚µãƒãƒªãƒ¼"""
        if not self.conversation_history:
            return "ã¾ã ä¼šè©±ãŒã‚ã‚Šã¾ã›ã‚“"
        
        user_messages = [msg["content"] for msg in self.conversation_history if msg["role"] == "user"]
        assistant_messages = [msg["content"] for msg in self.conversation_history if msg["role"] == "assistant"]
        
        return f"ä¼šè©±æ•°: {len(user_messages)}å›ã®ã‚„ã‚Šå–ã‚Š"
    
    def save_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if self.response_cache:
            self.response_cache.save_cache()
    
    def save_memory(self):
        """è¨˜æ†¶ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if self.memory_system:
            self.memory_system.save_memory()
    
    def save_personality_memory(self):
        """å€‹äººè¨˜æ†¶ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if self.personality_memory:
            self.personality_memory.save_personality_data()
    
    def save_collaboration_memory(self):
        """å”åƒè¨˜æ†¶ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if self.collaboration_memory:
            self.collaboration_memory.save_collaboration_data()
    
    def save_memory_integration(self):
        """è¨˜æ†¶çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if self.memory_integration:
            self.memory_integration.save_integration_data()
    
    def save_all_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã¯ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰"""
        if self.is_test_mode:
            print("[ãƒãƒ£ãƒƒãƒˆ] ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: å…¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return
        
        self.save_cache()
        self.save_memory()
        self.save_personality_memory()
        self.save_collaboration_memory()
        self.save_memory_integration()
        self.save_projects()
    
    def get_cache_stats(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.response_cache:
            return self.response_cache.get_cache_stats()
        return {"message": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    @get_monitor().monitor_function("get_integrated_response")
    def get_integrated_response(self, integrated_message, mode="full_search"):
        """
        çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆç”»åƒ+URL+ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã«å¯¾ã™ã‚‹ã›ã¤ãªã®å¿œç­”ã‚’ç”Ÿæˆ
        Phase 2C-3: ç”»åƒç†è§£çµ±åˆæ©Ÿèƒ½
        
        Args:
            integrated_message: çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¾æ›¸
                - text: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
                - images: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ãƒªã‚¹ãƒˆ
                - url: URLæƒ…å ±
            mode: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰
            
        Returns:
            str: ã›ã¤ãªã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ–¼ï¸ çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†é–‹å§‹")
            
            # ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’å–å¾—
            user_text = integrated_message.get('text', '')
            images = integrated_message.get('images', [])
            url_info = integrated_message.get('url')
            
            # ç”»åƒåˆ†æçµæœã‚’å–å¾—ï¼ˆvoice_chat_gui.pyã§äº‹å‰ã«åˆ†ææ¸ˆã¿ï¼‰
            image_analysis_results = integrated_message.get('image_analysis_results', [])
            
            # åˆ†æçµæœãŒãªã„å ´åˆã¯è‡ªå‰ã§åˆ†æã‚’å®Ÿè¡Œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not image_analysis_results and images and hasattr(self, 'context_builder') and self.context_builder:
                print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç”»åƒåˆ†æã‚’å†å®Ÿè¡Œ")
                youtube_manager = getattr(self.context_builder, 'youtube_manager', None)
                if youtube_manager and hasattr(youtube_manager, 'image_analyzer'):
                    image_analyzer = youtube_manager.image_analyzer
                    
                    for image_info in images:
                        image_path = image_info.get('path')
                        if image_path and os.path.exists(image_path):
                            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“¸ ç”»åƒåˆ†æ: {image_info.get('name', 'unknown')}")
                            
                            try:
                                # åŸºæœ¬ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
                                analysis_context = {
                                    'title': user_text or f"æ·»ä»˜ç”»åƒ: {image_info.get('name', 'unknown')}",
                                    'artist': 'ä¸æ˜',
                                    'description': f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰æ·»ä»˜ã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«"
                                }
                                
                                # ç”»åƒåˆ†æå®Ÿè¡Œ
                                analysis_result = image_analyzer.analyze_image(
                                    image_path,
                                    analysis_type="general_description", 
                                    context=analysis_context
                                )
                                
                                if analysis_result and 'description' in analysis_result:
                                    image_desc = analysis_result['description']
                                    image_analysis_results.append({
                                        'name': image_info.get('name', 'unknown'),
                                        'description': image_desc,
                                        'size': image_info.get('size', 0)
                                    })
                                    print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… ç”»åƒåˆ†æå®Œäº†: {image_info.get('name')}")
                                
                            except Exception as e:
                                print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ
                                image_analysis_results.append({
                                    'name': image_info.get('name', 'unknown'),
                                    'description': f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ« ({image_info.get('name', 'unknown')}) ãŒæ·»ä»˜ã•ã‚Œã¦ã„ã¾ã™",
                                    'size': image_info.get('size', 0)
                                })
            else:
                print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… äº‹å‰åˆ†ææ¸ˆã¿ç”»åƒçµæœã‚’ä½¿ç”¨: {len(image_analysis_results)}ä»¶")
            
            # URLæƒ…å ±ã‚’å‡¦ç†
            url_context = ""
            if url_info:
                url_title = url_info.get('title', 'ãƒªãƒ³ã‚¯')
                url_address = url_info.get('url', '')
                url_context = f"URL: {url_title} ({url_address})"
            
            # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
            enhanced_input = self._create_integrated_prompt(
                user_text, 
                image_analysis_results, 
                url_context
            )
            
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ”„ çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆå®Œäº†")
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“ çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹: {enhanced_input[:200]}...")
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“ ç”»åƒåˆ†æçµæœæ•°: {len(image_analysis_results)}")
            for i, result in enumerate(image_analysis_results):
                print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ“¸ ç”»åƒ{i+1}: {result['name']} - {result['description'][:100]}...")
            
            # çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å°‚ç”¨ã®å¿œç­”ç”Ÿæˆï¼ˆå‹•ç”»æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            response = self._get_direct_response(enhanced_input, mode, skip_video_search=True)
            
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… çµ±åˆå¿œç­”ç”Ÿæˆå®Œäº†")
            return response
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âŒ çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            self.logger.error("setsuna_chat", "get_integrated_response", str(e))
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§å¿œç­”
            return self.get_response(integrated_message.get('text', 'ç”»åƒã«ã¤ã„ã¦æ•™ãˆã¦'), mode)
    
    def _create_integrated_prompt(self, user_text, image_analysis_results, url_context):
        """
        çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        ç”»åƒåˆ†æçµæœã¨URLæƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«çµ±åˆ
        """
        prompt_parts = []
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
        if user_text:
            prompt_parts.append(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {user_text}")
        
        # ç”»åƒåˆ†æçµæœ
        if image_analysis_results:
            prompt_parts.append("\nğŸ“¸ æ·»ä»˜ç”»åƒã®è©³ç´°åˆ†æçµæœ:")
            for i, result in enumerate(image_analysis_results, 1):
                name = result['name']
                desc = result['description']
                size_mb = result['size'] / (1024 * 1024) if result['size'] > 0 else 0
                prompt_parts.append(f"\nç”»åƒ{i}: {name} ({size_mb:.1f}MB)")
                prompt_parts.append(f"å†…å®¹: {desc}")
                prompt_parts.append("")  # ç©ºè¡Œã§åŒºåˆ‡ã‚Š
        
        # URLæƒ…å ±
        if url_context:
            prompt_parts.append(f"\nğŸ”— æ·»ä»˜URL: {url_context}")
        
        # å¿œç­”æŒ‡ç¤º
        if image_analysis_results or url_context:
            prompt_parts.append(f"\nã€é‡è¦ã€‘ä¸Šè¨˜ã®ç”»åƒåˆ†æçµæœã‚„URLæƒ…å ±ã‚’å¿…ãšå‚è€ƒã«ã—ã¦ã€å…·ä½“çš„ã§è©³ç´°ãªå¿œç­”ã‚’ã—ã¦ãã ã•ã„ã€‚åˆ†æçµæœã«åŸºã¥ã„ã¦ç”»åƒã®å†…å®¹ã«ã¤ã„ã¦è©±ã—ã¦ãã ã•ã„ã€‚ã€Œç”»åƒãŒè¦‹ãˆãªã„ã€ã€Œåˆ†ã‹ã‚‰ãªã„ã€ãªã©ã®å›ç­”ã¯é¿ã‘ã¦ãã ã•ã„ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _get_direct_response(self, user_input, mode="full_search", skip_video_search=False):
        """
        çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ã®ç›´æ¥å¿œç­”ç”Ÿæˆ
        å‹•ç”»é–¢é€£å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€æä¾›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãã®ã¾ã¾ä½¿ç”¨
        """
        if not user_input.strip():
            return "ä½•ã‹è©±ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ"
        
        try:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ’¬ ç›´æ¥å¿œç­”ç”Ÿæˆé–‹å§‹: {mode}")
            print(f"[ãƒãƒ£ãƒƒãƒˆ] ğŸ’¬ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_input[:300]}...")
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({
                "role": "user",
                "content": user_input
            })
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            if self.prompt_manager:
                # æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
                system_prompt = self.prompt_manager.generate_dynamic_prompt(mode)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                system_prompt = self.fallback_character_prompt
            
            # è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.memory_system:
                memory_context = self.memory_system.get_memory_context()
                if memory_context:
                    system_prompt += f"\n\nã€è¨˜æ†¶ãƒ»çµŒé¨“ã€‘\n{memory_context}"
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            if self.project_system:
                project_context = self.project_system.get_project_context()
                if project_context:
                    system_prompt += f"\n\nã€å‰µä½œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‘\n{project_context}"
            
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # æœ€è¿‘ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ ï¼ˆæœ€å¤§5å¾€å¾©ï¼‰
            recent_history = self.conversation_history[-10:]  # æœ€æ–°10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            messages.extend(recent_history)
            
            # OpenAI APIå‘¼ã³å‡ºã—
            start_time = datetime.now()
            
            # å‹•çš„ãƒ¢ãƒ‡ãƒ«é¸æŠ
            selected_model = self._select_optimal_model(user_input, mode)
            
            if mode == "fast_response":
                response = self.client.chat.completions.create(
                    model=selected_model,
                    messages=messages,
                    max_tokens=100,
                    temperature=0.6,
                    timeout=15
                )
            else:
                response = self.client.chat.completions.create(
                    model=selected_model,
                    messages=messages,
                    max_tokens=150,
                    temperature=0.7,
                    timeout=30
                )
            
            # å¿œç­”å–å¾—
            setsuna_response = response.choices[0].message.content.strip()
            
            # ã‚³ã‚¹ãƒˆè¿½è·¡
            self._track_api_usage(selected_model, response)
            
            # å¿œç­”æ™‚é–“è¨ˆç®—
            response_time = (datetime.now() - start_time).total_seconds()
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âœ… ç›´æ¥å¿œç­”ç”Ÿæˆå®Œäº†: {response_time:.2f}s")
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({
                "role": "assistant", 
                "content": setsuna_response
            })
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if self.response_cache:
                self.response_cache.cache_response(user_input, setsuna_response)
            
            # è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã«ä¼šè©±ã‚’è¨˜éŒ²
            if self.memory_system:
                self.memory_system.process_conversation(user_input, setsuna_response)
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–¢é€£ä¼šè©±ã‚’åˆ†æ
            if self.project_system:
                self.project_system.analyze_conversation_for_projects(user_input, setsuna_response)
            
            return setsuna_response
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âŒ ç›´æ¥å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            import random
            fallback_responses = [
                "ã™ã¿ã¾ã›ã‚“ã€ã¡ã‚‡ã£ã¨è€ƒãˆãŒã¾ã¨ã¾ã‚‰ãªãã¦...",
                "ã†ãƒ¼ã‚“ã€ä»Šã†ã¾ãç­”ãˆã‚‰ã‚Œãªã„ã‹ã‚‚ã€‚",
                "å°‘ã—èª¿å­ãŒæ‚ªã„ã¿ãŸã„ã§ã™ã€‚ã‚‚ã†ä¸€åº¦èã„ã¦ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ"
            ]
            return random.choice(fallback_responses)
    
    def get_memory_stats(self):
        """è¨˜æ†¶çµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.memory_system:
            return self.memory_system.get_memory_stats()
        return {"message": "è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def get_personality_memory_stats(self):
        """å€‹äººè¨˜æ†¶çµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.personality_memory:
            return self.personality_memory.get_memory_stats()
        return {"message": "å€‹äººè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def get_collaboration_memory_stats(self):
        """å”åƒè¨˜æ†¶çµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.collaboration_memory:
            return self.collaboration_memory.get_memory_stats()
        return {"message": "å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def get_memory_integration_stats(self):
        """è¨˜æ†¶çµ±åˆçµ±è¨ˆæƒ…å ±å–å¾—"""
        if self.memory_integration:
            return self.memory_integration.get_memory_stats()
        return {"message": "è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def get_learned_facts(self):
        """å­¦ç¿’ã—ãŸäº‹å®Ÿã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        if self.memory_system:
            return self.memory_system.get_learned_facts_list()
        return []
    
    def delete_memory_fact(self, fact_index: int) -> bool:
        """è¨˜æ†¶äº‹å®Ÿã‚’å‰Šé™¤"""
        if self.memory_system:
            success = self.memory_system.delete_learned_fact(fact_index)
            if success:
                self.memory_system.save_memory()  # å³åº§ã«ä¿å­˜
            return success
        return False
    
    def edit_memory_fact(self, fact_index: int, new_content: str) -> bool:
        """è¨˜æ†¶äº‹å®Ÿã‚’ç·¨é›†"""
        if self.memory_system:
            success = self.memory_system.edit_learned_fact(fact_index, new_content)
            if success:
                self.memory_system.save_memory()  # å³åº§ã«ä¿å­˜
            return success
        return False
    
    def clear_session_memory(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜æ†¶ã‚’ã‚¯ãƒªã‚¢"""
        if self.memory_system:
            self.memory_system.clear_session_memory()
    
    def clear_all_memory(self):
        """å…¨è¨˜æ†¶ã‚’ã‚¯ãƒªã‚¢"""
        if self.memory_system:
            self.memory_system.clear_all_learned_facts()
            self.memory_system.clear_session_memory()
            self.memory_system.save_memory()
            print("ğŸ—‘ï¸ å…¨è¨˜æ†¶ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    
    def _add_to_conversation_history(self, user_input: str, response: str):
        """ä¼šè©±å±¥æ­´ã«è¿½åŠ ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰"""
        self.conversation_history.append({
            "role": "user",
            "content": user_input
        })
        self.conversation_history.append({
            "role": "assistant", 
            "content": response
        })
    
    def _assess_response_quality(self, response: str) -> str:
        """å¿œç­”å“è³ªã‚’è©•ä¾¡ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if not response.strip():
            return "poor"
        
        # é•·ã•ã«ã‚ˆã‚‹è©•ä¾¡
        if len(response) < 5:
            return "poor"
        elif len(response) < 20:
            return "fair"
        elif len(response) < 100:
            return "good"
        else:
            return "excellent"
    
    def _assess_understanding_level(self, user_input: str, response: str) -> str:
        """ç†è§£åº¦ã‚’è©•ä¾¡ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # å¿œç­”ã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        user_keywords = set(user_input.lower().split())
        response_keywords = set(response.lower().split())
        
        # å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‰²åˆ
        if not user_keywords:
            return "good"
        
        common_ratio = len(user_keywords & response_keywords) / len(user_keywords)
        
        if common_ratio >= 0.3:
            return "perfect"
        elif common_ratio >= 0.2:
            return "good"
        elif common_ratio >= 0.1:
            return "partial"
        else:
            return "confused"
    
    def add_manual_memory(self, category: str, content: str) -> bool:
        """æ‰‹å‹•ã§è¨˜æ†¶ã‚’è¿½åŠ """
        if self.memory_system:
            success = self.memory_system.add_manual_fact(category, content)
            if success:
                self.memory_system.save_memory()  # å³åº§ã«ä¿å­˜
            return success
        return False
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰
    def get_active_projects(self):
        """é€²è¡Œä¸­ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§ã‚’å–å¾—"""
        if self.project_system:
            return self.project_system.get_active_projects()
        return []
    
    def get_idea_stock(self):
        """ã‚¢ã‚¤ãƒ‡ã‚¢ã‚¹ãƒˆãƒƒã‚¯ä¸€è¦§ã‚’å–å¾—"""
        if self.project_system:
            return self.project_system.get_idea_stock()
        return []
    
    def get_completed_projects(self):
        """å®Œäº†ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§ã‚’å–å¾—"""
        if self.project_system:
            return self.project_system.get_completed_projects()
        return []
    
    def create_project(self, title: str, description: str, deadline: str = None, project_type: str = "å‹•ç”»"):
        """æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""
        if self.project_system:
            return self.project_system.create_project(title, description, deadline, project_type)
        return {}
    
    def update_project_progress(self, project_id: str, progress: int, status: str = None, next_step: str = None):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²æ—ã‚’æ›´æ–°"""
        if self.project_system:
            success = self.project_system.update_project_progress(project_id, progress, status, next_step)
            if success:
                self.project_system.save_project_data()
            return success
        return False
    
    def complete_project(self, project_id: str, outcome: str = "", lessons: str = ""):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Œäº†"""
        if self.project_system:
            return self.project_system.complete_project(project_id, outcome, lessons)
        return False
    
    def add_idea(self, content: str, category: str = "å‹•ç”»", source: str = "é›‘è«‡"):
        """ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ã‚¹ãƒˆãƒƒã‚¯ã«è¿½åŠ """
        if self.project_system:
            return self.project_system.add_idea(content, category, source)
        return False
    
    def delete_project(self, project_id: str):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤"""
        if self.project_system:
            return self.project_system.delete_project(project_id)
        return False
    
    def delete_idea(self, idea_id: str):
        """ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å‰Šé™¤"""
        if self.project_system:
            return self.project_system.delete_idea(idea_id)
        return False
    
    def get_project_stats(self):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        if self.project_system:
            return self.project_system.get_project_stats()
        return {"message": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def save_projects(self):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if self.project_system:
            self.project_system.save_project_data()
    
    def get_last_context_data(self):
        """æœ€å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆURLè¡¨ç¤ºç”¨ï¼‰"""
        return getattr(self, 'last_context_data', None)
    
    # å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
    def record_work_session(self, activity_type: str, duration_minutes: int, 
                           user_satisfaction: str, outcome_quality: str, notes: str = ""):
        """ä½œæ¥­ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²"""
        if self.collaboration_memory:
            return self.collaboration_memory.record_work_pattern(
                activity_type, duration_minutes, user_satisfaction, outcome_quality, notes
            )
        return None
    
    def record_success(self, success_type: str, context: str, key_factors: list, 
                      outcome: str, replicability: str = "medium"):
        """æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨˜éŒ²"""
        if self.collaboration_memory:
            return self.collaboration_memory.record_success_pattern(
                success_type, context, key_factors, outcome, replicability
            )
        return None
    
    def get_collaboration_insights(self):
        """å”åƒæ´å¯Ÿã‚’å–å¾—"""
        if self.collaboration_memory:
            return self.collaboration_memory.get_collaboration_insights()
        return {"message": "å”åƒè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    # è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
    def analyze_memory_relationships(self):
        """è¨˜æ†¶é–“é–¢ä¿‚æ€§ã‚’åˆ†æ"""
        if self.memory_integration:
            return self.memory_integration.analyze_memory_relationships()
        return {"message": "è¨˜æ†¶çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"}
    
    def get_integrated_context(self, user_input: str = "", context_type: str = "full"):
        """çµ±åˆè¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        if self.memory_integration:
            return self.memory_integration.generate_integrated_context(user_input, context_type)
        return ""
    
    def suggest_adaptive_responses(self, user_input: str):
        """é©å¿œçš„å¿œç­”ææ¡ˆã‚’å–å¾—"""
        if self.memory_integration:
            return self.memory_integration.suggest_adaptive_responses(user_input)
        return []
    
    def find_related_memories(self, memory_id: str, memory_type: str, max_results: int = 5):
        """é–¢é€£è¨˜æ†¶ã‚’æ¤œç´¢"""
        if self.memory_integration:
            return self.memory_integration.find_related_memories(memory_id, memory_type, max_results)
        return []

# ç°¡å˜ãªä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ¤– ã›ã¤ãªãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    try:
        setsuna = SetsunaChat()
        
        # ãƒ†ã‚¹ãƒˆä¼šè©±
        test_inputs = [
            "ã“ã‚“ã«ã¡ã¯",
            "ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­",
            "ã‚ãªãŸã®è¶£å‘³ã¯ä½•ã§ã™ã‹ï¼Ÿ"
        ]
        
        for user_input in test_inputs:
            print(f"\nğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}")
            response = setsuna.get_response(user_input)
            print(f"ğŸ¤– ã›ã¤ãª: {response}")
            
        print(f"\nğŸ“Š {setsuna.get_conversation_summary()}")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print("OPENAI_API_KEY ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
    
    print("\nã›ã¤ãªãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆå®Œäº†")

# === æ–°ã—ã„ä¸»ä½“æ€§å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆSetsunaChat ã‚¯ãƒ©ã‚¹å¤–ï¼‰ ===
# ã“ã‚Œã‚‰ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’SetsunaChat ã‚¯ãƒ©ã‚¹å†…ã«è¿½åŠ 

def add_proactivity_methods_to_setsuna_chat():
    """SetsunaChat ã‚¯ãƒ©ã‚¹ã«ä¸»ä½“æ€§å¼·åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã™ã‚‹é–¢æ•°"""
    
    def _check_proactive_opportunity(self, user_input: str, mode: str) -> dict:
        """ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–å¿œç­”ã®æ©Ÿä¼šã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            if not self.proactive_engine or mode == "fast_response":
                return None
            
            # ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
            conversation_context = {
                "last_user_input": user_input,
                "conversation_history": self.conversation_history[-5:],  # æœ€æ–°5ä»¶
                "music_mentioned": any(keyword in user_input.lower() 
                                     for keyword in ["æ¥½æ›²", "éŸ³æ¥½", "æ­Œ", "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ"]),
                "technical_discussion": any(keyword in user_input.lower() 
                                          for keyword in ["æŠ€è¡“", "åˆ†æ", "æ§‹æˆ", "åˆ¶ä½œ"]),
                "creative_context": any(keyword in user_input.lower() 
                                      for keyword in ["æ˜ åƒ", "ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«", "å‰µä½œ", "ã‚¢ã‚¤ãƒ‡ã‚¢"]),
                "collaborative_context": any(keyword in user_input.lower() 
                                           for keyword in ["ä¸€ç·’", "å…±åŒ", "é…ä¿¡", "å…±æœ‰"])
            }
            
            # ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–å¿œç­”åˆ¤å®š
            suggestion_decision = self.proactive_engine.should_suggest_proactive_response(conversation_context)
            
            if suggestion_decision.get("should_suggest", False):
                # å…·ä½“çš„ãªææ¡ˆã‚’ç”Ÿæˆ
                proactive_suggestion = self.proactive_engine.generate_proactive_suggestion(
                    conversation_context, suggestion_decision.get("suggested_type")
                )
                return proactive_suggestion
            
            return None
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _build_conversation_context(self, user_input: str, mode: str) -> dict:
        """ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        try:
            context = {
                "user_input": user_input,
                "mode": mode,
                "conversation_length": len(self.conversation_history),
                "recent_topics": [],
                "emotional_context": "neutral"
            }
            
            # æœ€è¿‘ã®è©±é¡Œã‚’æŠ½å‡º
            recent_messages = self.conversation_history[-6:]  # æœ€æ–°3å¾€å¾©
            for msg in recent_messages:
                if msg.get("role") == "user":
                    content = msg.get("content", "")
                    # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
                    if any(keyword in content.lower() for keyword in ["æ¥½æ›²", "éŸ³æ¥½", "æ­Œ"]):
                        context["recent_topics"].append("music")
                    if any(keyword in content.lower() for keyword in ["æ˜ åƒ", "åˆ¶ä½œ", "å‰µä½œ"]):
                        context["recent_topics"].append("creative")
                    if any(keyword in content.lower() for keyword in ["æŠ€è¡“", "åˆ†æ"]):
                        context["recent_topics"].append("technical")
            
            return context
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return {"user_input": user_input, "mode": mode}
    
    def _generate_opinion_context(self, user_input: str, context_info_dict: dict) -> dict:
        """æ„è¦‹ç”Ÿæˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        try:
            if not self.opinion_generator:
                return None
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‹ã‚‰ææ¡ˆã‚„è³ªå•ã‚’æ¤œå‡º
            has_proposal = any(keyword in user_input.lower() 
                             for keyword in ["ã—ã‚ˆã†", "ã‚„ã‚ã†", "ã©ã†", "ã„ã‹ãŒ", "ã—ã¦ã¿"])
            has_question = any(keyword in user_input.lower() 
                             for keyword in ["ï¼Ÿ", "?", "æ•™ãˆã¦", "ã©ã‚“ãª", "ãªã«"])
            
            if has_proposal or has_question:
                # æ„è¦‹ç”Ÿæˆã®æº–å‚™
                conversation_context = {
                    "last_user_input": user_input,
                    "music_mentioned": context_info_dict.get("is_video_query", False),
                    "technical_discussion": "æŠ€è¡“" in user_input.lower() or "åˆ†æ" in user_input.lower(),
                    "creative_context": any(keyword in user_input.lower() 
                                          for keyword in ["æ˜ åƒ", "å‰µä½œ", "åˆ¶ä½œ"]),
                    "has_proposal": has_proposal,
                    "has_question": has_question
                }
                
                # æ„è¦‹ã‚’ç”Ÿæˆ
                opinion_result = self.opinion_generator.generate_opinion(user_input, conversation_context)
                
                if opinion_result and opinion_result.get("opinion"):
                    return {
                        "generated_opinion": opinion_result["opinion"],
                        "opinion_type": opinion_result.get("opinion_type", "general"),
                        "confidence": opinion_result.get("confidence", 0.5),
                        "reasoning": opinion_result.get("reasoning", "")
                    }
            
            return None
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ æ„è¦‹ç”Ÿæˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _enhance_response_with_proactive_elements(self, response: str, proactive_suggestion: dict) -> str:
        """å¿œç­”ã«ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–è¦ç´ ã‚’è¿½åŠ """
        try:
            if not proactive_suggestion:
                return response
            
            suggestion_content = proactive_suggestion.get("suggestion", "")
            suggestion_type = proactive_suggestion.get("type", "")
            
            if suggestion_content:
                # ææ¡ˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦è¿½åŠ æ–¹æ³•ã‚’èª¿æ•´
                if suggestion_type == "creative_project_proposal":
                    enhanced_response = f"{response} {suggestion_content}"
                elif suggestion_type == "technical_exploration":
                    enhanced_response = f"{response} ã¨ã“ã‚ã§ã€{suggestion_content}"
                else:
                    enhanced_response = f"{response} {suggestion_content}"
                
                # é•·ã™ãã‚‹å ´åˆã¯å…ƒã®å¿œç­”ã‚’è¿”ã™
                if len(enhanced_response) > 200:
                    return response
                
                return enhanced_response
            
            return response
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–è¦ç´ è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
            return response
    
    def get_proactivity_stats(self) -> dict:
        """ä¸»ä½“æ€§ã‚·ã‚¹ãƒ†ãƒ ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        stats = {
            "proactive_suggestions": 0,
            "opinion_generations": 0,
            "consistency_checks": 0,
            "average_proactivity_score": 0.0
        }
        
        try:
            if self.proactive_engine and hasattr(self.proactive_engine, 'suggestion_history'):
                stats["proactive_suggestions"] = len(self.proactive_engine.suggestion_history)
            
            if self.new_consistency_checker and hasattr(self.new_consistency_checker, 'check_history'):
                stats["consistency_checks"] = len(self.new_consistency_checker.check_history)
                
                # å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                recent_checks = self.new_consistency_checker.check_history[-10:]
                if recent_checks:
                    scores = [check["result"]["overall_score"] for check in recent_checks]
                    stats["average_proactivity_score"] = sum(scores) / len(scores)
            
        except Exception as e:
            print(f"[ãƒãƒ£ãƒƒãƒˆ] âš ï¸ ä¸»ä½“æ€§çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return stats
    
    # ãƒ¡ã‚½ãƒƒãƒ‰ã‚’SetsunaChat ã‚¯ãƒ©ã‚¹ã«è¿½åŠ 
    SetsunaChat._check_proactive_opportunity = _check_proactive_opportunity
    SetsunaChat._build_conversation_context = _build_conversation_context
    SetsunaChat._generate_opinion_context = _generate_opinion_context
    SetsunaChat._enhance_response_with_proactive_elements = _enhance_response_with_proactive_elements
    SetsunaChat.get_proactivity_stats = get_proactivity_stats

# ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
add_proactivity_methods_to_setsuna_chat()