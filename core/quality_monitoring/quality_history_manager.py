#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
QualityHistoryManager - å“è³ªå±¥æ­´è¿½è·¡ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
ReportQualityValidatorã®æ¤œè¨¼çµæœã‚’æ°¸ç¶šåŒ–ã—ã€å“è³ªå‚¾å‘ã‚’åˆ†æãƒ»ç›£è¦–
"""

import json
import sqlite3
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
import statistics
import threading
from contextlib import contextmanager

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ç¢ºå®Ÿã«ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from core.knowledge_analysis.report_quality_validator import ValidationReport, ValidationSeverity, ValidationIssue
    VALIDATOR_AVAILABLE = True
except ImportError:
    VALIDATOR_AVAILABLE = False
    print("âš ï¸ ReportQualityValidatorãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

# ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
try:
    from logging_system import get_logger
    LOGGER_AVAILABLE = True
except ImportError:
    LOGGER_AVAILABLE = False

class QualityTrend(Enum):
    """å“è³ªå‚¾å‘"""
    IMPROVING = "improving"
    DECLINING = "declining"
    STABLE = "stable"
    VOLATILE = "volatile"

class AlertLevel(Enum):
    """ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«"""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    EMERGENCY = "emergency"

@dataclass
class QualitySnapshot:
    """å“è³ªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ"""
    timestamp: str
    overall_score: float
    total_issues: int
    critical_issues: int
    error_issues: int
    warning_issues: int
    info_issues: int
    validation_summary: str
    processing_time: float = 0.0
    search_count: int = 0
    cost: float = 0.0
    data_quality: float = 0.0

@dataclass
class QualityAlert:
    """å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆ"""
    alert_id: str
    timestamp: str
    level: AlertLevel
    message: str
    metrics: Dict[str, Any]
    threshold_violated: str
    suggested_action: str

@dataclass
class QualityTrendAnalysis:
    """å“è³ªå‚¾å‘åˆ†æ"""
    period_days: int
    trend: QualityTrend
    avg_score: float
    score_change: float
    volatility: float
    issue_trend: Dict[str, float]
    recommendations: List[str]

class QualityHistoryManager:
    """å“è³ªå±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, db_path: Optional[str] = None):
        """åˆæœŸåŒ–"""
        # ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        if LOGGER_AVAILABLE:
            self.logger = get_logger()
            self.logger.info("quality_monitoring", "init", "QualityHistoryManageråˆæœŸåŒ–é–‹å§‹")
        else:
            self.logger = None
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹è¨­å®š
        if db_path:
            self.db_path = Path(db_path)
        else:
            self.db_path = Path("D:/setsuna_bot/quality_monitoring/quality_history.db")
        
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒƒã‚¯
        self._lock = threading.Lock()
        
        # å“è³ªè¨­å®š
        self.quality_config = {
            "score_warning_threshold": 0.6,
            "score_critical_threshold": 0.4,
            "max_critical_issues": 3,
            "max_error_issues": 5,
            "trend_analysis_days": 7,
            "volatility_threshold": 0.3,
            "cost_warning_threshold": 1.0,  # $1ä»¥ä¸Šã§è­¦å‘Š
            "processing_time_warning": 60.0  # 60ç§’ä»¥ä¸Šã§è­¦å‘Š
        }
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self._initialize_database()
        
        print("[å“è³ªå±¥æ­´] âœ… QualityHistoryManageråˆæœŸåŒ–å®Œäº†")
    
    def _initialize_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        try:
            with self._get_db_connection() as conn:
                # å“è³ªå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS quality_history (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp TEXT NOT NULL,
                        overall_score REAL NOT NULL,
                        total_issues INTEGER NOT NULL,
                        critical_issues INTEGER NOT NULL,
                        error_issues INTEGER NOT NULL,
                        warning_issues INTEGER NOT NULL,
                        info_issues INTEGER NOT NULL,
                        validation_summary TEXT,
                        processing_time REAL DEFAULT 0.0,
                        search_count INTEGER DEFAULT 0,
                        cost REAL DEFAULT 0.0,
                        data_quality REAL DEFAULT 0.0,
                        raw_data TEXT,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS quality_alerts (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        alert_id TEXT UNIQUE NOT NULL,
                        timestamp TEXT NOT NULL,
                        level TEXT NOT NULL,
                        message TEXT NOT NULL,
                        metrics TEXT,
                        threshold_violated TEXT,
                        suggested_action TEXT,
                        resolved BOOLEAN DEFAULT FALSE,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS quality_metrics (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp TEXT NOT NULL,
                        metric_name TEXT NOT NULL,
                        metric_value REAL NOT NULL,
                        metric_type TEXT DEFAULT 'gauge',
                        tags TEXT,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                conn.execute("CREATE INDEX IF NOT EXISTS idx_quality_timestamp ON quality_history(timestamp)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_alerts_timestamp ON quality_alerts(timestamp)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_metrics_timestamp ON quality_metrics(timestamp)")
                
                conn.commit()
            
            if self.logger:
                self.logger.info("quality_monitoring", "init_db", "å“è³ªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")
            
        except Exception as e:
            print(f"âŒ å“è³ªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å¤±æ•—: {e}")
            if self.logger:
                self.logger.error("quality_monitoring", "init_db", f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    @contextmanager
    def _get_db_connection(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£"""
        conn = None
        try:
            conn = sqlite3.connect(str(self.db_path), timeout=30.0)
            conn.row_factory = sqlite3.Row
            yield conn
        finally:
            if conn:
                conn.close()
    
    def record_validation_result(self, validation_report: ValidationReport, 
                               processing_time: float = 0.0, search_count: int = 0,
                               cost: float = 0.0, data_quality: float = 0.0) -> str:
        """æ¤œè¨¼çµæœã‚’è¨˜éŒ²"""
        try:
            with self._lock:
                # å“è³ªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆ
                snapshot = QualitySnapshot(
                    timestamp=validation_report.validation_timestamp,
                    overall_score=validation_report.overall_score,
                    total_issues=validation_report.total_issues,
                    critical_issues=validation_report.issues_by_severity.get("critical", 0),
                    error_issues=validation_report.issues_by_severity.get("error", 0),
                    warning_issues=validation_report.issues_by_severity.get("warning", 0),
                    info_issues=validation_report.issues_by_severity.get("info", 0),
                    validation_summary=validation_report.validation_summary,
                    processing_time=processing_time,
                    search_count=search_count,
                    cost=cost,
                    data_quality=data_quality
                )
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                record_id = self._save_quality_snapshot(snapshot, validation_report)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                self._record_metrics(snapshot)
                
                # ã‚¢ãƒ©ãƒ¼ãƒˆåˆ¤å®š
                alerts = self._check_quality_alerts(snapshot)
                for alert in alerts:
                    self._save_alert(alert)
                
                if self.logger:
                    self.logger.info("quality_monitoring", "record", f"å“è³ªè¨˜éŒ²ä¿å­˜å®Œäº†: ID={record_id}")
                
                print(f"[å“è³ªå±¥æ­´] ğŸ“Š å“è³ªè¨˜éŒ²ä¿å­˜: ã‚¹ã‚³ã‚¢{snapshot.overall_score:.2f}, å•é¡Œ{snapshot.total_issues}ä»¶")
                
                return record_id
                
        except Exception as e:
            print(f"âŒ å“è³ªè¨˜éŒ²ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            if self.logger:
                self.logger.error("quality_monitoring", "record", f"å“è³ªè¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def _save_quality_snapshot(self, snapshot: QualitySnapshot, validation_report: ValidationReport) -> str:
        """å“è³ªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            with self._get_db_connection() as conn:
                cursor = conn.execute("""
                    INSERT INTO quality_history (
                        timestamp, overall_score, total_issues, critical_issues,
                        error_issues, warning_issues, info_issues, validation_summary,
                        processing_time, search_count, cost, data_quality, raw_data
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    snapshot.timestamp,
                    snapshot.overall_score,
                    snapshot.total_issues,
                    snapshot.critical_issues,
                    snapshot.error_issues,
                    snapshot.warning_issues,
                    snapshot.info_issues,
                    snapshot.validation_summary,
                    snapshot.processing_time,
                    snapshot.search_count,
                    snapshot.cost,
                    snapshot.data_quality,
                    json.dumps(asdict(validation_report), default=str, ensure_ascii=False)
                ))
                
                conn.commit()
                return str(cursor.lastrowid)
                
        except Exception as e:
            print(f"âš ï¸ å“è³ªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def _record_metrics(self, snapshot: QualitySnapshot):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""
        try:
            metrics = [
                ("overall_score", snapshot.overall_score, "gauge"),
                ("total_issues", snapshot.total_issues, "gauge"),
                ("critical_issues", snapshot.critical_issues, "gauge"),
                ("processing_time", snapshot.processing_time, "gauge"),
                ("search_count", snapshot.search_count, "counter"),
                ("cost", snapshot.cost, "counter"),
                ("data_quality", snapshot.data_quality, "gauge")
            ]
            
            with self._get_db_connection() as conn:
                for metric_name, value, metric_type in metrics:
                    conn.execute("""
                        INSERT INTO quality_metrics (timestamp, metric_name, metric_value, metric_type)
                        VALUES (?, ?, ?, ?)
                    """, (snapshot.timestamp, metric_name, value, metric_type))
                
                conn.commit()
                
        except Exception as e:
            print(f"âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _check_quality_alerts(self, snapshot: QualitySnapshot) -> List[QualityAlert]:
        """å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆåˆ¤å®š"""
        alerts = []
        timestamp = datetime.now().isoformat()
        
        try:
            # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆ
            if snapshot.overall_score <= self.quality_config["score_critical_threshold"]:
                alerts.append(QualityAlert(
                    alert_id=f"score_critical_{timestamp}",
                    timestamp=timestamp,
                    level=AlertLevel.CRITICAL,
                    message=f"å“è³ªã‚¹ã‚³ã‚¢ãŒè‡¨ç•Œå€¤ã‚’ä¸‹å›ã‚Šã¾ã—ãŸ: {snapshot.overall_score:.2f}",
                    metrics={"overall_score": snapshot.overall_score},
                    threshold_violated="score_critical_threshold",
                    suggested_action="ç·Šæ€¥å¯¾å¿œ: ã‚·ã‚¹ãƒ†ãƒ è¨­å®šç¢ºèªãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ªæ”¹å–„ãŒå¿…è¦"
                ))
            elif snapshot.overall_score <= self.quality_config["score_warning_threshold"]:
                alerts.append(QualityAlert(
                    alert_id=f"score_warning_{timestamp}",
                    timestamp=timestamp,
                    level=AlertLevel.WARNING,
                    message=f"å“è³ªã‚¹ã‚³ã‚¢ãŒè­¦å‘Šå€¤ã‚’ä¸‹å›ã‚Šã¾ã—ãŸ: {snapshot.overall_score:.2f}",
                    metrics={"overall_score": snapshot.overall_score},
                    threshold_violated="score_warning_threshold",
                    suggested_action="å“è³ªæ”¹å–„ã®æ¤œè¨ãƒ»ç›£è¦–å¼·åŒ–ã‚’æ¨å¥¨"
                ))
            
            # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å•é¡Œã‚¢ãƒ©ãƒ¼ãƒˆ
            if snapshot.critical_issues >= self.quality_config["max_critical_issues"]:
                alerts.append(QualityAlert(
                    alert_id=f"critical_issues_{timestamp}",
                    timestamp=timestamp,
                    level=AlertLevel.EMERGENCY,
                    message=f"ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å•é¡ŒãŒå¤šç™º: {snapshot.critical_issues}ä»¶",
                    metrics={"critical_issues": snapshot.critical_issues},
                    threshold_violated="max_critical_issues",
                    suggested_action="ç·Šæ€¥å¯¾å¿œ: ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ãƒ»åŸå› èª¿æŸ»ãŒå¿…è¦"
                ))
            
            # ã‚³ã‚¹ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆ
            if snapshot.cost >= self.quality_config["cost_warning_threshold"]:
                alerts.append(QualityAlert(
                    alert_id=f"cost_warning_{timestamp}",
                    timestamp=timestamp,
                    level=AlertLevel.WARNING,
                    message=f"ã‚³ã‚¹ãƒˆãŒè­¦å‘Šå€¤ã‚’è¶…é: ${snapshot.cost:.2f}",
                    metrics={"cost": snapshot.cost},
                    threshold_violated="cost_warning_threshold",
                    suggested_action="ã‚³ã‚¹ãƒˆæœ€é©åŒ–ãƒ»ä½¿ç”¨é‡åˆ¶é™ã®æ¤œè¨"
                ))
            
            # å‡¦ç†æ™‚é–“ã‚¢ãƒ©ãƒ¼ãƒˆ
            if snapshot.processing_time >= self.quality_config["processing_time_warning"]:
                alerts.append(QualityAlert(
                    alert_id=f"performance_warning_{timestamp}",
                    timestamp=timestamp,
                    level=AlertLevel.WARNING,
                    message=f"å‡¦ç†æ™‚é–“ãŒé…å»¶: {snapshot.processing_time:.1f}ç§’",
                    metrics={"processing_time": snapshot.processing_time},
                    threshold_violated="processing_time_warning",
                    suggested_action="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ»ãƒªã‚½ãƒ¼ã‚¹å¢—å¼·ã®æ¤œè¨"
                ))
            
        except Exception as e:
            print(f"âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆåˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        return alerts
    
    def _save_alert(self, alert: QualityAlert):
        """ã‚¢ãƒ©ãƒ¼ãƒˆä¿å­˜"""
        try:
            with self._get_db_connection() as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO quality_alerts (
                        alert_id, timestamp, level, message, metrics,
                        threshold_violated, suggested_action
                    ) VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    alert.alert_id,
                    alert.timestamp,
                    alert.level.value,
                    alert.message,
                    json.dumps(alert.metrics),
                    alert.threshold_violated,
                    alert.suggested_action
                ))
                
                conn.commit()
                
                # ã‚¢ãƒ©ãƒ¼ãƒˆè¡¨ç¤º
                level_emoji = {"info": "â„¹ï¸", "warning": "âš ï¸", "critical": "ğŸ”´", "emergency": "ğŸš¨"}
                emoji = level_emoji.get(alert.level.value, "ğŸ“Š")
                print(f"[å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆ] {emoji} {alert.level.value.upper()}: {alert.message}")
                
        except Exception as e:
            print(f"âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_quality_trend_analysis(self, days: int = 7) -> QualityTrendAnalysis:
        """å“è³ªå‚¾å‘åˆ†æ"""
        try:
            cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()
            
            with self._get_db_connection() as conn:
                cursor = conn.execute("""
                    SELECT overall_score, total_issues, critical_issues, error_issues, 
                           warning_issues, timestamp
                    FROM quality_history 
                    WHERE timestamp >= ?
                    ORDER BY timestamp ASC
                """, (cutoff_date,))
                
                records = cursor.fetchall()
            
            if not records:
                return QualityTrendAnalysis(
                    period_days=days,
                    trend=QualityTrend.STABLE,
                    avg_score=0.0,
                    score_change=0.0,
                    volatility=0.0,
                    issue_trend={},
                    recommendations=["ãƒ‡ãƒ¼ã‚¿ä¸è¶³: å“è³ªè¨˜éŒ²ã‚’è“„ç©ã—ã¦ãã ã•ã„"]
                )
            
            # ã‚¹ã‚³ã‚¢åˆ†æ
            scores = [float(record["overall_score"]) for record in records]
            avg_score = statistics.mean(scores)
            
            # å‚¾å‘è¨ˆç®—
            if len(scores) >= 2:
                score_change = scores[-1] - scores[0]
                volatility = statistics.stdev(scores) if len(scores) > 1 else 0.0
            else:
                score_change = 0.0
                volatility = 0.0
            
            # å‚¾å‘åˆ¤å®š
            trend = self._determine_trend(score_change, volatility)
            
            # å•é¡Œå‚¾å‘åˆ†æ
            issue_trend = self._analyze_issue_trends(records)
            
            # æ¨å¥¨äº‹é …ç”Ÿæˆ
            recommendations = self._generate_trend_recommendations(
                avg_score, score_change, volatility, issue_trend
            )
            
            return QualityTrendAnalysis(
                period_days=days,
                trend=trend,
                avg_score=avg_score,
                score_change=score_change,
                volatility=volatility,
                issue_trend=issue_trend,
                recommendations=recommendations
            )
            
        except Exception as e:
            print(f"âš ï¸ å“è³ªå‚¾å‘åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return QualityTrendAnalysis(
                period_days=days,
                trend=QualityTrend.STABLE,
                avg_score=0.0,
                score_change=0.0,
                volatility=0.0,
                issue_trend={},
                recommendations=[f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}"]
            )
    
    def _determine_trend(self, score_change: float, volatility: float) -> QualityTrend:
        """å‚¾å‘åˆ¤å®š"""
        volatility_threshold = self.quality_config["volatility_threshold"]
        
        if volatility > volatility_threshold:
            return QualityTrend.VOLATILE
        elif score_change > 0.1:
            return QualityTrend.IMPROVING
        elif score_change < -0.1:
            return QualityTrend.DECLINING
        else:
            return QualityTrend.STABLE
    
    def _analyze_issue_trends(self, records: List) -> Dict[str, float]:
        """å•é¡Œå‚¾å‘åˆ†æ"""
        issue_types = ["critical_issues", "error_issues", "warning_issues"]
        trends = {}
        
        for issue_type in issue_types:
            values = [int(record[issue_type]) for record in records]
            if len(values) >= 2:
                change = values[-1] - values[0]
                avg = statistics.mean(values)
                trends[issue_type] = change / (avg + 1)  # æ­£è¦åŒ–
            else:
                trends[issue_type] = 0.0
        
        return trends
    
    def _generate_trend_recommendations(self, avg_score: float, score_change: float, 
                                      volatility: float, issue_trend: Dict[str, float]) -> List[str]:
        """å‚¾å‘æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹æ¨å¥¨
        if avg_score < 0.5:
            recommendations.append("ä½å“è³ªè­¦å‘Š: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®è¦‹ç›´ã—ãŒå¿…è¦")
        elif avg_score < 0.7:
            recommendations.append("å“è³ªæ”¹å–„: è¨­å®šèª¿æ•´ãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šã‚’æ¤œè¨")
        
        # å‚¾å‘ãƒ™ãƒ¼ã‚¹æ¨å¥¨
        if score_change < -0.2:
            recommendations.append("å“è³ªåŠ£åŒ–å‚¾å‘: åŸå› åˆ†æãƒ»ç·Šæ€¥å¯¾å¿œãŒå¿…è¦")
        elif score_change > 0.2:
            recommendations.append("å“è³ªæ”¹å–„å‚¾å‘: ç¾åœ¨ã®å–ã‚Šçµ„ã¿ã‚’ç¶™ç¶š")
        
        # å¤‰å‹•æ€§ãƒ™ãƒ¼ã‚¹æ¨å¥¨
        if volatility > 0.3:
            recommendations.append("å“è³ªä¸å®‰å®š: è¨­å®šã®æ¨™æº–åŒ–ãƒ»å®‰å®šåŒ–ãŒå¿…è¦")
        
        # å•é¡Œå‚¾å‘ãƒ™ãƒ¼ã‚¹æ¨å¥¨
        if issue_trend.get("critical_issues", 0) > 0.5:
            recommendations.append("ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å•é¡Œå¢—åŠ : ç·Šæ€¥å¯¾å¿œãƒ»æ ¹æœ¬åŸå› èª¿æŸ»")
        
        if not recommendations:
            recommendations.append("å“è³ªçŠ¶æ³ã¯è‰¯å¥½ã§ã™")
        
        return recommendations
    
    def get_recent_alerts(self, hours: int = 24, level: Optional[AlertLevel] = None) -> List[QualityAlert]:
        """æœ€è¿‘ã®ã‚¢ãƒ©ãƒ¼ãƒˆå–å¾—"""
        try:
            cutoff_time = (datetime.now() - timedelta(hours=hours)).isoformat()
            
            query = """
                SELECT alert_id, timestamp, level, message, metrics,
                       threshold_violated, suggested_action
                FROM quality_alerts 
                WHERE timestamp >= ? AND resolved = FALSE
            """
            params = [cutoff_time]
            
            if level:
                query += " AND level = ?"
                params.append(level.value)
            
            query += " ORDER BY timestamp DESC"
            
            with self._get_db_connection() as conn:
                cursor = conn.execute(query, params)
                records = cursor.fetchall()
            
            alerts = []
            for record in records:
                alerts.append(QualityAlert(
                    alert_id=record["alert_id"],
                    timestamp=record["timestamp"],
                    level=AlertLevel(record["level"]),
                    message=record["message"],
                    metrics=json.loads(record["metrics"]) if record["metrics"] else {},
                    threshold_violated=record["threshold_violated"],
                    suggested_action=record["suggested_action"]
                ))
            
            return alerts
            
        except Exception as e:
            print(f"âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def get_quality_statistics(self, days: int = 30) -> Dict[str, Any]:
        """å“è³ªçµ±è¨ˆæƒ…å ±å–å¾—"""
        try:
            cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()
            
            with self._get_db_connection() as conn:
                # åŸºæœ¬çµ±è¨ˆ
                cursor = conn.execute("""
                    SELECT 
                        COUNT(*) as total_records,
                        AVG(overall_score) as avg_score,
                        MIN(overall_score) as min_score,
                        MAX(overall_score) as max_score,
                        SUM(total_issues) as total_issues,
                        SUM(critical_issues) as total_critical,
                        SUM(cost) as total_cost,
                        AVG(processing_time) as avg_processing_time
                    FROM quality_history 
                    WHERE timestamp >= ?
                """, (cutoff_date,))
                
                stats = cursor.fetchone()
                
                # ã‚¢ãƒ©ãƒ¼ãƒˆçµ±è¨ˆ
                cursor = conn.execute("""
                    SELECT level, COUNT(*) as count
                    FROM quality_alerts 
                    WHERE timestamp >= ?
                    GROUP BY level
                """, (cutoff_date,))
                
                alert_stats = dict(cursor.fetchall())
            
            return {
                "period_days": days,
                "total_records": stats["total_records"] or 0,
                "average_score": round(stats["avg_score"] or 0, 3),
                "score_range": [round(stats["min_score"] or 0, 3), round(stats["max_score"] or 0, 3)],
                "total_issues": stats["total_issues"] or 0,
                "critical_issues": stats["total_critical"] or 0,
                "total_cost": round(stats["total_cost"] or 0, 4),
                "avg_processing_time": round(stats["avg_processing_time"] or 0, 2),
                "alert_counts": alert_stats,
                "db_path": str(self.db_path),
                "db_size_mb": round(self.db_path.stat().st_size / (1024*1024), 2) if self.db_path.exists() else 0
            }
            
        except Exception as e:
            print(f"âš ï¸ çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def cleanup_old_records(self, keep_days: int = 90):
        """å¤ã„è¨˜éŒ²ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            cutoff_date = (datetime.now() - timedelta(days=keep_days)).isoformat()
            
            with self._get_db_connection() as conn:
                # å¤ã„å±¥æ­´å‰Šé™¤
                cursor = conn.execute("DELETE FROM quality_history WHERE timestamp < ?", (cutoff_date,))
                history_deleted = cursor.rowcount
                
                # å¤ã„ã‚¢ãƒ©ãƒ¼ãƒˆå‰Šé™¤ï¼ˆè§£æ±ºæ¸ˆã¿ï¼‰
                cursor = conn.execute("""
                    DELETE FROM quality_alerts 
                    WHERE timestamp < ? AND resolved = TRUE
                """, (cutoff_date,))
                alerts_deleted = cursor.rowcount
                
                # å¤ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‰Šé™¤
                cursor = conn.execute("DELETE FROM quality_metrics WHERE timestamp < ?", (cutoff_date,))
                metrics_deleted = cursor.rowcount
                
                conn.commit()
            
            print(f"[å“è³ªå±¥æ­´] ğŸ—‘ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: å±¥æ­´{history_deleted}ä»¶, ã‚¢ãƒ©ãƒ¼ãƒˆ{alerts_deleted}ä»¶, ãƒ¡ãƒˆãƒªã‚¯ã‚¹{metrics_deleted}ä»¶å‰Šé™¤")
            
            if self.logger:
                self.logger.info("quality_monitoring", "cleanup", 
                               f"å¤ã„è¨˜éŒ²å‰Šé™¤å®Œäº†: {history_deleted + alerts_deleted + metrics_deleted}ä»¶")
            
        except Exception as e:
            print(f"âš ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸ§ª QualityHistoryManager ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 60)
    
    manager = QualityHistoryManager()
    
    # çµ±è¨ˆæƒ…å ±ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“Š å“è³ªçµ±è¨ˆæƒ…å ±:")
    stats = manager.get_quality_statistics()
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # å‚¾å‘åˆ†æãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ“ˆ å“è³ªå‚¾å‘åˆ†æ:")
    trend = manager.get_quality_trend_analysis()
    print(f"   å‚¾å‘: {trend.trend.value}")
    print(f"   å¹³å‡ã‚¹ã‚³ã‚¢: {trend.avg_score:.3f}")
    print(f"   ã‚¹ã‚³ã‚¢å¤‰åŒ–: {trend.score_change:+.3f}")
    print(f"   æ¨å¥¨äº‹é …: {trend.recommendations}")
    
    # ã‚¢ãƒ©ãƒ¼ãƒˆç¢ºèªãƒ†ã‚¹ãƒˆ
    print(f"\nğŸš¨ æœ€è¿‘ã®ã‚¢ãƒ©ãƒ¼ãƒˆ:")
    alerts = manager.get_recent_alerts()
    if alerts:
        for alert in alerts[:3]:
            print(f"   {alert.level.value}: {alert.message}")
    else:
        print("   ã‚¢ãƒ©ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“")
    
    print(f"\nâœ… QualityHistoryManager ãƒ†ã‚¹ãƒˆå®Œäº†")