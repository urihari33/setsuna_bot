#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ActivityLearningEngine - Phase 2A-1
æ´»å‹•èª¿æŸ»ãƒ»å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ç®¡ç†ãƒ»å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³
"""

import json
import os
import time
import threading
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Callable
import uuid
import openai
from dataclasses import dataclass, asdict
import requests
from urllib.parse import quote
import hashlib
from .preprocessing_engine import PreProcessingEngine
from .config_manager import get_config_manager
from .debug_logger import get_debug_logger, debug_function
from .mock_search_service import SearchEngineManager

# Windowsç’°å¢ƒã®ãƒ‘ã‚¹è¨­å®šï¼ˆCLAUDE.mdã®æŒ‡ç¤ºã«å¾“ã„Windowsãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼‰
# WSL2ç’°å¢ƒã§ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ã¯Windowså´ã§è¡Œã†
DATA_DIR = Path("D:/setsuna_bot/data/activity_knowledge")

@dataclass
class LearningSession:
    """å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    session_id: str
    theme: str
    learning_type: str  # "æ¦‚è¦", "æ·±æ˜ã‚Š", "å®Ÿç”¨"
    depth_level: int    # 1-5
    time_limit: int     # ç§’
    budget_limit: float # ãƒ‰ãƒ«
    status: str         # "ready", "running", "paused", "completed", "error"
    parent_session: Optional[str] = None
    tags: List[str] = None
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æƒ…å ±
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    current_phase: str = "ready"  # "collection", "analysis", "integration"
    collected_items: int = 0
    processed_items: int = 0
    important_findings: List[Dict] = None
    current_cost: float = 0.0
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = []
        if self.important_findings is None:
            self.important_findings = []

class ActivityLearningEngine:
    """æ´»å‹•å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.sessions_dir = DATA_DIR / "sessions"
        self.sessions_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
        self.current_session: Optional[LearningSession] = None
        self.session_history: Dict[str, LearningSession] = {}
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°åˆæœŸåŒ–
        self.debug_logger = get_debug_logger(component="LEARNING_ENGINE")
        self.debug_logger.info("ActivityLearningEngineåˆæœŸåŒ–é–‹å§‹")
        
        # APIè¨­å®š
        self.openai_client = None
        self._initialize_apis()
        
        # æƒ…å ±åé›†è¨­å®š
        self.collection_config = {
            "web_search_enabled": True,
            "news_search_enabled": True,
            "social_search_enabled": False,
            "max_sources_per_query": 10,
            "quality_threshold": 0.6,
            "relevance_threshold": 0.7
        }
        
        # GPT-4-turboè¨­å®š
        self.gpt_config = {
            "model": "gpt-4-turbo-preview",
            "temperature": 0.3,
            "max_tokens": 4000,
            "max_input_tokens": 50000
        }
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        self.progress_callbacks: List[Callable] = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ï¼ˆãƒ‡ãƒ¼ã‚¿ç«¶åˆé˜²æ­¢ï¼‰
        self.file_lock = threading.Lock()
        
        # å‰å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
        self.preprocessing_engine = PreProcessingEngine()
        
        # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ç®¡ç†
        self.search_manager = SearchEngineManager()
        
        # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³çŠ¶æ…‹ç¢ºèª
        search_status = self.search_manager.get_status()
        self.debug_logger.info("Googleæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†", {
            "ready": search_status["ready"],
            "google_service_available": search_status["google_service_available"],
            "config_valid": search_status["config_valid"],
            "quota_remaining": search_status["quota_remaining"]
        })
        
        # æ®µéšçš„åˆ†æè¨­å®š
        self.staged_analysis_config = {
            "enable_preprocessing": True,
            "preprocessing_thresholds": {
                "relevance_min": 0.4,
                "quality_min": 0.5,
                "combined_min": 0.6
            },
            "max_detailed_analysis": 15,  # è©³ç´°åˆ†æã™ã‚‹æœ€å¤§ä»¶æ•°
            "gpt35_batch_size": 10,       # GPT-3.5ãƒãƒƒãƒã‚µã‚¤ã‚º
            "gpt4_batch_size": 5          # GPT-4-turboãƒãƒƒãƒã‚µã‚¤ã‚º
        }
        
        self.debug_logger.info("ActivityLearningEngineåˆæœŸåŒ–å®Œäº†")
        print("[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âœ… ActivityLearningEngineåˆæœŸåŒ–å®Œäº†")
    
    def _initialize_apis(self):
        """APIåˆæœŸåŒ–"""
        self.debug_logger.info("APIåˆæœŸåŒ–é–‹å§‹")
        
        try:
            # ConfigManagerçµŒç”±ã§OpenAIè¨­å®šå–å¾—
            config = get_config_manager()
            openai_key = config.get_openai_key()
            
            self.debug_logger.debug("OpenAI APIã‚­ãƒ¼å–å¾—çŠ¶æ³", {
                "key_available": bool(openai_key),
                "key_length": len(openai_key) if openai_key else 0
            })
            
            if openai_key:
                openai.api_key = openai_key
                self.openai_client = openai
                
                # æ¥ç¶šãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                try:
                    self.debug_logger.debug("OpenAI APIæ¥ç¶šãƒ†ã‚¹ãƒˆé–‹å§‹")
                    test_response = openai.models.list()
                    
                    if test_response:
                        self.debug_logger.info("OpenAI APIæ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ", {
                            "available_models": len(test_response.data)
                        })
                        print("[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âœ… OpenAI APIè¨­å®šãƒ»æ¥ç¶šç¢ºèªå®Œäº†")
                        return True
                except Exception as api_error:
                    self.debug_logger.error("OpenAI APIæ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—", {
                        "error_type": type(api_error).__name__,
                        "error_message": str(api_error)
                    }, api_error)
                    print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ OpenAI APIæ¥ç¶šå¤±æ•—: {api_error}")
                    self.openai_client = None
                    return False
            else:
                self.debug_logger.warning("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                print("[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                print("  .envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
                self.openai_client = None
                return False
                
        except Exception as e:
            self.debug_logger.error("APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼", {
                "error_type": type(e).__name__,
                "error_message": str(e)
            }, e)
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.openai_client = None
            return False
    
    def add_progress_callback(self, callback: Callable):
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ """
        self.progress_callbacks.append(callback)
    
    def configure_staged_analysis(self, **kwargs):
        """
        æ®µéšçš„åˆ†æè¨­å®š
        
        Args:
            enable_preprocessing: å‰å‡¦ç†æœ‰åŠ¹/ç„¡åŠ¹
            relevance_min: é–¢é€£æ€§æœ€å°é–¾å€¤
            quality_min: å“è³ªæœ€å°é–¾å€¤
            combined_min: ç·åˆã‚¹ã‚³ã‚¢æœ€å°é–¾å€¤
            max_detailed_analysis: è©³ç´°åˆ†ææœ€å¤§ä»¶æ•°
            gpt35_batch_size: GPT-3.5ãƒãƒƒãƒã‚µã‚¤ã‚º
            gpt4_batch_size: GPT-4ãƒãƒƒãƒã‚µã‚¤ã‚º
        """
        if "enable_preprocessing" in kwargs:
            self.staged_analysis_config["enable_preprocessing"] = kwargs["enable_preprocessing"]
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âš™ï¸ å‰å‡¦ç†: {'æœ‰åŠ¹' if kwargs['enable_preprocessing'] else 'ç„¡åŠ¹'}")
        
        # å‰å‡¦ç†é–¾å€¤æ›´æ–°
        preprocessing_thresholds = self.staged_analysis_config["preprocessing_thresholds"]
        for key in ["relevance_min", "quality_min", "combined_min"]:
            if key in kwargs:
                preprocessing_thresholds[key] = kwargs[key]
                print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âš™ï¸ é–¾å€¤æ›´æ–°: {key} = {kwargs[key]}")
        
        # ãã®ä»–è¨­å®šæ›´æ–°
        for key in ["max_detailed_analysis", "gpt35_batch_size", "gpt4_batch_size"]:
            if key in kwargs:
                self.staged_analysis_config[key] = kwargs[key]
                print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âš™ï¸ è¨­å®šæ›´æ–°: {key} = {kwargs[key]}")
    
    def get_staged_analysis_config(self) -> Dict[str, Any]:
        """æ®µéšçš„åˆ†æè¨­å®šå–å¾—"""
        return self.staged_analysis_config.copy()
    
    def enable_preprocessing(self, enable: bool = True):
        """å‰å‡¦ç†æœ‰åŠ¹/ç„¡åŠ¹åˆ‡ã‚Šæ›¿ãˆ"""
        self.staged_analysis_config["enable_preprocessing"] = enable
        print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âš™ï¸ å‰å‡¦ç†: {'æœ‰åŠ¹' if enable else 'ç„¡åŠ¹'}")
    
    def set_preprocessing_thresholds(self, relevance_min: float = None, quality_min: float = None, combined_min: float = None):
        """å‰å‡¦ç†é–¾å€¤è¨­å®š"""
        thresholds = self.staged_analysis_config["preprocessing_thresholds"]
        
        if relevance_min is not None:
            thresholds["relevance_min"] = relevance_min
        if quality_min is not None:
            thresholds["quality_min"] = quality_min
        if combined_min is not None:
            thresholds["combined_min"] = combined_min
        
        print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âš™ï¸ å‰å‡¦ç†é–¾å€¤æ›´æ–°: {thresholds}")
    
    def get_preprocessing_statistics(self) -> Dict[str, Any]:
        """å‰å‡¦ç†çµ±è¨ˆæƒ…å ±å–å¾—"""
        return self.preprocessing_engine.get_statistics()
    
    def _notify_progress(self, phase: str, progress: float, message: str):
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹é€šçŸ¥"""
        for callback in self.progress_callbacks:
            try:
                callback(phase, progress, message)
            except Exception as e:
                print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âš ï¸ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
    
    def create_session(self, 
                      theme: str,
                      learning_type: str = "æ¦‚è¦",
                      depth_level: int = 3,
                      time_limit: int = 1800,  # 30åˆ†
                      budget_limit: float = 5.0,
                      parent_session: Optional[str] = None,
                      tags: List[str] = None) -> str:
        """
        æ–°ã—ã„å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        
        Args:
            theme: å­¦ç¿’ãƒ†ãƒ¼ãƒ
            learning_type: å­¦ç¿’ã‚¿ã‚¤ãƒ— ("æ¦‚è¦", "æ·±æ˜ã‚Š", "å®Ÿç”¨")
            depth_level: å­¦ç¿’æ·±åº¦ (1-5)
            time_limit: æ™‚é–“åˆ¶é™ï¼ˆç§’ï¼‰
            budget_limit: äºˆç®—åˆ¶é™ï¼ˆãƒ‰ãƒ«ï¼‰
            parent_session: è¦ªã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            tags: ã‚¿ã‚°ãƒªã‚¹ãƒˆ
            
        Returns:
            ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
        """
        try:
            session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            
            session = LearningSession(
                session_id=session_id,
                theme=theme,
                learning_type=learning_type,
                depth_level=depth_level,
                time_limit=time_limit,
                budget_limit=budget_limit,
                status="ready",
                parent_session=parent_session,
                tags=tags or []
            )
            
            self.session_history[session_id] = session
            self._save_session(session)
            
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] ğŸ“ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ: {session_id}")
            print(f"  ãƒ†ãƒ¼ãƒ: {theme}")
            print(f"  ã‚¿ã‚¤ãƒ—: {learning_type} (æ·±åº¦ãƒ¬ãƒ™ãƒ«{depth_level})")
            print(f"  åˆ¶é™: {time_limit}ç§’, ${budget_limit}")
            
            return session_id
            
        except Exception as e:
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—: {e}")
            raise
    
    def start_session(self, session_id: str) -> bool:
        """
        å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹
        
        Args:
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            
        Returns:
            é–‹å§‹æˆåŠŸãƒ•ãƒ©ã‚°
        """
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³å°‚ç”¨ãƒ­ã‚°ã‚¬ãƒ¼åˆæœŸåŒ–
        session_logger = get_debug_logger(session_id, "SESSION")
        
        try:
            session_logger.info("ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹å‡¦ç†é–‹å§‹", {
                "session_id": session_id,
                "available_sessions": list(self.session_history.keys())
            })
            
            if session_id not in self.session_history:
                session_logger.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³æœªç™ºè¦‹", {
                    "session_id": session_id,
                    "available_sessions": list(self.session_history.keys())
                })
                print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³æœªç™ºè¦‹: {session_id}")
                return False
            
            session = self.session_history[session_id]
            session_logger.debug("ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ç¢ºèª", {
                "session_status": session.status,
                "session_theme": session.theme,
                "learning_type": session.learning_type,
                "depth_level": session.depth_level,
                "time_limit": session.time_limit,
                "budget_limit": session.budget_limit
            })
            
            if session.status != "ready":
                session_logger.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ä¸æ­£", {
                    "current_status": session.status,
                    "expected_status": "ready"
                })
                print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ä¸æ­£: {session.status}")
                return False
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹
            session.status = "running"
            session.start_time = datetime.now()
            session.current_phase = "collection"
            self.current_session = session
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³å°‚ç”¨ãƒ­ã‚°ã‚¬ãƒ¼ã‚’ãƒ¡ã‚¤ãƒ³ãƒ­ã‚°ã‚¬ãƒ¼ã«è¨­å®š
            self.debug_logger = session_logger
            
            self._save_session(session)
            self._notify_progress("collection", 0.0, "å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹")
            
            session_logger.info("ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æˆåŠŸ", {
                "session_id": session_id,
                "start_time": session.start_time.isoformat(),
                "initial_phase": session.current_phase
            })
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] ğŸš€ ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹: {session_id}")
            
            # ã‚¹ãƒ¬ãƒƒãƒ‰ã§å­¦ç¿’å®Ÿè¡Œ
            learning_thread = threading.Thread(
                target=self._execute_learning_session,
                daemon=True,
                name=f"learning_session_{session_id}"
            )
            learning_thread.start()
            
            session_logger.debug("å­¦ç¿’ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹", {
                "thread_name": learning_thread.name,
                "thread_id": learning_thread.ident
            })
            
            return True
            
        except Exception as e:
            session_logger.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹å¤±æ•—", {
                "session_id": session_id,
                "error_type": type(e).__name__
            }, e)
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹å¤±æ•—: {e}")
            return False
    
    def _execute_learning_session(self):
        """å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Ÿè¡Œï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰"""
        session = self.current_session
        if not session:
            return
        
        try:
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] ğŸ“š å­¦ç¿’å®Ÿè¡Œé–‹å§‹: {session.theme}")
            
            # Phase 1: æƒ…å ±åé›†
            self._phase_information_collection(session)
            
            # Phase 2: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ
            self._phase_content_analysis(session)
            
            # Phase 3: çŸ¥è­˜çµ±åˆ
            self._phase_knowledge_integration(session)
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†
            session.status = "completed"
            session.end_time = datetime.now()
            session.current_phase = "completed"
            
            self._save_session(session)
            self._notify_progress("completed", 1.0, "å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†")
            
            duration = (session.end_time - session.start_time).total_seconds()
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†: {session.session_id}")
            print(f"  æ‰€è¦æ™‚é–“: {duration:.1f}ç§’")
            print(f"  ç·ã‚³ã‚¹ãƒˆ: ${session.current_cost:.2f}")
            print(f"  åé›†è¨˜äº‹: {session.collected_items}ä»¶")
            print(f"  é‡è¦ç™ºè¦‹: {len(session.important_findings)}ä»¶")
            
        except Exception as e:
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ å­¦ç¿’å®Ÿè¡Œå¤±æ•—: {e}")
            session.status = "error"
            self._save_session(session)
            self._notify_progress("error", 0.0, f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _phase_information_collection(self, session: LearningSession):
        """Phase 1: æƒ…å ±åé›†"""
        phase_start_time = time.time()
        
        self.debug_logger.log_session_phase(
            "information_collection", "started", 0.1, 
            {
                "session_id": session.session_id,
                "theme": session.theme,
                "depth_level": session.depth_level
            }
        )
        
        print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] ğŸ” Phase 1: æƒ…å ±åé›†é–‹å§‹")
        session.current_phase = "collection"
        self._notify_progress("collection", 0.1, "æƒ…å ±åé›†é–‹å§‹")
        
        # æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ
        search_queries = self._generate_search_queries(session.theme, session.depth_level)
        
        collected_sources = []
        search_errors = []  # æ¤œç´¢ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
        total_queries = len(search_queries)
        
        self.debug_logger.info("æƒ…å ±åé›†ãƒ•ã‚§ãƒ¼ã‚ºè©³ç´°", {
            "total_queries": total_queries,
            "search_queries": search_queries,
            "collection_config": self.collection_config
        })
        
        for i, query in enumerate(search_queries):
            if self._should_stop_session(session):
                self.debug_logger.warning("ã‚»ãƒƒã‚·ãƒ§ãƒ³åœæ­¢æ¡ä»¶ã«ã‚ˆã‚Šä¸­æ–­", {
                    "completed_queries": i,
                    "total_queries": total_queries,
                    "collected_sources": len(collected_sources)
                })
                break
                
            self.debug_logger.info(f"æ¤œç´¢å®Ÿè¡Œ ({i+1}/{total_queries})", {
                "query": query,
                "query_index": i,
                "current_sources_count": len(collected_sources)
            })
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] ğŸ” æ¤œç´¢å®Ÿè¡Œ ({i+1}/{total_queries}): {query}")
            
            # Webæ¤œç´¢å®Ÿè¡Œ
            query_start_time = time.time()
            search_result = self._perform_web_search_detailed(query)
            query_execution_time = time.time() - query_start_time
            
            # æ¤œç´¢çµæœå‡¦ç†
            if search_result["success"]:
                sources = search_result["sources"]
                collected_sources.extend(sources)
                print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âœ… æ¤œç´¢æˆåŠŸ: {len(sources)}ä»¶å–å¾—")
            else:
                # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¨˜éŒ²
                error_info = {
                    "query": query,
                    "error_message": search_result.get("error_message", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"),
                    "error_type": search_result.get("error_type", "unknown"),
                    "quota_exceeded": search_result.get("quota_exceeded", False),
                    "timestamp": datetime.now().isoformat(),
                    "execution_time": query_execution_time
                }
                search_errors.append(error_info)
                print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ æ¤œç´¢å¤±æ•—: {error_info['error_message']}")
            
            self.debug_logger.info(f"æ¤œç´¢çµæœ ({i+1}/{total_queries})", {
                "query": query,
                "success": search_result["success"],
                "results_count": len(search_result.get("sources", [])),
                "execution_time": query_execution_time,
                "total_collected": len(collected_sources),
                "error_message": search_result.get("error_message") if not search_result["success"] else None
            })
            
            session.collected_items = len(collected_sources)
            progress = 0.1 + (0.3 * (i + 1) / total_queries)
            self._notify_progress("collection", progress, f"æ¤œç´¢å®Œäº†: {query}")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
            time.sleep(1)
        
        # Phase 1.5: å‰å‡¦ç†ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆGPT-3.5ï¼‰
        if self.staged_analysis_config["enable_preprocessing"] and collected_sources:
            preprocessing_start_time = time.time()
            
            self.debug_logger.log_session_phase(
                "preprocessing", "started", 0.35,
                {
                    "collected_sources_count": len(collected_sources),
                    "preprocessing_thresholds": self.staged_analysis_config["preprocessing_thresholds"],
                    "max_detailed_analysis": self.staged_analysis_config["max_detailed_analysis"]
                }
            )
            
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] ğŸ”„ Phase 1.5: å‰å‡¦ç†ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹")
            self._notify_progress("preprocessing", 0.35, "å‰å‡¦ç†é–‹å§‹")
            
            # å‰å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã§é–¾å€¤è¨­å®š
            self.preprocessing_engine.set_thresholds(**self.staged_analysis_config["preprocessing_thresholds"])
            
            # å‰å‡¦ç†å®Ÿè¡Œ
            preprocessing_results = self.preprocessing_engine.preprocess_content_batch(
                sources=collected_sources,
                theme=session.theme,
                target_categories=["æŠ€è¡“", "å¸‚å ´", "ãƒˆãƒ¬ãƒ³ãƒ‰", "å®Ÿç”¨"]
            )
            
            preprocessing_execution_time = time.time() - preprocessing_start_time
            
            self.debug_logger.info("å‰å‡¦ç†çµæœ", {
                "preprocessing_results_count": len(preprocessing_results),
                "execution_time": preprocessing_execution_time
            })
            
            # é€šéã—ãŸã‚½ãƒ¼ã‚¹ã®ã¿æŠ½å‡º
            passed_sources = []
            for result in preprocessing_results:
                if result.should_proceed:
                    # å…ƒã®ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«å‰å‡¦ç†çµæœã‚’ä»˜åŠ 
                    for source in collected_sources:
                        if source.get('source_id') == result.source_id:
                            source['preprocessing_result'] = {
                                'relevance_score': result.relevance_score,
                                'quality_score': result.quality_score,
                                'importance_score': result.importance_score,
                                'category': result.category,
                                'key_topics': result.key_topics,
                                'confidence': result.confidence,
                                'reason': result.reason
                            }
                            passed_sources.append(source)
                            break
            
            # é«˜å“è³ªãªã‚½ãƒ¼ã‚¹ã‚’ä¸Šä½ã«é¸æŠ
            if len(passed_sources) > self.staged_analysis_config["max_detailed_analysis"]:
                top_quality_results = self.preprocessing_engine.get_top_quality_sources(
                    preprocessing_results, 
                    self.staged_analysis_config["max_detailed_analysis"]
                )
                top_source_ids = [r.source_id for r in top_quality_results]
                passed_sources = [s for s in passed_sources if s.get('source_id') in top_source_ids]
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚µãƒãƒªãƒ¼
            filtering_summary = self.preprocessing_engine.get_filtering_summary(preprocessing_results)
            
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âœ… å‰å‡¦ç†å®Œäº†:")
            print(f"  åé›†: {len(collected_sources)}ä»¶")
            print(f"  é€šé: {len(passed_sources)}ä»¶ ({filtering_summary['pass_rate']:.1f}%)")
            print(f"  è©³ç´°åˆ†æå¯¾è±¡: {len(passed_sources)}ä»¶")
            
            # å‰å‡¦ç†ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã‚’ä½¿ç”¨
            final_sources = passed_sources
        else:
            # å‰å‡¦ç†ç„¡åŠ¹æ™‚ã¯å…¨ã‚½ãƒ¼ã‚¹ã‚’ä½¿ç”¨
            final_sources = collected_sources
        
        # åé›†çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚‚å«ã‚€ï¼‰
        phase_execution_time = time.time() - phase_start_time
        
        # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—
        search_engine_status = self.search_manager.get_status()
        
        session_data = {
            "collection_results": {
                "information_sources": final_sources,
                "raw_content_count": len(collected_sources),
                "filtered_content_count": len(final_sources),
                "search_queries": search_queries,
                "preprocessing_enabled": self.staged_analysis_config["enable_preprocessing"],
                "execution_time": phase_execution_time,
                "queries_executed": len(search_queries),
                "search_engine_status": search_engine_status,
                "collection_timestamp": datetime.now().isoformat(),
                "collection_success": len(final_sources) > 0,
                "search_errors": search_errors  # æ¤œç´¢ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            }
        }
        
        if self.staged_analysis_config["enable_preprocessing"] and collected_sources:
            session_data["preprocessing_summary"] = filtering_summary
        
        self._save_session_data(session, session_data)
        
        self.debug_logger.log_session_phase(
            "information_collection", "completed", 0.4,
            {
                "final_sources_count": len(final_sources),
                "raw_sources_count": len(collected_sources),
                "execution_time": phase_execution_time,
                "preprocessing_enabled": self.staged_analysis_config["enable_preprocessing"]
            }
        )
        
        print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âœ… æƒ…å ±åé›†å®Œäº†: {len(final_sources)}ä»¶ï¼ˆè©³ç´°åˆ†æå¯¾è±¡ï¼‰")
    
    def _phase_content_analysis(self, session: LearningSession):
        """Phase 2: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ"""
        print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] ğŸ§  Phase 2: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æé–‹å§‹")
        session.current_phase = "analysis"
        self._notify_progress("analysis", 0.4, "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æé–‹å§‹")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        session_file = self.sessions_dir / f"{session.session_id}.json"
        with open(session_file, 'r', encoding='utf-8') as f:
            session_data = json.load(f)
        
        sources = session_data.get("collection_results", {}).get("information_sources", [])
        
        # ãƒãƒƒãƒå‡¦ç†ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ
        analysis_results = []
        batch_size = 5  # GPT-4-turboåŠ¹ç‡åŒ–ã®ãŸã‚
        
        for i in range(0, len(sources), batch_size):
            if self._should_stop_session(session):
                break
                
            batch = sources[i:i+batch_size]
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] ğŸ§  åˆ†æå®Ÿè¡Œ ({i//batch_size + 1}/{(len(sources)-1)//batch_size + 1})")
            
            # GPT-4-turboã§åˆ†æ
            batch_analysis = self._analyze_content_batch(batch, session.theme)
            analysis_results.extend(batch_analysis)
            
            session.processed_items = len(analysis_results)
            progress = 0.4 + (0.3 * len(analysis_results) / len(sources))
            self._notify_progress("analysis", progress, f"åˆ†æé€²è¡Œä¸­: {len(analysis_results)}/{len(sources)}")
            
            # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
            time.sleep(2)
        
        # åˆ†æçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«è¿½åŠ ä¿å­˜
        session_data["analysis_results"] = {
            "analyzed_content": analysis_results,
            "key_findings": self._extract_key_findings(analysis_results),
            "extracted_entities": self._extract_entities(analysis_results),
            "identified_relationships": self._identify_relationships(analysis_results)
        }
        
        session.important_findings = session_data["analysis_results"]["key_findings"]
        self._save_session_data(session, session_data)
        
        print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æå®Œäº†: {len(analysis_results)}ä»¶")
    
    def _phase_knowledge_integration(self, session: LearningSession):
        """Phase 3: çŸ¥è­˜çµ±åˆ"""
        print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] ğŸ”— Phase 3: çŸ¥è­˜çµ±åˆé–‹å§‹")
        session.current_phase = "integration"
        self._notify_progress("integration", 0.7, "çŸ¥è­˜çµ±åˆé–‹å§‹")
        
        # çµ±åˆçš„ãªçŸ¥è­˜ç”Ÿæˆ
        integrated_knowledge = self._generate_integrated_knowledge(session)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆçµæœã‚’è¿½åŠ 
        session_file = self.sessions_dir / f"{session.session_id}.json"
        with open(session_file, 'r', encoding='utf-8') as f:
            session_data = json.load(f)
        
        session_data["generated_knowledge"] = integrated_knowledge
        session_data["session_statistics"] = self._calculate_session_statistics(session)
        
        self._save_session_data(session, session_data)
        self._notify_progress("integration", 0.9, "çŸ¥è­˜çµ±åˆå®Œäº†")
        
        print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âœ… çŸ¥è­˜çµ±åˆå®Œäº†")
    
    def _generate_search_queries(self, theme: str, depth_level: int) -> List[str]:
        """æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ"""
        self.debug_logger.debug("Webæ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆé–‹å§‹", {
            "theme": theme,
            "depth_level": depth_level
        })
        
        base_queries = [
            f"{theme} æœ€æ–°æƒ…å ±",
            f"{theme} ãƒˆãƒ¬ãƒ³ãƒ‰ 2024",
            f"{theme} æŠ€è¡“å‹•å‘",
        ]
        
        if depth_level >= 3:
            base_queries.extend([
                f"{theme} å¸‚å ´åˆ†æ",
                f"{theme} èª²é¡Œ",
                f"{theme} å°†æ¥æ€§",
            ])
        
        if depth_level >= 4:
            base_queries.extend([
                f"{theme} å…·ä½“çš„äº‹ä¾‹",
                f"{theme} å®Ÿè£…æ–¹æ³•",
                f"{theme} ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹",
            ])
        
        self.debug_logger.info("Webæ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆå®Œäº†", {
            "generated_queries": base_queries,
            "total_queries": len(base_queries)
        })
        
        return base_queries
    
    def _perform_web_search(self, query: str) -> List[Dict]:
        """Webæ¤œç´¢å®Ÿè¡Œï¼ˆçµ±åˆæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ä½¿ç”¨ï¼‰- å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚"""
        result = self._perform_web_search_detailed(query)
        return result.get("sources", [])
    
    def _perform_web_search_detailed(self, query: str) -> Dict[str, Any]:
        """è©³ç´°Webæ¤œç´¢å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚‚å«ã‚€ï¼‰"""
        start_time = time.time()
        
        self.debug_logger.debug("Webæ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹", {
            "query": query,
            "search_manager": type(self.search_manager).__name__
        })
        
        try:
            # çµ±åˆæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§æ¤œç´¢å®Ÿè¡Œ
            search_result = self.search_manager.search(query, max_results=5)
            execution_time = time.time() - start_time
            
            self.debug_logger.info("Googleæ¤œç´¢çµæœ", {
                "query": query,
                "engine_used": search_result["engine_used"],
                "success": search_result["success"],
                "total_results": search_result["total_results"],
                "execution_time": execution_time,
                "quota_remaining": search_result.get("quota_remaining", "ä¸æ˜")
            })
            
            if search_result["success"] and search_result["results"]:
                sources = search_result["results"]
                
                # çµæœã®è©³ç´°ãƒ­ã‚°
                for i, source in enumerate(sources, 1):
                    self.debug_logger.debug(f"Webæ¤œç´¢çµæœå‡¦ç†: {i}", {
                        "source_id": source.get("source_id", "unknown"),
                        "source_type": source.get("source_type", "unknown"),
                        "url": source.get("url", ""),
                        "title_length": len(source.get("title", "")),
                        "content_length": len(source.get("content", ""))
                    })
                
                # æˆåŠŸãƒ­ã‚°
                self.debug_logger.log_web_search(
                    query, f"{search_result['engine_used']}_engine", 200, 
                    len(sources), {
                        "execution_time": execution_time,
                        "engine_used": search_result["engine_used"],
                        "quota_remaining": search_result.get("quota_remaining", "ä¸æ˜")
                    }
                )
                
                return {
                    "success": True,
                    "sources": sources,
                    "execution_time": execution_time,
                    "engine_used": search_result["engine_used"],
                    "quota_remaining": search_result.get("quota_remaining")
                }
            
            else:
                # æ¤œç´¢å¤±æ•—ã®å ´åˆ
                error_message = search_result.get("error", "æ¤œç´¢çµæœãŒç©º")
                quota_exceeded = search_result.get("quota_exceeded", False)
                
                self.debug_logger.warning("Googleæ¤œç´¢å¤±æ•—", {
                    "query": query,
                    "engine_used": search_result["engine_used"],
                    "error": error_message,
                    "execution_time": execution_time,
                    "quota_exceeded": quota_exceeded,
                    "quota_remaining": search_result.get("quota_remaining", "ä¸æ˜")
                })
                
                if quota_exceeded:
                    print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âš ï¸ Googleæ¤œç´¢åˆ¶é™åˆ°é”: {error_message}")
                    print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] ğŸ’¡ æ˜æ—¥ã¾ã§å¾…ã¤ã‹ã€åˆ¥ã®APIã®è¿½åŠ ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
                else:
                    print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âš ï¸ Googleæ¤œç´¢å¤±æ•—: {error_message}")
                
                return {
                    "success": False,
                    "sources": [],
                    "error_message": error_message,
                    "error_type": "search_failed",
                    "quota_exceeded": quota_exceeded,
                    "execution_time": execution_time,
                    "engine_used": search_result.get("engine_used", "unknown"),
                    "quota_remaining": search_result.get("quota_remaining")
                }
        
        except Exception as e:
            execution_time = time.time() - start_time
            self.debug_logger.error("Webæ¤œç´¢äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼", {
                "query": query,
                "execution_time": execution_time,
                "error_type": type(e).__name__
            }, e)
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âš ï¸ Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            
            return {
                "success": False,
                "sources": [],
                "error_message": str(e),
                "error_type": "exception",
                "quota_exceeded": False,
                "execution_time": execution_time,
                "engine_used": "unknown"
            }
    
    def _analyze_content_batch(self, sources: List[Dict], theme: str) -> List[Dict]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒãƒåˆ†æ"""
        if not self.openai_client:
            return []
        
        try:
            # ãƒãƒƒãƒåˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            content_texts = []
            for i, source in enumerate(sources):
                content_texts.append(f"[è¨˜äº‹{i+1}] {source.get('title', '')}\n{source.get('content', '')}")
            
            batch_content = "\n\n".join(content_texts)
            
            prompt = f"""
ä»¥ä¸‹ã®{len(sources)}ä»¶ã®è¨˜äº‹ã‚’ã€Œ{theme}ã€ã«é–¢ã™ã‚‹æƒ…å ±ã¨ã—ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚

{batch_content}

å„è¨˜äº‹ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š
1. é‡è¦åº¦ (1-10)
2. ã‚«ãƒ†ã‚´ãƒª (æŠ€è¡“/å¸‚å ´/ãƒˆãƒ¬ãƒ³ãƒ‰/å®Ÿç”¨/ãã®ä»–)
3. ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ (3å€‹ä»¥å†…)
4. é–¢é€£ã™ã‚‹æŠ€è¡“ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»äººç‰©
5. ä¿¡é ¼æ€§ (1-10)

JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
            
            response = self.openai_client.chat.completions.create(
                model=self.gpt_config["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=self.gpt_config["temperature"],
                max_tokens=self.gpt_config["max_tokens"]
            )
            
            # ã‚³ã‚¹ãƒˆè¨ˆç®—
            input_tokens = len(prompt.split())
            output_tokens = len(response.choices[0].message.content.split())
            cost = (input_tokens * 0.01 / 1000) + (output_tokens * 0.03 / 1000)
            self.current_session.current_cost += cost
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æ
            analysis_text = response.choices[0].message.content
            
            # åˆ†æçµæœã‚’ã‚½ãƒ¼ã‚¹ã¨é–¢é€£ä»˜ã‘
            analysis_results = []
            for i, source in enumerate(sources):
                analysis_results.append({
                    "source": source,
                    "analysis": f"è¨˜äº‹{i+1}ã®åˆ†æçµæœ",  # å®Ÿéš›ã¯GPTãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
                    "importance": 7,  # å®Ÿéš›ã¯GPTãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰æŠ½å‡º
                    "category": "æŠ€è¡“",
                    "key_points": ["ãƒã‚¤ãƒ³ãƒˆ1", "ãƒã‚¤ãƒ³ãƒˆ2"],
                    "related_entities": ["ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£1", "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£2"]
                })
            
            return analysis_results
            
        except Exception as e:
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _generate_integrated_knowledge(self, session: LearningSession) -> Dict:
        """çµ±åˆçŸ¥è­˜ç”Ÿæˆ"""
        # çµ±åˆçŸ¥è­˜ã®ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
        return {
            "structured_knowledge": [],
            "integration_knowledge": [],
            "activity_implications": []
        }
    
    def _extract_key_findings(self, analysis_results: List[Dict]) -> List[Dict]:
        """é‡è¦ç™ºè¦‹æŠ½å‡º"""
        return []
    
    def _extract_entities(self, analysis_results: List[Dict]) -> List[Dict]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º"""
        return []
    
    def _identify_relationships(self, analysis_results: List[Dict]) -> List[Dict]:
        """é–¢ä¿‚æ€§ç‰¹å®š"""
        return []
    
    def _calculate_session_statistics(self, session: LearningSession) -> Dict:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆè¨ˆç®—"""
        return {
            "total_cost": session.current_cost,
            "total_items_processed": session.processed_items,
            "processing_time": 0
        }
    
    def _should_stop_session(self, session: LearningSession) -> bool:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³åœæ­¢åˆ¤å®š"""
        if not session.start_time:
            return False
        
        # æ™‚é–“åˆ¶é™ãƒã‚§ãƒƒã‚¯
        elapsed = (datetime.now() - session.start_time).total_seconds()
        if elapsed > session.time_limit:
            return True
        
        # äºˆç®—åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if session.current_cost > session.budget_limit:
            return True
        
        return False
    
    def _save_session(self, session: LearningSession):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼‰"""
        with self.file_lock:  # ãƒ•ã‚¡ã‚¤ãƒ«ç«¶åˆé˜²æ­¢
            session_file = self.sessions_dir / f"{session.session_id}.json"
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ‡ãƒ¼ã‚¿ä¿æŒã®ãŸã‚ï¼‰
            existing_data = {}
            if session_file.exists():
                try:
                    with open(session_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                except (json.JSONDecodeError, Exception) as e:
                    self.debug_logger.warning("æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—", {
                        "session_id": session.session_id,
                        "error": str(e)
                    })
                    existing_data = {}
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿æ›´æ–°ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒï¼‰
            existing_data["session_metadata"] = asdict(session)
            existing_data["last_updated"] = datetime.now().isoformat()
            
            # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿
            try:
                with open(session_file, 'w', encoding='utf-8') as f:
                    json.dump(existing_data, f, ensure_ascii=False, indent=2, default=str)
                
                self.debug_logger.debug("ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜æˆåŠŸ", {
                    "session_id": session.session_id,
                    "file_size": session_file.stat().st_size,
                    "data_keys": list(existing_data.keys())
                })
                
            except Exception as e:
                self.debug_logger.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜å¤±æ•—", {
                    "session_id": session.session_id,
                    "file_path": str(session_file)
                }, e)
                raise
    
    def _save_session_data(self, session: LearningSession, additional_data: Dict):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿è¿½åŠ ä¿å­˜"""
        with self.file_lock:  # ãƒ•ã‚¡ã‚¤ãƒ«ç«¶åˆé˜²æ­¢
            session_file = self.sessions_dir / f"{session.session_id}.json"
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            existing_data = {}
            if session_file.exists():
                try:
                    with open(session_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                except (json.JSONDecodeError, Exception) as e:
                    self.debug_logger.warning("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—", {
                        "session_id": session.session_id,
                        "error": str(e)
                    })
                    existing_data = {}
            
            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
            existing_data.update(additional_data)
            existing_data["session_metadata"] = asdict(session)
            existing_data["last_updated"] = datetime.now().isoformat()
            
            try:
                with open(session_file, 'w', encoding='utf-8') as f:
                    json.dump(existing_data, f, ensure_ascii=False, indent=2, default=str)
                
                self.debug_logger.debug("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ä¿å­˜æˆåŠŸ", {
                    "session_id": session.session_id,
                    "additional_keys": list(additional_data.keys()),
                    "file_size": session_file.stat().st_size
                })
                
            except Exception as e:
                self.debug_logger.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—", {
                    "session_id": session.session_id,
                    "file_path": str(session_file)
                }, e)
                raise
    
    def pause_session(self, session_id: str) -> bool:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€æ™‚åœæ­¢"""
        try:
            if session_id in self.session_history:
                session = self.session_history[session_id]
                session.status = "paused"
                self._save_session(session)
                self._notify_progress("paused", 0.0, "ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€æ™‚åœæ­¢")
                return True
        except Exception as e:
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€æ™‚åœæ­¢å¤±æ•—: {e}")
        return False
    
    def stop_session(self, session_id: str) -> bool:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³å¼·åˆ¶åœæ­¢"""
        try:
            if session_id in self.session_history:
                session = self.session_history[session_id]
                session.status = "completed"
                session.end_time = datetime.now()
                self._save_session(session)
                self._notify_progress("stopped", 1.0, "ã‚»ãƒƒã‚·ãƒ§ãƒ³å¼·åˆ¶åœæ­¢")
                return True
        except Exception as e:
            print(f"[å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³] âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³åœæ­¢å¤±æ•—: {e}")
        return False
    
    def get_session_status(self, session_id: str) -> Optional[Dict]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹å–å¾—"""
        if session_id in self.session_history:
            session = self.session_history[session_id]
            return {
                "session_id": session.session_id,
                "status": session.status,
                "current_phase": session.current_phase,
                "progress": {
                    "collected_items": session.collected_items,
                    "processed_items": session.processed_items,
                    "current_cost": session.current_cost,
                    "time_elapsed": (datetime.now() - session.start_time).total_seconds() if session.start_time else 0
                }
            }
        return None
    
    def list_sessions(self, limit: int = 10) -> List[Dict]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—"""
        sessions = list(self.session_history.values())
        sessions.sort(key=lambda x: x.start_time or datetime.min, reverse=True)
        
        return [
            {
                "session_id": s.session_id,
                "theme": s.theme,
                "learning_type": s.learning_type,
                "status": s.status,
                "start_time": s.start_time.isoformat() if s.start_time else None,
                "current_cost": s.current_cost
            }
            for s in sessions[:limit]
        ]


# ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    print("=== ActivityLearningEngine ãƒ†ã‚¹ãƒˆ ===")
    
    engine = ActivityLearningEngine()
    
    # ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    session_id = engine.create_session(
        theme="AIéŸ³æ¥½ç”ŸæˆæŠ€è¡“",
        learning_type="æ¦‚è¦",
        depth_level=3,
        time_limit=300,  # 5åˆ†é–“ãƒ†ã‚¹ãƒˆ
        budget_limit=2.0,
        tags=["AI", "éŸ³æ¥½", "æŠ€è¡“èª¿æŸ»"]
    )
    
    print(f"\nğŸ“‹ ä½œæˆã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {session_id}")
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
    def progress_callback(phase: str, progress: float, message: str):
        print(f"[é€²æ—] {phase}: {progress:.1%} - {message}")
    
    engine.add_progress_callback(progress_callback)
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ï¼ˆãƒ†ã‚¹ãƒˆï¼‰
    print("\nğŸš€ ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ãƒ†ã‚¹ãƒˆ:")
    success = engine.start_session(session_id)
    
    if success:
        print("âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æˆåŠŸ")
        
        # å°‘ã—å¾…ã£ã¦ã‹ã‚‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
        time.sleep(5)
        status = engine.get_session_status(session_id)
        if status:
            print(f"\nğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹:")
            print(f"  çŠ¶æ…‹: {status['status']}")
            print(f"  ãƒ•ã‚§ãƒ¼ã‚º: {status['current_phase']}")
            print(f"  é€²æ—: {status['progress']}")
    else:
        print("âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹å¤±æ•—")