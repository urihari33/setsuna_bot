#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ - Phase 2Aå®Ÿè£…
è‡ªç„¶è¨€èªã«ã‚ˆã‚‹æ„å‘³ãƒ™ãƒ¼ã‚¹ã®é«˜ç²¾åº¦å‹•ç”»æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
import re
import math
from collections import defaultdict, Counter
from datetime import datetime
import hashlib

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Windowsãƒ‘ã‚¹è¨­å®š
if os.name == 'nt':
    DATA_DIR = Path("D:/setsuna_bot/youtube_knowledge_system/data")
    CACHE_DIR = Path("D:/setsuna_bot/semantic_search_cache")
else:
    DATA_DIR = Path("/mnt/d/setsuna_bot/youtube_knowledge_system/data")
    CACHE_DIR = Path("/mnt/d/setsuna_bot/semantic_search_cache")

CACHE_DIR.mkdir(parents=True, exist_ok=True)

@dataclass
class SearchResult:
    """æ¤œç´¢çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    video_id: str
    title: str
    artist: str
    relevance_score: float
    semantic_matches: List[str]
    content_type: str  # "music", "gameplay", "talk", "tutorial" etc.
    confidence: float
    matched_keywords: List[str]
    context_relevance: float  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé–¢é€£åº¦

@dataclass
class SemanticQuery:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¯ã‚¨ãƒªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    original_query: str
    normalized_query: str
    extracted_keywords: List[str]
    intent_type: str  # "search", "recommendation", "comparison", "analysis"
    target_attributes: List[str]  # "title", "artist", "genre", "mood", "theme"
    temporal_context: Optional[str]  # "recent", "classic", "trending"

class SemanticSearchEngine:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.knowledge_db_path = DATA_DIR / "unified_knowledge_db.json"
        self.search_cache_path = CACHE_DIR / "semantic_search_cache.json"
        self.knowledge_db = {}
        self.search_cache = {}
        
        # æ„å‘³è§£æç”¨è¾æ›¸ã¨ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.semantic_patterns = self._build_semantic_patterns()
        self.keyword_synonyms = self._build_keyword_synonyms()
        self.genre_mappings = self._build_genre_mappings()
        self.mood_indicators = self._build_mood_indicators()
        
        # çµ±è¨ˆæƒ…å ±
        self.search_statistics = {
            "total_searches": 0,
            "cache_hits": 0,
            "average_results": 0,
            "query_types": defaultdict(int)
        }
        
        self._load_knowledge_db()
        self._load_search_cache()
        
        print("[ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢] âœ… ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
    
    def _build_semantic_patterns(self) -> Dict[str, List[str]]:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰"""
        return {
            "search_intent": [
                r"(.+)(ã‚’|ã®)?(æ¢ã—|æ¤œç´¢|è¦‹ã¤ã‘|çŸ¥ã‚Š)ãŸã„",
                r"(.+)(ã‚ã‚‹|ãªã„)?(ã‹|ï¼Ÿ)?",
                r"(.+)(ã«ã¤ã„ã¦|ã«é–¢ã—ã¦)(æ•™ãˆã¦|èã‹ã›ã¦)",
                r"(.+)(ã©ã†|ã©ã‚“ãª)(æ„Ÿã˜|å°è±¡|æ€ã†)",
                r"(.+)(ãŠã™ã™ã‚|æ¨è–¦)(ã—ã¦|ã®|ã¯)",
            ],
            "recommendation_intent": [
                r"(ä½•ã‹|ã©ã‚“ãª|ã„ã„)(.+)(ãªã„|ã‚ã‚‹)?(ã‹|ï¼Ÿ)?",
                r"(æ–°ã—ã„|æœ€è¿‘ã®|ä»Šã®)(.+)(æ•™ãˆã¦|ç´¹ä»‹)",
                r"(ä¼¼ãŸ|åŒã˜ã‚ˆã†ãª)(.+)(ãªã„|ã‚ã‚‹)?(ã‹|ï¼Ÿ)?",
                r"(æ¬¡ã«|ä»Šåº¦)(ä½•|ã©ã‚Œ)(.+)(ã„ã„|ãŠã™ã™ã‚)",
            ],
            "comparison_intent": [
                r"(.+)(ã¨)(.+)(æ¯”ã¹|é•ã„|å·®)",
                r"(.+)(ã‚ˆã‚Š)(.+)(ã„ã„|è‰¯ã„|å¥½ã)",
                r"(.+)(ã©ã£ã¡|ã©ã¡ã‚‰)(ãŒ|ã®æ–¹ãŒ)",
            ],
            "analysis_intent": [
                r"(.+)(åˆ†æ|è§£æ|è©•ä¾¡)(ã—ã¦|ã‚’)",
                r"(.+)(ç‰¹å¾´|é­…åŠ›|è‰¯ã•)(ã¯|ã£ã¦|ã‚’)",
                r"(.+)(ãªãœ|ã©ã†ã—ã¦)(.+)(ãªã®|ã ã‚ã†)",
            ]
        }
    
    def _build_keyword_synonyms(self) -> Dict[str, List[str]]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åŒç¾©èªè¾æ›¸ã‚’æ§‹ç¯‰"""
        return {
            "éŸ³æ¥½": ["æ›²", "æ­Œ", "ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯", "ã‚µã‚¦ãƒ³ãƒ‰", "æ¥½æ›²", "éŸ³æº"],
            "å‹•ç”»": ["æ˜ åƒ", "ãƒ“ãƒ‡ã‚ª", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„", "ãƒãƒ£ãƒ³ãƒãƒ«", "é…ä¿¡"],
            "ã‚²ãƒ¼ãƒ ": ["ãƒ—ãƒ¬ã‚¤", "å®Ÿæ³", "é…ä¿¡", "ã‚¹ãƒˆãƒªãƒ¼ãƒ ", "ã‚²ãƒ¼ãƒŸãƒ³ã‚°"],
            "ã‚¢ãƒ‹ãƒ¡": ["ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³", "äºŒæ¬¡å…ƒ", "æ¼«ç”»", "ãƒãƒ³ã‚¬"],
            "ãƒœã‚«ãƒ­": ["ãƒœãƒ¼ã‚«ãƒ­ã‚¤ãƒ‰", "åˆéŸ³ãƒŸã‚¯", "VOCALOID", "æ­Œå£°åˆæˆ"],
            "ãƒ­ãƒƒã‚¯": ["ãƒ­ãƒƒã‚¯ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯", "ãƒãƒ³ãƒ‰", "ã‚®ã‚¿ãƒ¼"],
            "ãƒãƒƒãƒ—": ["ãƒãƒƒãƒ—ã‚¹", "J-POP", "æµè¡Œæ­Œ"],
            "ã‚¯ãƒ©ã‚·ãƒƒã‚¯": ["ã‚¯ãƒ©ã‚·ãƒƒã‚¯éŸ³æ¥½", "ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ©", "äº¤éŸ¿æ›²"],
            "EDM": ["é›»å­éŸ³æ¥½", "ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ãƒƒã‚¯", "ãƒ€ãƒ³ã‚¹ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯"],
            "ãƒãƒ©ãƒ¼ãƒ‰": ["ã‚¹ãƒ­ãƒ¼ã‚½ãƒ³ã‚°", "æ„Ÿå‹•çš„", "æ³£ã‘ã‚‹"],
            "ã‚¢ãƒƒãƒ—ãƒ†ãƒ³ãƒ": ["å…ƒæ°—", "æ˜ã‚‹ã„", "ãƒãƒªãŒã„ã„", "ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é«˜ã„"],
            "ç™’ã—": ["ãƒªãƒ©ãƒƒã‚¯ã‚¹", "ç©ã‚„ã‹", "å®‰ã‚‰ã", "è½ã¡ç€ã"],
            "ã‹ã£ã“ã„ã„": ["ã‚¯ãƒ¼ãƒ«", "ã‚¹ã‚¿ã‚¤ãƒªãƒƒã‚·ãƒ¥", "æ ¼å¥½è‰¯ã„"],
            "å¯æ„›ã„": ["ã‚­ãƒ¥ãƒ¼ãƒˆ", "æ„›ã‚‰ã—ã„", "ã‹ã‚ã„ã„"],
            "æ„Ÿå‹•": ["æ³£ã‘ã‚‹", "å¿ƒã«éŸ¿ã", "æ„Ÿæ¿€", "èƒ¸ç†±"],
            "é¢ç™½ã„": ["æ¥½ã—ã„", "ãŠã‚‚ã—ã‚ã„", "ç¬‘ãˆã‚‹", "ãƒ¦ãƒ‹ãƒ¼ã‚¯"]
        }
    
    def _build_genre_mappings(self) -> Dict[str, List[str]]:
        """ã‚¸ãƒ£ãƒ³ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰"""
        return {
            "ãƒ­ãƒƒã‚¯": ["rock", "ãƒãƒ¼ãƒ‰ãƒ­ãƒƒã‚¯", "ãƒ‘ãƒ³ã‚¯", "ãƒ¡ã‚¿ãƒ«", "ã‚ªãƒ«ã‚¿ãƒŠãƒ†ã‚£ãƒ–"],
            "ãƒãƒƒãƒ—": ["pop", "J-POP", "ã‚¢ã‚¤ãƒ‰ãƒ«", "mainstream"],
            "é›»å­éŸ³æ¥½": ["EDM", "ãƒ†ã‚¯ãƒ", "ãƒã‚¦ã‚¹", "dubstep", "ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚«"],
            "ã‚¯ãƒ©ã‚·ãƒƒã‚¯": ["classical", "ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ©", "å®¤å†…æ¥½", "ã‚ªãƒšãƒ©"],
            "ã‚¸ãƒ£ã‚º": ["jazz", "ãƒ–ãƒ«ãƒ¼ã‚¹", "ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³", "ã‚¹ã‚¤ãƒ³ã‚°"],
            "ãƒ’ãƒƒãƒ—ãƒ›ãƒƒãƒ—": ["hip-hop", "rap", "ãƒˆãƒ©ãƒƒãƒ—", "R&B"],
            "ãƒ•ã‚©ãƒ¼ã‚¯": ["folk", "ã‚¢ã‚³ãƒ¼ã‚¹ãƒ†ã‚£ãƒƒã‚¯", "ã‚«ãƒ³ãƒˆãƒªãƒ¼"],
            "ã‚¢ãƒ‹ã‚½ãƒ³": ["ã‚¢ãƒ‹ãƒ¡ã‚½ãƒ³ã‚°", "ã‚­ãƒ£ãƒ©ã‚½ãƒ³", "ã‚²ãƒ¼ã‚½ãƒ³", "å£°å„ª"],
            "ãƒœã‚«ãƒ­": ["VOCALOID", "åˆéŸ³ãƒŸã‚¯", "æ­Œå£°åˆæˆ", "ãƒ‹ã‚³ãƒ‹ã‚³"]
        }
    
    def _build_mood_indicators(self) -> Dict[str, List[str]]:
        """ãƒ ãƒ¼ãƒ‰æŒ‡æ¨™ã‚’æ§‹ç¯‰"""
        return {
            "æ˜ã‚‹ã„": ["æ¥½ã—ã„", "ãƒãƒƒãƒ”ãƒ¼", "å…ƒæ°—", "ãƒã‚¸ãƒ†ã‚£ãƒ–", "é™½æ°—", "çˆ½ã‚„ã‹"],
            "æš—ã„": ["æ‚²ã—ã„", "ãƒ¡ãƒ©ãƒ³ã‚³ãƒªãƒƒã‚¯", "æ†‚é¬±", "é‡ã„", "ã‚·ãƒªã‚¢ã‚¹"],
            "æ¿€ã—ã„": ["ã‚¨ãƒãƒ«ã‚®ãƒƒã‚·ãƒ¥", "ãƒ‘ãƒ¯ãƒ•ãƒ«", "ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯", "ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–"],
            "ç©ã‚„ã‹": ["ãƒªãƒ©ãƒƒã‚¯ã‚¹", "ç™’ã—", "å¹³å’Œ", "å®‰ã‚‰ã", "é™ã‹"],
            "ãƒã‚¹ã‚¿ãƒ«ã‚¸ãƒƒã‚¯": ["æ‡ã‹ã—ã„", "æ˜”", "ãƒ¬ãƒˆãƒ­", "æ€ã„å‡º"],
            "ãƒ­ãƒãƒ³ãƒãƒƒã‚¯": ["æ‹æ„›", "æ„›", "ç”˜ã„", "ãƒ‰ãƒ©ãƒãƒãƒƒã‚¯"],
            "ã‚¯ãƒ¼ãƒ«": ["ã‹ã£ã“ã„ã„", "ã‚¹ã‚¿ã‚¤ãƒªãƒƒã‚·ãƒ¥", "æ´—ç·´ã•ã‚ŒãŸ"],
            "ã‚­ãƒ¥ãƒ¼ãƒˆ": ["å¯æ„›ã„", "æ„›ã‚‰ã—ã„", "ãƒãƒ£ãƒ¼ãƒŸãƒ³ã‚°"]
        }
    
    def _load_knowledge_db(self):
        """çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            if self.knowledge_db_path.exists():
                with open(self.knowledge_db_path, 'r', encoding='utf-8') as f:
                    self.knowledge_db = json.load(f)
                video_count = len(self.knowledge_db.get("videos", {}))
                print(f"[ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢] ğŸ“Š {video_count}ä»¶ã®å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰")
            else:
                print(f"[ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢] âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.knowledge_db_path}")
        except Exception as e:
            print(f"[ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢] âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _load_search_cache(self):
        """æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            if self.search_cache_path.exists():
                with open(self.search_cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    self.search_cache = cache_data.get("searches", {})
                    self.search_statistics = cache_data.get("statistics", self.search_statistics)
                print(f"[ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢] ğŸ’¾ {len(self.search_cache)}ä»¶ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ­ãƒ¼ãƒ‰")
        except Exception as e:
            print(f"[ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢] âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_search_cache(self):
        """æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        try:
            cache_data = {
                "searches": self.search_cache,
                "statistics": dict(self.search_statistics),
                "last_updated": datetime.now().isoformat()
            }
            with open(self.search_cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢] âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def parse_semantic_query(self, query: str) -> SemanticQuery:
        """ã‚¯ã‚¨ãƒªã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è§£æ"""
        normalized_query = query.strip().lower()
        
        # ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†æ
        intent_type = "search"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        for intent, patterns in self.semantic_patterns.items():
            for pattern in patterns:
                if re.search(pattern, normalized_query):
                    intent_type = intent.replace("_intent", "")
                    break
            if intent_type != "search":
                break
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        extracted_keywords = self._extract_keywords(normalized_query)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±æ€§åˆ¤å®š
        target_attributes = self._determine_target_attributes(normalized_query)
        
        # æ™‚é–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        temporal_context = self._detect_temporal_context(normalized_query)
        
        return SemanticQuery(
            original_query=query,
            normalized_query=normalized_query,
            extracted_keywords=extracted_keywords,
            intent_type=intent_type,
            target_attributes=target_attributes,
            temporal_context=temporal_context
        )
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        keywords = []
        
        # ç›´æ¥çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        for main_keyword, synonyms in self.keyword_synonyms.items():
            if main_keyword in text or any(syn in text for syn in synonyms):
                keywords.append(main_keyword)
        
        # ã‚¸ãƒ£ãƒ³ãƒ«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        for genre, terms in self.genre_mappings.items():
            if any(term in text for term in terms):
                keywords.append(genre)
        
        # ãƒ ãƒ¼ãƒ‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        for mood, indicators in self.mood_indicators.items():
            if any(indicator in text for indicator in indicators):
                keywords.append(mood)
        
        # ä¸€èˆ¬çš„ãªéŸ³æ¥½ç”¨èª
        music_terms = ["ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ", "æ­Œæ‰‹", "ãƒãƒ³ãƒ‰", "ã‚°ãƒ«ãƒ¼ãƒ—", "ã‚½ãƒ­", "ãƒ‡ãƒ¥ã‚ª"]
        for term in music_terms:
            if term in text:
                keywords.append(term)
        
        return list(set(keywords))
    
    def _determine_target_attributes(self, text: str) -> List[str]:
        """æ¤œç´¢å¯¾è±¡å±æ€§ã‚’åˆ¤å®š"""
        attributes = []
        
        title_indicators = ["ã‚¿ã‚¤ãƒˆãƒ«", "æ›²å", "åå‰", "é¡Œå"]
        if any(indicator in text for indicator in title_indicators):
            attributes.append("title")
        
        artist_indicators = ["ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ", "æ­Œæ‰‹", "ãƒãƒ³ãƒ‰", "ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼"]
        if any(indicator in text for indicator in artist_indicators):
            attributes.append("artist")
        
        genre_indicators = ["ã‚¸ãƒ£ãƒ³ãƒ«", "ç¨®é¡", "ã‚«ãƒ†ã‚´ãƒª"]
        if any(indicator in text for indicator in genre_indicators):
            attributes.append("genre")
        
        mood_indicators = ["é›°å›²æ°—", "ãƒ ãƒ¼ãƒ‰", "æ„Ÿã˜", "å°è±¡"]
        if any(indicator in text for indicator in mood_indicators):
            attributes.append("mood")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨å±æ€§
        if not attributes:
            attributes = ["title", "artist", "genre", "mood", "theme"]
        
        return attributes
    
    def _detect_temporal_context(self, text: str) -> Optional[str]:
        """æ™‚é–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œå‡º"""
        recent_indicators = ["æœ€è¿‘", "æ–°ã—ã„", "ä»Šã®", "ç¾åœ¨", "ä»Šå¹´"]
        if any(indicator in text for indicator in recent_indicators):
            return "recent"
        
        classic_indicators = ["æ˜”", "å¤ã„", "ã‚¯ãƒ©ã‚·ãƒƒã‚¯", "éå»", "æ‡ã‹ã—ã„"]
        if any(indicator in text for indicator in classic_indicators):
            return "classic"
        
        trending_indicators = ["äººæ°—", "æµè¡Œ", "ãƒˆãƒ¬ãƒ³ãƒ‰", "è©±é¡Œ"]
        if any(indicator in text for indicator in trending_indicators):
            return "trending"
        
        return None
    
    def search(self, query: str, max_results: int = 10, use_cache: bool = True) -> List[SearchResult]:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Ÿè¡Œ"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if use_cache:
            cache_key = self._generate_cache_key(query, max_results)
            if cache_key in self.search_cache:
                self.search_statistics["cache_hits"] += 1
                cached_results = self.search_cache[cache_key]
                return [SearchResult(**result) for result in cached_results]
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è§£æ
        semantic_query = self.parse_semantic_query(query)
        
        # æ¤œç´¢å®Ÿè¡Œ
        search_results = self._execute_semantic_search(semantic_query, max_results)
        
        # çµ±è¨ˆæ›´æ–°
        self.search_statistics["total_searches"] += 1
        self.search_statistics["query_types"][semantic_query.intent_type] += 1
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        if use_cache:
            cache_key = self._generate_cache_key(query, max_results)
            self.search_cache[cache_key] = [asdict(result) for result in search_results]
            self._save_search_cache()
        
        return search_results
    
    def _generate_cache_key(self, query: str, max_results: int) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        content = f"{query}_{max_results}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _execute_semantic_search(self, semantic_query: SemanticQuery, max_results: int) -> List[SearchResult]:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Ÿè¡Œ"""
        results = []
        videos = self.knowledge_db.get("videos", {})
        
        for video_id, video_data in videos.items():
            # é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            relevance_score = self._calculate_relevance_score(video_data, semantic_query)
            
            if relevance_score > 0.1:  # æœ€å°é–¾å€¤
                # æ¤œç´¢çµæœä½œæˆ
                result = self._create_search_result(
                    video_id, video_data, semantic_query, relevance_score
                )
                results.append(result)
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        # çµæœæ•°åˆ¶é™
        return results[:max_results]
    
    def _calculate_relevance_score(self, video_data: Dict, semantic_query: SemanticQuery) -> float:
        """é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        total_score = 0.0
        max_score = 0.0
        
        metadata = video_data.get("metadata", {})
        creative_insight = video_data.get("creative_insight", {})
        
        # ã‚¿ã‚¤ãƒˆãƒ«ä¸€è‡´åº¦ (30%)
        title_score = self._calculate_text_similarity(
            metadata.get("title", ""), semantic_query.normalized_query
        )
        total_score += title_score * 0.3
        max_score += 0.3
        
        # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ/ãƒãƒ£ãƒ³ãƒãƒ«ä¸€è‡´åº¦ (20%)
        channel_title = metadata.get("channel_title", "")
        artist_score = self._calculate_text_similarity(
            channel_title, semantic_query.normalized_query
        )
        total_score += artist_score * 0.2
        max_score += 0.2
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´åº¦ (25%)
        keyword_score = self._calculate_keyword_similarity(
            video_data, semantic_query.extracted_keywords
        )
        total_score += keyword_score * 0.25
        max_score += 0.25
        
        # ãƒ†ãƒ¼ãƒ/ã‚¸ãƒ£ãƒ³ãƒ«ä¸€è‡´åº¦ (15%)
        theme_score = self._calculate_theme_similarity(
            creative_insight, semantic_query.extracted_keywords
        )
        total_score += theme_score * 0.15
        max_score += 0.15
        
        # æ™‚é–“çš„é–¢é€£æ€§ (10%)
        temporal_score = self._calculate_temporal_relevance(
            metadata, semantic_query.temporal_context
        )
        total_score += temporal_score * 0.1
        max_score += 0.1
        
        # æ­£è¦åŒ–
        return total_score / max_score if max_score > 0 else 0.0
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—"""
        if not text1 or not text2:
            return 0.0
        
        text1_lower = text1.lower()
        text2_lower = text2.lower()
        
        # å®Œå…¨ä¸€è‡´
        if text2_lower in text1_lower:
            return 1.0
        
        # éƒ¨åˆ†ä¸€è‡´
        words1 = set(text1_lower.split())
        words2 = set(text2_lower.split())
        
        if words1 and words2:
            intersection = words1.intersection(words2)
            union = words1.union(words2)
            return len(intersection) / len(union)
        
        return 0.0
    
    def _calculate_keyword_similarity(self, video_data: Dict, keywords: List[str]) -> float:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¡ä¼¼åº¦è¨ˆç®—"""
        if not keywords:
            return 0.0
        
        video_text = " ".join([
            video_data.get("metadata", {}).get("title", ""),
            video_data.get("metadata", {}).get("description", ""),
            " ".join(video_data.get("metadata", {}).get("tags", []))
        ]).lower()
        
        matches = 0
        for keyword in keywords:
            # ç›´æ¥ä¸€è‡´
            if keyword.lower() in video_text:
                matches += 1
                continue
            
            # åŒç¾©èªä¸€è‡´
            synonyms = self.keyword_synonyms.get(keyword, [])
            if any(syn.lower() in video_text for syn in synonyms):
                matches += 1
        
        return matches / len(keywords)
    
    def _calculate_theme_similarity(self, creative_insight: Dict, keywords: List[str]) -> float:
        """ãƒ†ãƒ¼ãƒé¡ä¼¼åº¦è¨ˆç®—"""
        if not keywords or not creative_insight:
            return 0.0
        
        themes = creative_insight.get("themes", [])
        if not themes:
            return 0.0
        
        theme_text = " ".join(themes).lower()
        
        matches = 0
        for keyword in keywords:
            if keyword.lower() in theme_text:
                matches += 1
        
        return matches / len(keywords) if keywords else 0.0
    
    def _calculate_temporal_relevance(self, metadata: Dict, temporal_context: Optional[str]) -> float:
        """æ™‚é–“çš„é–¢é€£æ€§è¨ˆç®—"""
        if not temporal_context:
            return 0.5  # ä¸­æ€§å€¤
        
        published_at = metadata.get("published_at", "")
        if not published_at:
            return 0.5
        
        try:
            pub_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            now = datetime.now()
            days_ago = (now - pub_date).days
            
            if temporal_context == "recent":
                # æœ€è¿‘ï¼ˆ30æ—¥ä»¥å†…ï¼‰
                return max(0.0, 1.0 - days_ago / 365)
            elif temporal_context == "classic":
                # å¤ã„ï¼ˆ1å¹´ä»¥ä¸Šå‰ï¼‰
                return min(1.0, days_ago / 365)
            elif temporal_context == "trending":
                # äººæ°—åº¦ã«åŸºã¥ãï¼ˆview_countä½¿ç”¨ï¼‰
                view_count = metadata.get("view_count", 0)
                if view_count > 100000:
                    return 1.0
                elif view_count > 10000:
                    return 0.7
                else:
                    return 0.3
        except:
            pass
        
        return 0.5
    
    def _create_search_result(self, video_id: str, video_data: Dict, 
                            semantic_query: SemanticQuery, relevance_score: float) -> SearchResult:
        """æ¤œç´¢çµæœä½œæˆ"""
        metadata = video_data.get("metadata", {})
        custom_info = video_data.get("custom_info", {})
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ
        title = custom_info.get("manual_title") or metadata.get("title", "Unknown")
        artist = custom_info.get("manual_artist") or metadata.get("channel_title", "Unknown")
        
        # ä¸€è‡´ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        matched_keywords = [
            kw for kw in semantic_query.extracted_keywords 
            if kw.lower() in (title + " " + artist).lower()
        ]
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒƒãƒ
        semantic_matches = []
        for keyword in semantic_query.extracted_keywords:
            if keyword in self.keyword_synonyms:
                semantic_matches.append(f"{keyword} (æ„å‘³çš„ä¸€è‡´)")
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—åˆ¤å®š
        content_type = self._determine_content_type(video_data)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé–¢é€£åº¦
        context_relevance = self._calculate_context_relevance(video_data, semantic_query)
        
        return SearchResult(
            video_id=video_id,
            title=title,
            artist=artist,
            relevance_score=relevance_score,
            semantic_matches=semantic_matches,
            content_type=content_type,
            confidence=min(1.0, relevance_score * 1.2),
            matched_keywords=matched_keywords,
            context_relevance=context_relevance
        )
    
    def _determine_content_type(self, video_data: Dict) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—åˆ¤å®š"""
        metadata = video_data.get("metadata", {})
        title = metadata.get("title", "").lower()
        description = metadata.get("description", "").lower()
        tags = " ".join(metadata.get("tags", [])).lower()
        
        content_text = f"{title} {description} {tags}"
        
        if any(word in content_text for word in ["music", "ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯", "æ­Œ", "æ›²", "mv"]):
            return "music"
        elif any(word in content_text for word in ["ã‚²ãƒ¼ãƒ ", "game", "å®Ÿæ³", "ãƒ—ãƒ¬ã‚¤"]):
            return "gameplay"
        elif any(word in content_text for word in ["talk", "ãƒˆãƒ¼ã‚¯", "é›‘è«‡", "é…ä¿¡"]):
            return "talk"
        elif any(word in content_text for word in ["tutorial", "è§£èª¬", "è¬›åº§", "æ•™ãˆ"]):
            return "tutorial"
        else:
            return "general"
    
    def _calculate_context_relevance(self, video_data: Dict, semantic_query: SemanticQuery) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé–¢é€£åº¦è¨ˆç®—"""
        # ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã«åŸºã¥ãé–¢é€£åº¦èª¿æ•´
        base_relevance = 0.5
        
        content_type = self._determine_content_type(video_data)
        
        if semantic_query.intent_type == "recommendation":
            # æ¨è–¦ã®å ´åˆã€äººæ°—åº¦ã‚’è€ƒæ…®
            view_count = video_data.get("metadata", {}).get("view_count", 0)
            if view_count > 50000:
                base_relevance += 0.3
        
        elif semantic_query.intent_type == "analysis":
            # åˆ†æã®å ´åˆã€è©³ç´°æƒ…å ±ãŒã‚ã‚‹ã‹ã‚’è€ƒæ…®
            creative_insight = video_data.get("creative_insight", {})
            if creative_insight.get("themes") or creative_insight.get("insights"):
                base_relevance += 0.4
        
        return min(1.0, base_relevance)
    
    def get_search_statistics(self) -> Dict[str, Any]:
        """æ¤œç´¢çµ±è¨ˆæƒ…å ±å–å¾—"""
        return dict(self.search_statistics)
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        self.search_cache.clear()
        self._save_search_cache()
        print("[ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢] ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    
    def suggest_related_queries(self, query: str) -> List[str]:
        """é–¢é€£ã‚¯ã‚¨ãƒªææ¡ˆ"""
        semantic_query = self.parse_semantic_query(query)
        suggestions = []
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ææ¡ˆ
        for keyword in semantic_query.extracted_keywords:
            if keyword in self.keyword_synonyms:
                for synonym in self.keyword_synonyms[keyword][:2]:
                    suggestions.append(f"{synonym}ã®å‹•ç”»")
        
        # ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®ææ¡ˆ
        if semantic_query.intent_type == "search":
            suggestions.extend([
                f"{query}ã«ä¼¼ãŸæ›²",
                f"{query}ã®ãŠã™ã™ã‚",
                f"{query}ã«ã¤ã„ã¦è©³ã—ã"
            ])
        
        return suggestions[:5]  # ä¸Šä½5ä»¶