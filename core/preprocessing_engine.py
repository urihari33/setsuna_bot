#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PreProcessingEngine - GPT-3.5ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªå‰å‡¦ç†ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
"""

import json
import os
import openai
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import hashlib
import time
from datetime import datetime
from .config_manager import get_config_manager

@dataclass
class PreProcessingResult:
    """å‰å‡¦ç†çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    source_id: str
    content_hash: str
    relevance_score: float  # 0.0-1.0
    quality_score: float   # 0.0-1.0
    importance_score: float # 0.0-1.0
    category: str          # "æŠ€è¡“", "å¸‚å ´", "ãƒˆãƒ¬ãƒ³ãƒ‰", "å®Ÿç”¨", "ãã®ä»–", "ç„¡é–¢ä¿‚"
    key_topics: List[str]
    confidence: float      # åˆ¤å®šã®ä¿¡é ¼åº¦
    processing_time: float
    should_proceed: bool   # è©³ç´°åˆ†æã«é€²ã‚€ã¹ãã‹
    reason: str           # åˆ¤å®šç†ç”±
    gpt_tokens_used: int

class PreProcessingEngine:
    """GPT-3.5ã«ã‚ˆã‚‹å‰å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # OpenAIè¨­å®š
        self.openai_client = None
        self._initialize_openai()
        
        # GPT-3.5è¨­å®š
        self.gpt35_config = {
            "model": "gpt-3.5-turbo",
            "temperature": 0.1,  # ä¸€è²«æ€§é‡è¦–
            "max_tokens": 300,   # ç°¡æ½”ãªåˆ†æ
            "timeout": 30
        }
        
        # Rate Limitingå¯¾ç­–è¨­å®š
        self.rate_limiting = {
            "request_interval": 2.0,    # APIå‘¼ã³å‡ºã—é–“éš”ï¼ˆç§’ï¼‰
            "batch_size": 5,            # ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¶é™
            "max_retries": 3,           # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            "backoff_factor": 2.0       # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ä¿‚æ•°
        }
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¾å€¤
        self.thresholds = {
            "relevance_min": 0.3,     # é–¢é€£æ€§æœ€å°å€¤
            "quality_min": 0.4,       # å“è³ªæœ€å°å€¤
            "importance_min": 0.2,    # é‡è¦åº¦æœ€å°å€¤
            "confidence_min": 0.6,    # ä¿¡é ¼åº¦æœ€å°å€¤
            "combined_min": 0.5       # ç·åˆã‚¹ã‚³ã‚¢æœ€å°å€¤
        }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.enable_cache = True
        self.cache_duration_hours = 24
        self._cache = {}
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_processed": 0,
            "filtered_out": 0,
            "cache_hits": 0,
            "total_tokens_used": 0,
            "total_cost": 0.0,
            "average_processing_time": 0.0
        }
        
        print("[å‰å‡¦ç†] âœ… PreProcessingEngineåˆæœŸåŒ–å®Œäº†")
    
    def _initialize_openai(self):
        """OpenAI APIåˆæœŸåŒ–"""
        try:
            # ConfigManagerçµŒç”±ã§OpenAIè¨­å®šå–å¾—
            config = get_config_manager()
            openai_key = config.get_openai_key()
            
            if openai_key:
                openai.api_key = openai_key
                self.openai_client = openai
                
                # æ¥ç¶šãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                try:
                    # ç°¡å˜ãªæ¥ç¶šãƒ†ã‚¹ãƒˆ
                    test_response = openai.models.list()
                    if test_response:
                        print("[å‰å‡¦ç†] âœ… OpenAI APIè¨­å®šãƒ»æ¥ç¶šç¢ºèªå®Œäº†")
                        return True
                except Exception as api_error:
                    print(f"[å‰å‡¦ç†] âŒ OpenAI APIæ¥ç¶šå¤±æ•—: {api_error}")
                    self.openai_client = None
                    return False
            else:
                print("[å‰å‡¦ç†] âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                print("  .envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
                self.openai_client = None
                return False
                
        except Exception as e:
            print(f"[å‰å‡¦ç†] âŒ OpenAI APIåˆæœŸåŒ–å¤±æ•—: {e}")
            self.openai_client = None
            return False
    
    def set_thresholds(self, **kwargs):
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¾å€¤è¨­å®š"""
        for key, value in kwargs.items():
            if key in self.thresholds:
                self.thresholds[key] = value
                print(f"[å‰å‡¦ç†] âš™ï¸ é–¾å€¤æ›´æ–°: {key} = {value}")
    
    def preprocess_content_batch(self, 
                                sources: List[Dict[str, Any]], 
                                theme: str,
                                target_categories: List[str] = None,
                                safe_mode: bool = False) -> List[PreProcessingResult]:
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒãƒå‰å‡¦ç†ï¼ˆRate Limitingå¯¾ç­–å¼·åŒ–ç‰ˆï¼‰
        
        Args:
            sources: ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
            theme: å­¦ç¿’ãƒ†ãƒ¼ãƒ
            target_categories: å¯¾è±¡ã‚«ãƒ†ã‚´ãƒª
            safe_mode: å®‰å…¨ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚ˆã‚Šé•·ã„é–“éš”ã§å‡¦ç†ï¼‰
            
        Returns:
            å‰å‡¦ç†çµæœãƒªã‚¹ãƒˆ
        """
        if not self.openai_client:
            print("[å‰å‡¦ç†] âš ï¸ OpenAI APIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ")
            return self._fallback_batch_processing(sources, theme, target_categories)
        
        results = []
        batch_start_time = time.time()
        
        # Rate Limitingè¨­å®š
        interval = self.rate_limiting["request_interval"] * (2 if safe_mode else 1)
        batch_size = min(self.rate_limiting["batch_size"], len(sources))
        
        print(f"[å‰å‡¦ç†] ğŸ” ãƒãƒƒãƒå‰å‡¦ç†é–‹å§‹: {len(sources)}ä»¶")
        print(f"[å‰å‡¦ç†] âš™ï¸ Rate Limitingè¨­å®š: é–“éš”{interval}ç§’, ãƒãƒƒãƒã‚µã‚¤ã‚º{batch_size}")
        
        # ãƒãƒƒãƒå˜ä½ã§å‡¦ç†
        for i in range(0, len(sources), batch_size):
            batch_sources = sources[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(sources) + batch_size - 1) // batch_size
            
            print(f"[å‰å‡¦ç†] ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches} å‡¦ç†ä¸­...")
            
            for j, source in enumerate(batch_sources):
                try:
                    result = self._preprocess_single_content_with_retry(source, theme, target_categories)
                    if result:
                        results.append(result)
                        
                        # çµ±è¨ˆæ›´æ–°
                        self.stats["total_processed"] += 1
                        if not result.should_proceed:
                            self.stats["filtered_out"] += 1
                    
                    # Rate Limitingå¯¾å¿œï¼ˆæœ€å¾Œã®ã‚¢ã‚¤ãƒ†ãƒ ä»¥å¤–ï¼‰
                    if j < len(batch_sources) - 1 or i + batch_size < len(sources):
                        print(f"[å‰å‡¦ç†] â³ APIåˆ¶é™å›é¿ã®ãŸã‚ {interval}ç§’å¾…æ©Ÿ...")
                        time.sleep(interval)
                        
                except Exception as e:
                    print(f"[å‰å‡¦ç†] âš ï¸ å€‹åˆ¥å‰å‡¦ç†å¤±æ•—: {e}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                    fallback_result = self._fallback_single_processing(source, theme)
                    if fallback_result:
                        results.append(fallback_result)
                    continue
        
        # ãƒãƒƒãƒçµ±è¨ˆæ›´æ–°
        batch_time = time.time() - batch_start_time
        if self.stats["total_processed"] > 0:
            self.stats["average_processing_time"] = (
                (self.stats["average_processing_time"] * (self.stats["total_processed"] - len(results)) + batch_time) /
                self.stats["total_processed"]
            )
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœï¼ˆã‚¼ãƒ­é™¤ç®—é˜²æ­¢ï¼‰
        passed_results = [r for r in results if r.should_proceed]
        
        print(f"[å‰å‡¦ç†] âœ… ãƒãƒƒãƒå‰å‡¦ç†å®Œäº†:")
        print(f"  å‡¦ç†: {len(results)}ä»¶")
        print(f"  é€šé: {len(passed_results)}ä»¶")
        print(f"  é™¤å¤–: {len(results) - len(passed_results)}ä»¶")
        
        # é€šéç‡è¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—é˜²æ­¢ï¼‰
        if len(results) > 0:
            pass_rate = len(passed_results) / len(results) * 100
            print(f"  é€šéç‡: {pass_rate:.1f}%")
        else:
            print(f"  é€šéç‡: 0.0% (å‡¦ç†çµæœãªã—)")
        
        return results
    
    def _preprocess_single_content_with_retry(self, 
                                            source: Dict[str, Any], 
                                            theme: str,
                                            target_categories: List[str] = None) -> Optional[PreProcessingResult]:
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãå˜ä¸€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‰å‡¦ç†"""
        max_retries = self.rate_limiting["max_retries"]
        backoff_factor = self.rate_limiting["backoff_factor"]
        
        for attempt in range(max_retries + 1):
            try:
                result = self._preprocess_single_content(source, theme, target_categories)
                if result:
                    return result
                else:
                    # GPTåˆ†æå¤±æ•— â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    return self._fallback_single_processing(source, theme)
                    
            except Exception as e:
                if "429" in str(e) or "rate_limit" in str(e).lower():
                    # Rate Limiting ã‚¨ãƒ©ãƒ¼
                    if attempt < max_retries:
                        wait_time = backoff_factor ** attempt
                        print(f"[å‰å‡¦ç†] âš ï¸ Rate Limit (è©¦è¡Œ{attempt+1}/{max_retries+1}) - {wait_time}ç§’å¾Œãƒªãƒˆãƒ©ã‚¤")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"[å‰å‡¦ç†] âŒ Rate Limitæœ€å¤§ãƒªãƒˆãƒ©ã‚¤æ•°åˆ°é” - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†")
                        return self._fallback_single_processing(source, theme)
                else:
                    # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
                    print(f"[å‰å‡¦ç†] âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    return self._fallback_single_processing(source, theme)
        
        return None
    
    def _preprocess_single_content(self, 
                                  source: Dict[str, Any], 
                                  theme: str,
                                  target_categories: List[str] = None) -> Optional[PreProcessingResult]:
        """å˜ä¸€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‰å‡¦ç†"""
        start_time = time.time()
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        content = source.get('content', '') + source.get('title', '')
        content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if self.enable_cache and content_hash in self._cache:
            cache_result = self._cache[content_hash]
            if self._is_cache_valid(cache_result['timestamp']):
                self.stats["cache_hits"] += 1
                print(f"[å‰å‡¦ç†] ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {content_hash}")
                return PreProcessingResult(**cache_result['result'])
        
        # GPT-3.5ã§å‰å‡¦ç†åˆ†æ
        analysis_result = self._analyze_with_gpt35(source, theme, target_categories)
        
        if not analysis_result:
            return None
        
        # å‰å‡¦ç†çµæœä½œæˆ
        processing_time = time.time() - start_time
        
        result = PreProcessingResult(
            source_id=source.get('source_id', ''),
            content_hash=content_hash,
            relevance_score=analysis_result['relevance_score'],
            quality_score=analysis_result['quality_score'],
            importance_score=analysis_result['importance_score'],
            category=analysis_result['category'],
            key_topics=analysis_result['key_topics'],
            confidence=analysis_result['confidence'],
            processing_time=processing_time,
            should_proceed=self._should_proceed_to_detailed_analysis(analysis_result),
            reason=analysis_result['reason'],
            gpt_tokens_used=analysis_result['tokens_used']
        )
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        if self.enable_cache:
            self._cache[content_hash] = {
                'result': asdict(result),
                'timestamp': datetime.now().isoformat()
            }
        
        # çµ±è¨ˆæ›´æ–°
        self.stats["total_tokens_used"] += result.gpt_tokens_used
        self.stats["total_cost"] += self._calculate_gpt35_cost(result.gpt_tokens_used)
        
        return result
    
    def _analyze_with_gpt35(self, 
                           source: Dict[str, Any], 
                           theme: str,
                           target_categories: List[str] = None) -> Optional[Dict]:
        """GPT-3.5ã«ã‚ˆã‚‹åˆ†æ"""
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            title = source.get('title', '')
            content = source.get('content', '')
            url = source.get('url', '')
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·åˆ¶é™ï¼ˆGPT-3.5ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™è€ƒæ…®ï¼‰
            max_content_length = 2000
            if len(content) > max_content_length:
                content = content[:max_content_length] + "..."
            
            categories_text = "ã€".join(target_categories) if target_categories else "æŠ€è¡“ã€å¸‚å ´ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã€å®Ÿç”¨"
            
            prompt = f"""
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã€Œ{theme}ã€ã«é–¢ã™ã‚‹å­¦ç¿’ç´ æã¨ã—ã¦è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘{title}
ã€URLã€‘{url}
ã€å†…å®¹ã€‘{content}

ä»¥ä¸‹ã®è¦³ç‚¹ã§0.0ï½1.0ã®ã‚¹ã‚³ã‚¢ã¨åˆ¤å®šç†ç”±ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

1. é–¢é€£æ€§ï¼ˆãƒ†ãƒ¼ãƒã¨ã®é–¢é€£åº¦ï¼‰
2. å“è³ªï¼ˆæƒ…å ±ã®ä¿¡é ¼æ€§ãƒ»å…·ä½“æ€§ï¼‰  
3. é‡è¦åº¦ï¼ˆå­¦ç¿’ä¾¡å€¤ã®é«˜ã•ï¼‰
4. ã‚«ãƒ†ã‚´ãƒªï¼ˆ{categories_text}ã€ç„¡é–¢ä¿‚ï¼‰
5. ã‚­ãƒ¼ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ3å€‹ä»¥å†…ï¼‰
6. ä¿¡é ¼åº¦ï¼ˆåˆ¤å®šã®ç¢ºä¿¡åº¦ï¼‰

å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
  "relevance_score": 0.8,
  "quality_score": 0.7,
  "importance_score": 0.6,
  "category": "æŠ€è¡“",
  "key_topics": ["AI", "éŸ³æ¥½ç”Ÿæˆ"],
  "confidence": 0.9,
  "reason": "åˆ¤å®šç†ç”±ã®ç°¡æ½”ãªèª¬æ˜"
}}
"""
            
            # GPT-3.5å‘¼ã³å‡ºã—
            response = self.openai_client.chat.completions.create(
                model=self.gpt35_config["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=self.gpt35_config["temperature"],
                max_tokens=self.gpt35_config["max_tokens"]
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æ
            response_text = response.choices[0].message.content.strip()
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
            tokens_used = response.usage.total_tokens if hasattr(response, 'usage') else len(prompt.split()) + len(response_text.split())
            
            # JSONè§£æ
            try:
                # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
                if "```json" in response_text:
                    json_start = response_text.find("```json") + 7
                    json_end = response_text.find("```", json_start)
                    json_text = response_text[json_start:json_end].strip()
                elif "{" in response_text and "}" in response_text:
                    json_start = response_text.find("{")
                    json_end = response_text.rfind("}") + 1
                    json_text = response_text[json_start:json_end]
                else:
                    raise ValueError("JSONå½¢å¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
                analysis_data = json.loads(json_text)
                
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯ãƒ»ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
                result = {
                    "relevance_score": float(analysis_data.get("relevance_score", 0.0)),
                    "quality_score": float(analysis_data.get("quality_score", 0.0)),
                    "importance_score": float(analysis_data.get("importance_score", 0.0)),
                    "category": analysis_data.get("category", "ãã®ä»–"),
                    "key_topics": analysis_data.get("key_topics", []),
                    "confidence": float(analysis_data.get("confidence", 0.5)),
                    "reason": analysis_data.get("reason", "åˆ†æå®Œäº†"),
                    "tokens_used": tokens_used
                }
                
                # ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
                for score_key in ["relevance_score", "quality_score", "importance_score", "confidence"]:
                    result[score_key] = max(0.0, min(1.0, result[score_key]))
                
                return result
                
            except (json.JSONDecodeError, ValueError) as e:
                print(f"[å‰å‡¦ç†] âš ï¸ JSONè§£æå¤±æ•—: {e}")
                print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_text[:200]}...")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç°¡æ˜“åˆ†æï¼‰
                return self._fallback_analysis(source, theme, tokens_used)
                
        except Exception as e:
            print(f"[å‰å‡¦ç†] âŒ GPT-3.5åˆ†æå¤±æ•—: {e}")
            return None
    
    def _fallback_analysis(self, source: Dict[str, Any], theme: str, tokens_used: int = 100) -> Dict:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æï¼ˆGPTå¤±æ•—æ™‚ï¼‰"""
        title = source.get('title', '').lower()
        content = source.get('content', '').lower()
        theme_lower = theme.lower()
        
        # ç°¡æ˜“é–¢é€£æ€§åˆ¤å®š
        relevance_score = 0.0
        if theme_lower in title:
            relevance_score += 0.6
        if theme_lower in content:
            relevance_score += 0.3
        
        # ç°¡æ˜“å“è³ªåˆ¤å®š
        quality_score = 0.5
        if len(content) > 200:
            quality_score += 0.2
        if source.get('source_type') == 'web_search':
            quality_score += 0.1
        
        return {
            "relevance_score": min(1.0, relevance_score),
            "quality_score": min(1.0, quality_score),
            "importance_score": 0.4,
            "category": "ãã®ä»–",
            "key_topics": [theme],
            "confidence": 0.3,
            "reason": "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ",
            "tokens_used": tokens_used
        }
    
    def _should_proceed_to_detailed_analysis(self, analysis_result: Dict) -> bool:
        """è©³ç´°åˆ†æé€²è¡Œåˆ¤å®š"""
        # å€‹åˆ¥é–¾å€¤ãƒã‚§ãƒƒã‚¯
        if analysis_result['relevance_score'] < self.thresholds['relevance_min']:
            return False
        
        if analysis_result['quality_score'] < self.thresholds['quality_min']:
            return False
        
        if analysis_result['importance_score'] < self.thresholds['importance_min']:
            return False
        
        if analysis_result['confidence'] < self.thresholds['confidence_min']:
            return False
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        combined_score = (
            analysis_result['relevance_score'] * 0.4 +
            analysis_result['quality_score'] * 0.3 +
            analysis_result['importance_score'] * 0.3
        )
        
        return combined_score >= self.thresholds['combined_min']
    
    def _is_cache_valid(self, timestamp: str) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            cache_time = datetime.fromisoformat(timestamp)
            age_hours = (datetime.now() - cache_time).total_seconds() / 3600
            return age_hours < self.cache_duration_hours
        except:
            return False
    
    def _calculate_gpt35_cost(self, tokens: int) -> float:
        """GPT-3.5ã‚³ã‚¹ãƒˆè¨ˆç®—"""
        # GPT-3.5-turboæ–™é‡‘: $0.002/1K tokens (å…¥å‡ºåŠ›å…±é€š)
        return tokens * 0.002 / 1000
    
    def get_filtering_summary(self, results: List[PreProcessingResult]) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚µãƒãƒªãƒ¼å–å¾—ï¼ˆã‚¼ãƒ­é™¤ç®—é˜²æ­¢ç‰ˆï¼‰"""
        if not results:
            return {
                "error": "çµæœãƒ‡ãƒ¼ã‚¿ãªã—",
                "total_processed": 0,
                "passed_count": 0,
                "filtered_count": 0,
                "pass_rate": 0.0
            }
        
        passed = [r for r in results if r.should_proceed]
        filtered = [r for r in results if not r.should_proceed]
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        category_stats = {}
        for result in results:
            category = result.category
            if category not in category_stats:
                category_stats[category] = {"total": 0, "passed": 0}
            category_stats[category]["total"] += 1
            if result.should_proceed:
                category_stats[category]["passed"] += 1
        
        # ã‚¹ã‚³ã‚¢çµ±è¨ˆï¼ˆã‚¼ãƒ­é™¤ç®—é˜²æ­¢ï¼‰
        relevance_scores = [r.relevance_score for r in results]
        quality_scores = [r.quality_score for r in results]
        importance_scores = [r.importance_score for r in results]
        
        # å®‰å…¨ãªå¹³å‡è¨ˆç®—
        def safe_average(scores: List[float]) -> float:
            return sum(scores) / len(scores) if scores else 0.0
        
        summary = {
            "total_processed": len(results),
            "passed_count": len(passed),
            "filtered_count": len(filtered),
            "pass_rate": (len(passed) / len(results) * 100) if results else 0.0,
            "category_breakdown": category_stats,
            "score_averages": {
                "relevance": safe_average(relevance_scores),
                "quality": safe_average(quality_scores),
                "importance": safe_average(importance_scores)
            },
            "total_tokens_used": sum(r.gpt_tokens_used for r in results),
            "estimated_cost": sum(self._calculate_gpt35_cost(r.gpt_tokens_used) for r in results),
            "average_processing_time": safe_average([r.processing_time for r in results])
        }
        
        return summary
    
    def get_passed_sources(self, results: List[PreProcessingResult]) -> List[str]:
        """é€šéã—ãŸã‚½ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆå–å¾—"""
        return [r.source_id for r in results if r.should_proceed]
    
    def _fallback_batch_processing(self, sources: List[Dict[str, Any]], theme: str, target_categories: List[str] = None) -> List[PreProcessingResult]:
        """OpenAIå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒãƒƒãƒå‡¦ç†"""
        print("[å‰å‡¦ç†] ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒãƒƒãƒå‡¦ç†é–‹å§‹")
        results = []
        
        for source in sources:
            try:
                result = self._fallback_single_processing(source, theme)
                if result:
                    results.append(result)
                    self.stats["total_processed"] += 1
                    if not result.should_proceed:
                        self.stats["filtered_out"] += 1
            except Exception as e:
                print(f"[å‰å‡¦ç†] âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å¤±æ•—: {e}")
                continue
        
        passed_results = [r for r in results if r.should_proceed]
        print(f"[å‰å‡¦ç†] âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(passed_results)}/{len(results)}ä»¶é€šé")
        
        return results
    
    def _fallback_single_processing(self, source: Dict[str, Any], theme: str) -> Optional[PreProcessingResult]:
        """OpenAIå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å˜ä¸€å‡¦ç†"""
        start_time = time.time()
        
        # åŸºæœ¬æƒ…å ±å–å¾—
        title = source.get('title', '')
        content = source.get('content', '')
        url = source.get('url', '')
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        content_hash = hashlib.sha256((content + title).encode()).hexdigest()[:16]
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹é–¢é€£æ€§åˆ†æ
        relevance_score = self._calculate_keyword_relevance(title + " " + content, theme)
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ»é•·ã•ãƒ™ãƒ¼ã‚¹å“è³ªåˆ†æ
        quality_score = self._calculate_basic_quality(source)
        
        # çµ±è¨ˆçš„é‡è¦åº¦åˆ†æ
        importance_score = self._calculate_statistical_importance(title, content)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼‰
        category = self._determine_basic_category(title + " " + content)
        
        # ã‚­ãƒ¼ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºï¼ˆå˜èªé »åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
        key_topics = self._extract_key_topics(title + " " + content, theme)
        
        # ä¿¡é ¼åº¦ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ãªã®ã§ä½ã‚ï¼‰
        confidence = 0.4
        
        # å‡¦ç†æ™‚é–“
        processing_time = time.time() - start_time
        
        # é€²è¡Œåˆ¤å®š
        combined_score = (relevance_score * 0.4 + quality_score * 0.3 + importance_score * 0.3)
        should_proceed = combined_score >= self.thresholds['combined_min']
        
        result = PreProcessingResult(
            source_id=source.get('source_id', content_hash),
            content_hash=content_hash,
            relevance_score=relevance_score,
            quality_score=quality_score,
            importance_score=importance_score,
            category=category,
            key_topics=key_topics,
            confidence=confidence,
            processing_time=processing_time,
            should_proceed=should_proceed,
            reason="ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æï¼ˆåŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»çµ±è¨ˆè§£æï¼‰",
            gpt_tokens_used=0
        )
        
        return result
    
    def _calculate_keyword_relevance(self, text: str, theme: str) -> float:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹é–¢é€£æ€§è¨ˆç®—"""
        if not text or not theme:
            return 0.0
        
        text_lower = text.lower()
        theme_lower = theme.lower()
        
        # ãƒ†ãƒ¼ãƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç›´æ¥ãƒãƒƒãƒ
        theme_words = theme_lower.split()
        matches = sum(1 for word in theme_words if word in text_lower)
        direct_relevance = min(1.0, matches / len(theme_words))
        
        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        tech_keywords = ['æŠ€è¡“', 'é–‹ç™º', 'ai', 'äººå·¥çŸ¥èƒ½', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚·ã‚¹ãƒ†ãƒ ']
        market_keywords = ['å¸‚å ´', 'ãƒ“ã‚¸ãƒã‚¹', 'ä¼æ¥­', 'æ¥­ç•Œ', 'æˆé•·', 'æŠ•è³‡']
        trend_keywords = ['ãƒˆãƒ¬ãƒ³ãƒ‰', 'å‹•å‘', 'æœ€æ–°', 'å°†æ¥', 'äºˆæ¸¬', 'å±•æœ›']
        
        all_keywords = tech_keywords + market_keywords + trend_keywords
        keyword_matches = sum(1 for keyword in all_keywords if keyword in text_lower)
        keyword_relevance = min(1.0, keyword_matches / 10)
        
        # çµ„ã¿åˆã‚ã›
        return min(1.0, direct_relevance * 0.7 + keyword_relevance * 0.3)
    
    def _calculate_basic_quality(self, source: Dict[str, Any]) -> float:
        """åŸºæœ¬çš„ãªå“è³ªè¨ˆç®—"""
        score = 0.3  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        
        title = source.get('title', '')
        content = source.get('content', '')
        url = source.get('url', '')
        
        # ã‚¿ã‚¤ãƒˆãƒ«å“è³ª
        if len(title) > 10:
            score += 0.1
        if len(title) > 30:
            score += 0.1
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ª
        if len(content) > 100:
            score += 0.1
        if len(content) > 500:
            score += 0.2
        if len(content) > 1000:
            score += 0.1
        
        # URLå“è³ªï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¤å®šï¼‰
        if url:
            trusted_domains = ['wikipedia.org', 'github.com', 'arxiv.org', 'ieee.org', '.edu', '.gov']
            if any(domain in url for domain in trusted_domains):
                score += 0.2
        
        return min(1.0, score)
    
    def _calculate_statistical_importance(self, title: str, content: str) -> float:
        """çµ±è¨ˆçš„é‡è¦åº¦è¨ˆç®—"""
        text = (title + " " + content).lower()
        
        # é‡è¦æŒ‡æ¨™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        important_keywords = [
            'é‡è¦', 'ä¸»è¦', 'æ³¨ç›®', 'ç”»æœŸçš„', 'é©æ–°', 'æ–°ã—ã„', 'æœ€æ–°',
            'important', 'key', 'major', 'significant', 'breakthrough'
        ]
        
        importance_matches = sum(1 for keyword in important_keywords if keyword in text)
        
        # æ•°å€¤ãƒ»ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨
        import re
        numbers = len(re.findall(r'\d+', text))
        
        # æ–‡ç« ã®æ§‹é€ æ€§ï¼ˆå¥èª­ç‚¹ãªã©ï¼‰
        structure_score = min(1.0, text.count('ã€‚') / 10 + text.count('.') / 20)
        
        # çµ„ã¿åˆã‚ã›
        base_score = min(1.0, importance_matches / 5)
        data_score = min(1.0, numbers / 10)
        
        return min(1.0, base_score * 0.5 + data_score * 0.2 + structure_score * 0.3)
    
    def _determine_basic_category(self, text: str) -> str:
        """åŸºæœ¬çš„ãªã‚«ãƒ†ã‚´ãƒªåˆ¤å®š"""
        text_lower = text.lower()
        
        # ã‚«ãƒ†ã‚´ãƒªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        tech_keywords = ['æŠ€è¡“', 'é–‹ç™º', 'ai', 'äººå·¥çŸ¥èƒ½', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ']
        market_keywords = ['å¸‚å ´', 'ãƒ“ã‚¸ãƒã‚¹', 'ä¼æ¥­', 'æ¥­ç•Œ', 'æŠ•è³‡', 'å£²ä¸Š', 'åˆ©ç›Š']
        trend_keywords = ['ãƒˆãƒ¬ãƒ³ãƒ‰', 'å‹•å‘', 'äºˆæ¸¬', 'å°†æ¥', 'å±•æœ›', 'forecast']
        practical_keywords = ['å®Ÿç”¨', 'å¿œç”¨', 'æ´»ç”¨', 'å°å…¥', 'å®Ÿè£…', 'äº‹ä¾‹']
        
        tech_score = sum(1 for keyword in tech_keywords if keyword in text_lower)
        market_score = sum(1 for keyword in market_keywords if keyword in text_lower)
        trend_score = sum(1 for keyword in trend_keywords if keyword in text_lower)
        practical_score = sum(1 for keyword in practical_keywords if keyword in text_lower)
        
        scores = {
            'æŠ€è¡“': tech_score,
            'å¸‚å ´': market_score,
            'ãƒˆãƒ¬ãƒ³ãƒ‰': trend_score,
            'å®Ÿç”¨': practical_score
        }
        
        max_category = max(scores, key=scores.get)
        return max_category if scores[max_category] > 0 else 'ãã®ä»–'
    
    def _extract_key_topics(self, text: str, theme: str, max_topics: int = 3) -> List[str]:
        """ã‚­ãƒ¼ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºï¼ˆå˜èªé »åº¦ãƒ™ãƒ¼ã‚¹ï¼‰"""
        import re
        
        # åŸºæœ¬çš„ãªå‰å‡¦ç†
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text_clean.split()
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã§', 'ã¨', 'ãŸ', 'ã¦', 'ãª', 'ã«', 'ã‚’', 'ã¯',
            'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with'
        }
        
        # å˜èªé »åº¦è¨ˆç®—
        word_freq = {}
        for word in words:
            if len(word) > 2 and word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ãƒ†ãƒ¼ãƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å„ªå…ˆ
        theme_words = theme.lower().split()
        for word in theme_words:
            if word in word_freq:
                word_freq[word] *= 2  # ãƒ†ãƒ¼ãƒé–¢é€£èªã‚’å„ªå…ˆ
        
        # é »åº¦é †ã‚½ãƒ¼ãƒˆ
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        
        # ãƒˆãƒƒãƒ—NæŠ½å‡º
        key_topics = [word for word, freq in sorted_words[:max_topics]]
        
        return key_topics if key_topics else [theme]
    
    def get_top_quality_sources(self, results: List[PreProcessingResult], limit: int = 10) -> List[PreProcessingResult]:
        """é«˜å“è³ªã‚½ãƒ¼ã‚¹ã®ãƒˆãƒƒãƒ—Nå–å¾—"""
        passed_results = [r for r in results if r.should_proceed]
        
        # ç·åˆã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        scored_results = []
        for result in passed_results:
            combined_score = (
                result.relevance_score * 0.4 +
                result.quality_score * 0.3 +
                result.importance_score * 0.3
            )
            scored_results.append((combined_score, result))
        
        scored_results.sort(key=lambda x: x[0], reverse=True)
        
        return [result for _, result in scored_results[:limit]]
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        self._cache.clear()
        print("[å‰å‡¦ç†] ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†")
    
    def get_statistics(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            **self.stats,
            "cache_size": len(self._cache),
            "filter_rate": (self.stats["filtered_out"] / self.stats["total_processed"] * 100) if self.stats["total_processed"] > 0 else 0,
            "cache_hit_rate": (self.stats["cache_hits"] / self.stats["total_processed"] * 100) if self.stats["total_processed"] > 0 else 0,
            "current_thresholds": self.thresholds
        }


# ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    print("=== PreProcessingEngine ãƒ†ã‚¹ãƒˆ ===")
    
    engine = PreProcessingEngine()
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
    test_sources = [
        {
            "source_id": "test_001",
            "title": "AIéŸ³æ¥½ç”Ÿæˆã®æœ€æ–°æŠ€è¡“å‹•å‘",
            "content": "Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ã£ãŸéŸ³æ¥½ç”ŸæˆæŠ€è¡“ãŒæ€¥é€Ÿã«ç™ºå±•ã—ã¦ã„ã‚‹ã€‚ç‰¹ã«OpenAIã®MuseNetã‚„Googleã®MusicTransformerãªã©ãŒæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ã€‚",
            "url": "https://example.com/ai-music-tech",
            "source_type": "web_search"
        },
        {
            "source_id": "test_002", 
            "title": "ä»Šæ—¥ã®å¤©æ°—äºˆå ±",
            "content": "æ˜æ—¥ã¯å…¨å›½çš„ã«æ™´ã‚Œã®äºˆå ±ã§ã™ã€‚æ°—æ¸©ã¯25åº¦ç¨‹åº¦ã«ãªã‚‹è¦‹è¾¼ã¿ã§ã™ã€‚",
            "url": "https://example.com/weather",
            "source_type": "news"
        },
        {
            "source_id": "test_003",
            "title": "éŸ³æ¥½ç”ŸæˆAIãƒ„ãƒ¼ãƒ«ã®æ¯”è¼ƒåˆ†æ",
            "content": "å•†ç”¨AIéŸ³æ¥½ç”Ÿæˆãƒ„ãƒ¼ãƒ«ã®æ©Ÿèƒ½æ¯”è¼ƒã€‚AIVAã€Amper Musicã€Jukedeckç­‰ã®ç‰¹å¾´ã‚’è©³ç´°ã«åˆ†æã€‚ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ã¨ç”Ÿæˆå“è³ªã®è¦³ç‚¹ã‹ã‚‰è©•ä¾¡ã€‚",
            "url": "https://example.com/ai-tools-comparison",
            "source_type": "web_search"
        }
    ]
    
    # å‰å‡¦ç†å®Ÿè¡Œ
    print("\nğŸ” å‰å‡¦ç†ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ:")
    results = engine.preprocess_content_batch(
        sources=test_sources,
        theme="AIéŸ³æ¥½ç”ŸæˆæŠ€è¡“",
        target_categories=["æŠ€è¡“", "å¸‚å ´", "ãƒ„ãƒ¼ãƒ«"]
    )
    
    # çµæœè¡¨ç¤º
    print(f"\nğŸ“Š å‰å‡¦ç†çµæœ:")
    for result in results:
        status = "âœ… é€šé" if result.should_proceed else "âŒ é™¤å¤–"
        print(f"  {result.source_id}: {status}")
        print(f"    é–¢é€£æ€§: {result.relevance_score:.2f}, å“è³ª: {result.quality_score:.2f}, é‡è¦åº¦: {result.importance_score:.2f}")
        print(f"    ã‚«ãƒ†ã‚´ãƒª: {result.category}, ç†ç”±: {result.reason}")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\nğŸ“ˆ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚µãƒãƒªãƒ¼:")
    summary = engine.get_filtering_summary(results)
    print(f"  å‡¦ç†æ•°: {summary['total_processed']}ä»¶")
    print(f"  é€šéæ•°: {summary['passed_count']}ä»¶")
    print(f"  é€šéç‡: {summary['pass_rate']:.1f}%")
    print(f"  æ¨å®šã‚³ã‚¹ãƒˆ: ${summary['estimated_cost']:.4f}")
    
    # çµ±è¨ˆæƒ…å ±
    print(f"\nğŸ“Š ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆ:")
    stats = engine.get_statistics()
    print(f"  ç·å‡¦ç†æ•°: {stats['total_processed']}")
    print(f"  ãƒ•ã‚£ãƒ«ã‚¿ç‡: {stats['filter_rate']:.1f}%")
    print(f"  ç·ã‚³ã‚¹ãƒˆ: ${stats['total_cost']:.4f}")