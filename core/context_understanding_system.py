#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡è„ˆç†è§£å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ  - Phase 2-B-3
ä¼šè©±ã®æ–‡è„ˆã‚’ã‚ˆã‚Šæ·±ãç†è§£ã—ã€æš—é»™çš„ãªè¨€åŠã‚„ç¶™ç¶šæ€§ã‚’èªè­˜ã™ã‚‹
"""

import re
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from collections import deque
import os

class ContextUnderstandingSystem:
    """æ–‡è„ˆç†è§£ã¨å‚ç…§è§£æ±ºã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # Windowsç’°å¢ƒã¨WSL2ç’°å¢ƒä¸¡æ–¹ã«å¯¾å¿œ
        if os.name == 'nt':  # Windows
            self.context_file = Path("D:/setsuna_bot/data/conversation_context.json")
        else:  # Linux/WSL2
            self.context_file = Path("/mnt/d/setsuna_bot/data/conversation_context.json")
        
        # ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†
        self.conversation_memory = deque(maxlen=10)  # æœ€è¿‘10å›ã®ä¼šè©±ã‚’è¨˜æ†¶
        self.active_topics = {}  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªè©±é¡Œï¼ˆå‹•ç”»ID -> æƒ…å ±ï¼‰
        self.emotional_context = {}  # æ„Ÿæƒ…æ–‡è„ˆ
        self.reference_cache = {}  # ä»£åè©å‚ç…§ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        # è¨­å®š
        self.config = {
            "max_conversation_memory": 10,
            "context_timeout_minutes": 30,  # æ–‡è„ˆã®æœ‰åŠ¹æ™‚é–“
            "reference_confidence_threshold": 0.7,
            "enable_emotional_analysis": True,
            "enable_pronoun_resolution": True,
            "enable_topic_tracking": True
        }
        
        self._ensure_data_dir()
        self._load_context()
        
        print("[æ–‡è„ˆç†è§£] âœ… æ–‡è„ˆç†è§£å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _ensure_data_dir(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºä¿"""
        self.context_file.parent.mkdir(parents=True, exist_ok=True)
    
    def _load_context(self):
        """æ–‡è„ˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        try:
            if self.context_file.exists():
                with open(self.context_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # ä¼šè©±è¨˜æ†¶ã®å¾©å…ƒï¼ˆæœ€è¿‘ã®ã‚‚ã®ã®ã¿ï¼‰
                    saved_memory = data.get('conversation_memory', [])
                    recent_memory = [
                        mem for mem in saved_memory 
                        if self._is_recent_conversation(mem.get('timestamp'))
                    ]
                    self.conversation_memory.extend(recent_memory)
                    
                    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–è©±é¡Œã®å¾©å…ƒ
                    saved_topics = data.get('active_topics', {})
                    self.active_topics = {
                        topic_id: topic_data for topic_id, topic_data in saved_topics.items()
                        if self._is_recent_conversation(topic_data.get('last_mentioned'))
                    }
                    
                    # æ„Ÿæƒ…æ–‡è„ˆã®å¾©å…ƒ
                    self.emotional_context = data.get('emotional_context', {})
                    
                    print(f"[æ–‡è„ˆç†è§£] ğŸ“Š æ–‡è„ˆãƒ‡ãƒ¼ã‚¿å¾©å…ƒ: ä¼šè©±{len(self.conversation_memory)}ä»¶, è©±é¡Œ{len(self.active_topics)}ä»¶")
            else:
                print("[æ–‡è„ˆç†è§£] ğŸ“ æ–°è¦æ–‡è„ˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ")
                
        except Exception as e:
            print(f"[æ–‡è„ˆç†è§£] âš ï¸ æ–‡è„ˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            self._initialize_empty_context()
    
    def _initialize_empty_context(self):
        """ç©ºã®æ–‡è„ˆã§åˆæœŸåŒ–"""
        self.conversation_memory.clear()
        self.active_topics = {}
        self.emotional_context = {}
        self.reference_cache = {}
    
    def _save_context(self):
        """æ–‡è„ˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        try:
            data = {
                'conversation_memory': list(self.conversation_memory),
                'active_topics': self.active_topics,
                'emotional_context': self.emotional_context,
                'config': self.config,
                'last_updated': datetime.now().isoformat()
            }
            
            with open(self.context_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"[æ–‡è„ˆç†è§£] âŒ æ–‡è„ˆãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—: {e}")
    
    def _is_recent_conversation(self, timestamp_str: str) -> bool:
        """ä¼šè©±ãŒæœ€è¿‘ã®ã‚‚ã®ã‹ãƒã‚§ãƒƒã‚¯"""
        if not timestamp_str:
            return False
        
        try:
            timestamp = datetime.fromisoformat(timestamp_str)
            now = datetime.now()
            return now - timestamp < timedelta(minutes=self.config["context_timeout_minutes"])
        except:
            return False
    
    def analyze_input_context(self, user_input: str) -> Dict[str, Any]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®æ–‡è„ˆã‚’åˆ†æ
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
            
        Returns:
            æ–‡è„ˆåˆ†æçµæœ
        """
        analysis = {
            "input": user_input,
            "timestamp": datetime.now().isoformat(),
            "pronoun_references": [],
            "continuity_indicators": [],
            "emotional_signals": {},
            "topic_relevance": {},
            "requires_resolution": False
        }
        
        # ä»£åè©ãƒ»æŒ‡ç¤ºèªæ¤œå‡º
        if self.config["enable_pronoun_resolution"]:
            analysis["pronoun_references"] = self._detect_pronoun_references(user_input)
            analysis["requires_resolution"] = len(analysis["pronoun_references"]) > 0
        
        # ç¶™ç¶šæ€§æŒ‡æ¨™ã®æ¤œå‡º
        analysis["continuity_indicators"] = self._detect_continuity_indicators(user_input)
        
        # æ„Ÿæƒ…ã‚·ã‚°ãƒŠãƒ«ã®åˆ†æ
        if self.config["enable_emotional_analysis"]:
            analysis["emotional_signals"] = self._analyze_emotional_signals(user_input)
        
        # è©±é¡Œé–¢é€£æ€§ã®åˆ†æ
        if self.config["enable_topic_tracking"]:
            analysis["topic_relevance"] = self._analyze_topic_relevance(user_input)
        
        return analysis
    
    def _detect_pronoun_references(self, user_input: str) -> List[Dict[str, Any]]:
        """ä»£åè©ãƒ»æŒ‡ç¤ºèªã®æ¤œå‡º"""
        pronouns = []
        
        # ä»£åè©ãƒ‘ã‚¿ãƒ¼ãƒ³
        pronoun_patterns = {
            "demonstrative": {  # æŒ‡ç¤ºèª
                "patterns": [r"(ã‚ã®|ãã®|ã“ã®)(æ›²|å‹•ç”»|æ¥½æ›²|æ­Œ|éŸ³æ¥½)", r"(ãã‚Œ|ã‚ã‚Œ|ã“ã‚Œ)", r"(ã“ã®|ãã®|ã‚ã®)"],
                "confidence": 0.9
            },
            "temporal": {  # æ™‚é–“çš„æŒ‡ç¤º
                "patterns": [r"(ã•ã£ã|å…ˆã»ã©|å‰|ä»Š|æœ€è¿‘)(ã®|ã«|èã„ãŸ|è¦‹ãŸ|è©±ã—ãŸ)", r"(ã•ã£ã|å…ˆã»ã©|å‰)", r"(ã•ã£ãã®|å‰ã®)"],
                "confidence": 0.8
            },
            "relative": {  # ç›¸å¯¾çš„æŒ‡ç¤º
                "patterns": [r"(åŒã˜|ä¼¼ãŸ|åˆ¥ã®|é•ã†)(ã‚„ã¤|ã‚‚ã®|æ›²|å‹•ç”»)", r"(ã‚‚ã†ä¸€åº¦|ã¾ãŸ|ç¹°ã‚Šè¿”ã—)", r"(ä¼¼ãŸã‚ˆã†ãª|åŒã˜ã‚ˆã†ãª)"],
                "confidence": 0.7
            },
            "implicit": {  # æš—é»™çš„æŒ‡ç¤º
                "patterns": [r"(è©³ã—ã|ã‚‚ã£ã¨|ä»–ã«|ç¶šã)", r"(ã©ã†|ãªãœ|ã„ã¤|ã©ã“)", r"(ã‚‚ã£ã¨|ã•ã‚‰ã«)"],
                "confidence": 0.6
            }
        }
        
        for category, info in pronoun_patterns.items():
            for pattern in info["patterns"]:
                matches = re.finditer(pattern, user_input, re.IGNORECASE)
                for match in matches:
                    pronouns.append({
                        "type": category,
                        "text": match.group(),
                        "position": match.span(),
                        "confidence": info["confidence"]
                    })
        
        return pronouns
    
    def _detect_continuity_indicators(self, user_input: str) -> List[Dict[str, Any]]:
        """ä¼šè©±ã®ç¶™ç¶šæ€§æŒ‡æ¨™ã‚’æ¤œå‡º"""
        indicators = []
        
        # ç¶™ç¶šæ€§ãƒ‘ã‚¿ãƒ¼ãƒ³
        continuity_patterns = {
            "agreement": {  # åŒæ„ãƒ»è‚¯å®š
                "patterns": [r"(ãã†ã ã­|ãã†|ã†ã‚“|ã¯ã„|ã„ã„ã­|å¥½ã|æ°—ã«å…¥)", r"(ã‚ã‚ŠãŒã¨ã†|ã‚µãƒ³ã‚­ãƒ¥ãƒ¼)"],
                "strength": "strong"
            },
            "disagreement": {  # å¦å®šãƒ»åå¯¾
                "patterns": [r"(é•ã†|ã¡ãŒã†|ãã†ã˜ã‚ƒãªã„|å«Œ|å¾®å¦™|ã‚¤ãƒã‚¤ãƒ)", r"(ã§ã‚‚|ã‘ã©|ã—ã‹ã—)"],
                "strength": "strong"
            },
            "continuation": {  # ç¶™ç¶š
                "patterns": [r"(ãã‚Œã§|ãã—ã¦|ã¾ãŸ|æ¬¡ã«|ä»Šåº¦)", r"(ã‚‚ã£ã¨|ã•ã‚‰ã«|ä»–ã«|åˆ¥ã®)"],
                "strength": "medium"
            },
            "clarification": {  # æ˜ç¢ºåŒ–
                "patterns": [r"(ã¤ã¾ã‚Š|è¦ã™ã‚‹ã«|ã¨ã„ã†ã“ã¨ã¯)", r"(ã©ã†ã„ã†|ã©ã‚“ãª|ãªãœ|ãªã«)"],
                "strength": "medium"
            },
            "transition": {  # è©±é¡Œè»¢æ›
                "patterns": [r"(ã¨ã“ã‚ã§|ãã†ã„ãˆã°|è©±ã¯å¤‰ã‚ã£ã¦)", r"(åˆ¥ã®è©±|æ–°ã—ã„|ä»Šåº¦ã¯)"],
                "strength": "weak"
            }
        }
        
        for category, info in continuity_patterns.items():
            for pattern in info["patterns"]:
                matches = re.finditer(pattern, user_input, re.IGNORECASE)
                for match in matches:
                    indicators.append({
                        "type": category,
                        "text": match.group(),
                        "position": match.span(),
                        "strength": info["strength"]
                    })
        
        return indicators
    
    def _analyze_emotional_signals(self, user_input: str) -> Dict[str, Any]:
        """æ„Ÿæƒ…ã‚·ã‚°ãƒŠãƒ«ã®åˆ†æ"""
        emotional_signals = {
            "positive": 0.0,
            "negative": 0.0,
            "excitement": 0.0,
            "curiosity": 0.0,
            "satisfaction": 0.0,
            "detected_emotions": []
        }
        
        # æ„Ÿæƒ…ãƒ‘ã‚¿ãƒ¼ãƒ³
        emotion_patterns = {
            "positive": {
                "patterns": [r"(ã„ã„|è‰¯ã„|å¥½ã|ç´ æ™´ã‚‰ã—ã„|æœ€é«˜|ã™ã”ã„|ãã‚Œã„|ç¾ã—ã„)", r"(ã‚ã‚ŠãŒã¨ã†|æ„Ÿè¬|å¬‰ã—ã„|æ¥½ã—ã„)"],
                "weight": 1.0
            },
            "negative": {
                "patterns": [r"(å«Œ|ãƒ€ãƒ¡|æ‚ªã„|å¾®å¦™|ã‚¤ãƒã‚¤ãƒ|ã¤ã¾ã‚‰ãªã„)", r"(æ®‹å¿µ|ãŒã£ã‹ã‚Š|æ‚²ã—ã„|å›°ã£ãŸ)"],
                "weight": 1.0
            },
            "excitement": {
                "patterns": [r"(ï¼|!!|ã‚ãƒ¼|ã‚„ã£ãŸ|ã™ã’ãƒ¼)", r"(èˆˆå¥®|ãƒ†ãƒ³ã‚·ãƒ§ãƒ³|ç››ã‚Šä¸ŠãŒã‚‹)"],
                "weight": 0.8
            },
            "curiosity": {
                "patterns": [r"(ï¼Ÿ|\?|ãªãœ|ã©ã†ã—ã¦|ã©ã‚“ãª|æ°—ã«ãªã‚‹)", r"(çŸ¥ã‚ŠãŸã„|æ•™ãˆã¦|èããŸã„)"],
                "weight": 0.7
            },
            "satisfaction": {
                "patterns": [r"(æº€è¶³|ç´å¾—|ç†è§£|ã‚ã‹ã£ãŸ|ãã†ã‹)", r"(å®Œç’§|ååˆ†|ä¸åº¦ã„ã„)"],
                "weight": 0.6
            }
        }
        
        for emotion, info in emotion_patterns.items():
            score = 0
            for pattern in info["patterns"]:
                matches = re.findall(pattern, user_input, re.IGNORECASE)
                score += len(matches) * info["weight"]
            
            if score > 0:
                emotional_signals[emotion] = min(1.0, score)
                emotional_signals["detected_emotions"].append({
                    "emotion": emotion,
                    "strength": score
                })
        
        return emotional_signals
    
    def _analyze_topic_relevance(self, user_input: str) -> Dict[str, Any]:
        """è©±é¡Œé–¢é€£æ€§ã®åˆ†æ"""
        topic_relevance = {}
        
        # ç¾åœ¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªè©±é¡Œã¨ã®é–¢é€£æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        for topic_id, topic_data in self.active_topics.items():
            relevance_score = self._calculate_topic_relevance_score(user_input, topic_data)
            if relevance_score > 0.3:  # é–¾å€¤ä»¥ä¸Šã®é–¢é€£æ€§
                topic_relevance[topic_id] = {
                    "relevance_score": relevance_score,
                    "topic_data": topic_data,
                    "related_keywords": self._extract_related_keywords(user_input, topic_data)
                }
        
        return topic_relevance
    
    def _calculate_topic_relevance_score(self, user_input: str, topic_data: Dict[str, Any]) -> float:
        """è©±é¡Œã¨ã®é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0.0
        user_lower = user_input.lower()
        
        # ä»£åè©ãƒ»æŒ‡ç¤ºèªãŒã‚ã‚‹å ´åˆã¯åŸºæœ¬ã‚¹ã‚³ã‚¢ã‚’ä¸ãˆã‚‹
        pronoun_keywords = ["ã“ã®", "ãã®", "ã‚ã®", "ã•ã£ã", "å‰", "ä¼¼ãŸ", "åŒã˜"]
        has_pronoun = any(keyword in user_lower for keyword in pronoun_keywords)
        
        if has_pronoun:
            score += 0.5  # åŸºæœ¬çš„ãªä»£åè©é–¢é€£æ€§
        
        # ã‚¿ã‚¤ãƒˆãƒ«é–¢é€£æ€§
        title = topic_data.get("title", "").lower()
        if title:
            # å®Œå…¨ä¸€è‡´
            if title in user_lower:
                score += 1.0
            else:
                # éƒ¨åˆ†ä¸€è‡´
                title_words = title.split()
                for word in title_words:
                    if len(word) >= 2 and word in user_lower:
                        score += 0.3
        
        # ãƒãƒ£ãƒ³ãƒãƒ«é–¢é€£æ€§
        channel = topic_data.get("channel", "").lower()
        if channel and channel in user_lower:
            score += 0.5
        
        # ã‚¸ãƒ£ãƒ³ãƒ«é–¢é€£æ€§
        genre = topic_data.get("genre", "").lower()
        if genre and genre in user_lower:
            score += 0.4
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–¢é€£æ€§
        keywords = topic_data.get("keywords", [])
        for keyword in keywords:
            if keyword.lower() in user_lower:
                score += 0.2
        
        # æ›²ãƒ»å‹•ç”»é–¢é€£ã®ä¸€èˆ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        music_keywords = ["æ›²", "æ­Œ", "å‹•ç”»", "æ¥½æ›²", "éŸ³æ¥½"]
        if any(keyword in user_lower for keyword in music_keywords):
            score += 0.3
        
        return min(1.0, score)
    
    def _extract_related_keywords(self, user_input: str, topic_data: Dict[str, Any]) -> List[str]:
        """é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        related_keywords = []
        user_lower = user_input.lower()
        
        # å„ç¨®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        for key in ["title", "channel", "genre"]:
            value = topic_data.get(key, "")
            if value and value.lower() in user_lower:
                related_keywords.append(value)
        
        # å­˜åœ¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        for keyword in topic_data.get("keywords", []):
            if keyword.lower() in user_lower:
                related_keywords.append(keyword)
        
        return related_keywords
    
    def resolve_references(self, context_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»£åè©ãƒ»æŒ‡ç¤ºèªã®å‚ç…§è§£æ±º
        
        Args:
            context_analysis: æ–‡è„ˆåˆ†æçµæœ
            
        Returns:
            å‚ç…§è§£æ±ºçµæœ
        """
        resolution_result = {
            "resolved_references": [],
            "suggested_topics": [],
            "context_suggestions": [],
            "confidence": 0.0
        }
        
        if not context_analysis.get("requires_resolution"):
            return resolution_result
        
        # ä»£åè©ã”ã¨ã«è§£æ±ºã‚’è©¦è¡Œ
        for pronoun in context_analysis.get("pronoun_references", []):
            resolved = self._resolve_single_pronoun(pronoun, context_analysis)
            if resolved:
                resolution_result["resolved_references"].append(resolved)
        
        # è©±é¡Œé–¢é€£æ€§ã«åŸºã¥ãææ¡ˆ
        for topic_id, relevance_data in context_analysis.get("topic_relevance", {}).items():
            if relevance_data["relevance_score"] > self.config["reference_confidence_threshold"]:
                resolution_result["suggested_topics"].append({
                    "topic_id": topic_id,
                    "topic_data": relevance_data["topic_data"],
                    "confidence": relevance_data["relevance_score"]
                })
        
        # å…¨ä½“ã®ä¿¡é ¼åº¦è¨ˆç®—
        if resolution_result["resolved_references"] or resolution_result["suggested_topics"]:
            total_confidence = 0.0
            count = 0
            
            for ref in resolution_result["resolved_references"]:
                total_confidence += ref.get("confidence", 0.0)
                count += 1
            
            for topic in resolution_result["suggested_topics"]:
                total_confidence += topic.get("confidence", 0.0)
                count += 1
            
            if count > 0:
                resolution_result["confidence"] = total_confidence / count
        
        return resolution_result
    
    def _resolve_single_pronoun(self, pronoun: Dict[str, Any], context_analysis: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å˜ä¸€ã®ä»£åè©ã‚’è§£æ±º"""
        pronoun_type = pronoun.get("type")
        pronoun_text = pronoun.get("text", "").lower()
        
        # æ™‚é–“çš„æŒ‡ç¤ºèªã®è§£æ±º
        if pronoun_type == "temporal":
            return self._resolve_temporal_reference(pronoun, context_analysis)
        
        # æŒ‡ç¤ºèªã®è§£æ±º
        elif pronoun_type == "demonstrative":
            return self._resolve_demonstrative_reference(pronoun, context_analysis)
        
        # ç›¸å¯¾çš„æŒ‡ç¤ºã®è§£æ±º
        elif pronoun_type == "relative":
            return self._resolve_relative_reference(pronoun, context_analysis)
        
        return None
    
    def _resolve_temporal_reference(self, pronoun: Dict[str, Any], context_analysis: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æ™‚é–“çš„æŒ‡ç¤ºèªã®è§£æ±º"""
        if len(self.conversation_memory) == 0:
            return None
        
        # æœ€è¿‘ã®ä¼šè©±ã‹ã‚‰å€™è£œã‚’æŠ½å‡º
        recent_conversations = list(self.conversation_memory)[-3:]  # æœ€è¿‘3ä»¶
        
        for conv in reversed(recent_conversations):  # æ–°ã—ã„é †
            if conv.get("mentioned_videos"):
                for video_info in conv["mentioned_videos"]:
                    return {
                        "pronoun": pronoun,
                        "resolved_topic": video_info,
                        "confidence": 0.8,
                        "resolution_type": "temporal",
                        "source_conversation": conv["timestamp"]
                    }
        
        return None
    
    def _resolve_demonstrative_reference(self, pronoun: Dict[str, Any], context_analysis: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æŒ‡ç¤ºèªã®è§£æ±º"""
        # æœ€ã‚‚ã‚¹ã‚³ã‚¢ã®é«˜ã„è©±é¡Œã‚’è¿”ã™
        topic_relevance = context_analysis.get("topic_relevance", {})
        
        if topic_relevance:
            best_topic_id = max(topic_relevance.keys(), 
                              key=lambda tid: topic_relevance[tid]["relevance_score"])
            best_topic = topic_relevance[best_topic_id]
            
            return {
                "pronoun": pronoun,
                "resolved_topic": best_topic["topic_data"],
                "confidence": best_topic["relevance_score"],
                "resolution_type": "demonstrative",
                "related_keywords": best_topic["related_keywords"]
            }
        
        return None
    
    def _resolve_relative_reference(self, pronoun: Dict[str, Any], context_analysis: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ç›¸å¯¾çš„æŒ‡ç¤ºã®è§£æ±º"""
        pronoun_text = pronoun.get("text", "").lower()
        
        # ã€Œä¼¼ãŸã€ã€ŒåŒã˜ã€ç­‰ã¯æœ€è¿‘ã®è©±é¡Œã‹ã‚‰é¡ä¼¼ã‚’æ¤œç´¢
        if any(keyword in pronoun_text for keyword in ["ä¼¼ãŸ", "åŒã˜", "é¡ä¼¼"]):
            if self.active_topics:
                # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªè©±é¡Œã®ä¸­ã‹ã‚‰æœ€æ–°ã®ã‚‚ã®ã‚’é¸æŠ
                latest_topic_id = max(self.active_topics.keys(), 
                                    key=lambda tid: self.active_topics[tid].get("last_mentioned", ""))
                
                return {
                    "pronoun": pronoun,
                    "resolved_topic": self.active_topics[latest_topic_id],
                    "confidence": 0.7,
                    "resolution_type": "relative_similarity",
                    "similarity_request": True
                }
        
        return None
    
    def update_conversation_memory(self, user_input: str, context_analysis: Dict[str, Any], 
                                 mentioned_videos: List[Dict[str, Any]] = None):
        """
        ä¼šè©±è¨˜æ†¶ã‚’æ›´æ–°
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
            context_analysis: æ–‡è„ˆåˆ†æçµæœ
            mentioned_videos: è¨€åŠã•ã‚ŒãŸå‹•ç”»ã®ãƒªã‚¹ãƒˆ
        """
        conversation_entry = {
            "timestamp": datetime.now().isoformat(),
            "user_input": user_input,
            "context_analysis": context_analysis,
            "mentioned_videos": mentioned_videos or [],
            "emotional_state": context_analysis.get("emotional_signals", {}),
            "continuity_indicators": context_analysis.get("continuity_indicators", [])
        }
        
        self.conversation_memory.append(conversation_entry)
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–è©±é¡Œã®æ›´æ–°
        if mentioned_videos:
            for video_info in mentioned_videos:
                video_id = video_info.get("video_id")
                if video_id:
                    self.active_topics[video_id] = {
                        "video_id": video_id,
                        "title": video_info.get("title", ""),
                        "channel": video_info.get("channel", ""),
                        "genre": video_info.get("genre", ""),
                        "keywords": self._extract_keywords_from_video(video_info),
                        "last_mentioned": datetime.now().isoformat(),
                        "mention_count": self.active_topics.get(video_id, {}).get("mention_count", 0) + 1
                    }
        
        # å¤ã„è©±é¡Œã®å‰Šé™¤
        self._cleanup_old_topics()
        
        # è‡ªå‹•ä¿å­˜
        self._save_context()
        
        print(f"[æ–‡è„ˆç†è§£] ğŸ“ ä¼šè©±è¨˜æ†¶æ›´æ–°: {len(self.conversation_memory)}ä»¶, ã‚¢ã‚¯ãƒ†ã‚£ãƒ–è©±é¡Œ: {len(self.active_topics)}ä»¶")
    
    def _extract_keywords_from_video(self, video_info: Dict[str, Any]) -> List[str]:
        """å‹•ç”»æƒ…å ±ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = []
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é‡è¦èªå¥ã‚’æŠ½å‡º
        title = video_info.get("title", "")
        if title:
            # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã²ã‚‰ãŒãª3æ–‡å­—ä»¥ä¸Šï¼‰
            title_keywords = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]{3,}|[ã-ã‚–ä¸€-é¾¯]{3,}', title)
            keywords.extend(title_keywords)
        
        # ãƒãƒ£ãƒ³ãƒãƒ«å
        channel = video_info.get("channel", "")
        if channel:
            keywords.append(channel)
        
        # ã‚¸ãƒ£ãƒ³ãƒ«
        genre = video_info.get("genre", "")
        if genre:
            keywords.append(genre)
        
        return list(set(keywords))  # é‡è¤‡é™¤å»
    
    def _cleanup_old_topics(self):
        """å¤ã„è©±é¡Œã®å‰Šé™¤"""
        current_time = datetime.now()
        timeout_delta = timedelta(minutes=self.config["context_timeout_minutes"])
        
        topics_to_remove = []
        for topic_id, topic_data in self.active_topics.items():
            last_mentioned_str = topic_data.get("last_mentioned")
            if last_mentioned_str:
                try:
                    last_mentioned = datetime.fromisoformat(last_mentioned_str)
                    if current_time - last_mentioned > timeout_delta:
                        topics_to_remove.append(topic_id)
                except:
                    topics_to_remove.append(topic_id)
        
        for topic_id in topics_to_remove:
            del self.active_topics[topic_id]
    
    def get_context_summary(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®æ–‡è„ˆçŠ¶æ³ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        return {
            "conversation_count": len(self.conversation_memory),
            "active_topics_count": len(self.active_topics),
            "active_topics": list(self.active_topics.keys()),
            "recent_emotions": self._get_recent_emotional_trends(),
            "context_quality": self._assess_context_quality()
        }
    
    def _get_recent_emotional_trends(self) -> Dict[str, float]:
        """æœ€è¿‘ã®æ„Ÿæƒ…å‚¾å‘ã‚’å–å¾—"""
        if not self.conversation_memory:
            return {}
        
        # æœ€è¿‘3ä»¶ã®ä¼šè©±ã®æ„Ÿæƒ…ã‚’é›†è¨ˆ
        recent_conversations = list(self.conversation_memory)[-3:]
        emotion_totals = {}
        
        for conv in recent_conversations:
            emotional_signals = conv.get("emotional_state", {})
            for emotion, value in emotional_signals.items():
                if emotion != "detected_emotions" and isinstance(value, (int, float)):
                    emotion_totals[emotion] = emotion_totals.get(emotion, 0) + value
        
        # å¹³å‡åŒ–
        for emotion in emotion_totals:
            emotion_totals[emotion] /= len(recent_conversations)
        
        return emotion_totals
    
    def _assess_context_quality(self) -> Dict[str, Any]:
        """æ–‡è„ˆå“è³ªã®è©•ä¾¡"""
        quality_metrics = {
            "completeness": 0.0,  # æ–‡è„ˆã®å®Œå…¨æ€§
            "continuity": 0.0,    # ç¶™ç¶šæ€§
            "relevance": 0.0,     # é–¢é€£æ€§
            "overall": 0.0
        }
        
        # å®Œå…¨æ€§: ä¼šè©±è¨˜æ†¶ã®å……å®Ÿåº¦
        if len(self.conversation_memory) >= 3:
            quality_metrics["completeness"] = min(1.0, len(self.conversation_memory) / 5)
        
        # ç¶™ç¶šæ€§: ç¶™ç¶šæ€§æŒ‡æ¨™ã®å­˜åœ¨
        if self.conversation_memory:
            recent_conv = list(self.conversation_memory)[-1]
            continuity_indicators = recent_conv.get("continuity_indicators", [])
            quality_metrics["continuity"] = min(1.0, len(continuity_indicators) * 0.3)
        
        # é–¢é€£æ€§: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–è©±é¡Œã®å­˜åœ¨
        quality_metrics["relevance"] = min(1.0, len(self.active_topics) * 0.5)
        
        # ç·åˆè©•ä¾¡
        quality_metrics["overall"] = (
            quality_metrics["completeness"] * 0.4 +
            quality_metrics["continuity"] * 0.3 +
            quality_metrics["relevance"] * 0.3
        )
        
        return quality_metrics


# ä½¿ç”¨ä¾‹ãƒ»ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    print("=== æ–‡è„ˆç†è§£å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ ===")
    
    context_system = ContextUnderstandingSystem()
    
    # ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª
    test_conversations = [
        "ã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼ã«ã¤ã„ã¦æ•™ãˆã¦",
        "ã“ã®æ›²ã„ã„ã­ï¼",
        "ã‚‚ã£ã¨ä¼¼ãŸã‚ˆã†ãªæ›²ã‚ã‚‹ï¼Ÿ",
        "ã‚ã®å‹•ç”»ã®ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã¯èª°ï¼Ÿ"
    ]
    
    print("\nğŸ“ æ–‡è„ˆç†è§£ãƒ†ã‚¹ãƒˆ:")
    for i, conversation in enumerate(test_conversations):
        print(f"\nä¼šè©± {i+1}: '{conversation}'")
        
        # æ–‡è„ˆåˆ†æ
        analysis = context_system.analyze_input_context(conversation)
        print(f"  ä»£åè©æ¤œå‡º: {len(analysis['pronoun_references'])}ä»¶")
        print(f"  ç¶™ç¶šæ€§æŒ‡æ¨™: {len(analysis['continuity_indicators'])}ä»¶")
        print(f"  æ„Ÿæƒ…ã‚·ã‚°ãƒŠãƒ«: {analysis['emotional_signals']['detected_emotions']}")
        
        # å‚ç…§è§£æ±º
        if analysis["requires_resolution"]:
            resolution = context_system.resolve_references(analysis)
            print(f"  å‚ç…§è§£æ±º: ä¿¡é ¼åº¦ {resolution['confidence']:.2f}")
        
        # ä¼šè©±è¨˜æ†¶æ›´æ–°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼å‹•ç”»æƒ…å ±ï¼‰
        if i == 0:  # æœ€åˆã®ä¼šè©±ã§å‹•ç”»ã‚’è¨­å®š
            test_video = {
                "video_id": "test_adventure",
                "title": "ã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼",
                "channel": "ãƒ†ã‚¹ãƒˆãƒãƒ£ãƒ³ãƒãƒ«",
                "genre": "ãƒãƒƒãƒ—ã‚¹"
            }
            context_system.update_conversation_memory(conversation, analysis, [test_video])
        else:
            context_system.update_conversation_memory(conversation, analysis)
    
    # æœ€çµ‚ã‚µãƒãƒªãƒ¼
    summary = context_system.get_context_summary()
    print(f"\nğŸ“Š æ–‡è„ˆã‚µãƒãƒªãƒ¼:")
    print(f"  ä¼šè©±æ•°: {summary['conversation_count']}")
    print(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–è©±é¡Œ: {summary['active_topics_count']}ä»¶")
    print(f"  æ–‡è„ˆå“è³ª: {summary['context_quality']['overall']:.2f}")