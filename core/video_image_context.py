#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‹•ç”»-ç”»åƒçµ±åˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼
å‹•ç”»æƒ…å ±ã¨ç”»åƒåˆ†æçµæœã‚’çµ±åˆã—ã¦ã€ä¼šè©±ç”¨ã®æ–‡è„ˆã‚’æ§‹ç¯‰
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
from collections import defaultdict, Counter


class VideoImageContextBuilder:
    """å‹•ç”»ã¨ç”»åƒã®çµ±åˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, youtube_manager=None):
        """
        åˆæœŸåŒ–
        
        Args:
            youtube_manager: YouTubeKnowledgeManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.youtube_manager = youtube_manager
        
        # ä¼šè©±ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
        self.conversation_templates = self._load_conversation_templates()
        
        # ã‚·ãƒ¼ãƒ³åˆ†é¡è¨­å®š
        self.scene_classifications = self._load_scene_classifications()
        
        # è¦–è¦šè¦ç´ ã‚«ãƒ†ã‚´ãƒª
        self.visual_element_categories = self._load_visual_categories()
        
        print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âœ… VideoImageContextBuilderåˆæœŸåŒ–å®Œäº†")
    
    def _load_conversation_templates(self) -> Dict[str, str]:
        """ä¼šè©±ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        return {
            "general_video_discussion": """
{artist}ã®ã€Œ{title}ã€ã«ã¤ã„ã¦è©±ã—ã¾ã—ã‚‡ã†ã€‚

ã€å‹•ç”»ã®å°è±¡ã€‘
{video_description}

ã€ç”»åƒã‹ã‚‰åˆ†ã‹ã‚‹ã“ã¨ã€‘
{image_summaries}

ã€è©±ã—ãŸã„ãƒˆãƒ”ãƒƒã‚¯ã€‘
{discussion_topics}

ã“ã®å‹•ç”»ã®é­…åŠ›ã«ã¤ã„ã¦ã€ã©ã‚“ãªã“ã¨ãŒæ°—ã«ãªã‚Šã¾ã™ã‹ï¼Ÿ
""",

            "specific_image_focus": """
ã“ã®ç”»åƒã«ã¤ã„ã¦è©³ã—ãè¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

ã€ç”»åƒã®å†…å®¹ã€‘
{image_description}

ã€å‹•ç”»å…¨ä½“ã§ã®ä½ç½®ã¥ã‘ã€‘
{image_context_in_video}

ã€æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆã€‘
{key_visual_elements}

ã©ã‚“ãªã¨ã“ã‚ãŒå°è±¡çš„ã§ã—ãŸã‹ï¼Ÿ
""",

            "visual_analysis": """
ã€Œ{title}ã€ã®æ˜ åƒè¡¨ç¾ã«ã¤ã„ã¦åˆ†æã—ã¦ã¿ã¾ã—ãŸã€‚

ã€è¦–è¦šçš„ãªç‰¹å¾´ã€‘
{visual_characteristics}

ã€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æµã‚Œã€‘
{narrative_flow}

ã€ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªè¦ç´ ã€‘
{artistic_elements}

æ˜ åƒã®æ¼”å‡ºã«ã¤ã„ã¦ã©ã†æ€ã„ã¾ã™ã‹ï¼Ÿ
""",

            "music_video_comprehensive": """
{artist}ã®ã€Œ{title}ã€ã®é­…åŠ›ã‚’ç”»åƒã‹ã‚‰åˆ†æã—ã¦ã¿ã¾ã—ãŸï¼

ã€æ¥½æ›²ã®é›°å›²æ°—ã€‘
{video_mood}

ã€æ˜ åƒã®è¦‹ã©ã“ã‚ã€‘
{visual_highlights}

ã€æ¼”å‡ºã®ãƒã‚¤ãƒ³ãƒˆã€‘
{production_points}

ã€è©±ã—åˆã„ãŸã„ã“ã¨ã€‘
{conversation_topics}

ã©ã®ã‚·ãƒ¼ãƒ³ãŒä¸€ç•ªå°è±¡ã«æ®‹ã‚Šã¾ã—ãŸã‹ï¼Ÿ
""",

            "simple_image_chat": """
{image_description}

{context_summary}

ã“ã®ç”»åƒã«ã¤ã„ã¦ã€ä½•ã‹èããŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
"""
        }
    
    def _load_scene_classifications(self) -> Dict[str, List[str]]:
        """ã‚·ãƒ¼ãƒ³åˆ†é¡ã‚«ãƒ†ã‚´ãƒªã‚’èª­ã¿è¾¼ã¿"""
        return {
            "performance_scenes": [
                "ãƒ©ã‚¤ãƒ–æ¼”å¥", "ã‚¹ã‚¿ã‚¸ã‚ªéŒ²éŸ³", "ã‚½ãƒ­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹", 
                "æ¥½å™¨æ¼”å¥", "æ­Œå”±ã‚·ãƒ¼ãƒ³", "ãƒ€ãƒ³ã‚¹"
            ],
            "narrative_scenes": [
                "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å±•é–‹", "ãƒ‰ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯ã‚·ãƒ¼ãƒ³", "å›æƒ³ã‚·ãƒ¼ãƒ³",
                "æ—¥å¸¸é¢¨æ™¯", "æ„Ÿæƒ…è¡¨ç¾", "å¯¾è©±ã‚·ãƒ¼ãƒ³"
            ],
            "artistic_scenes": [
                "æŠ½è±¡çš„æ˜ åƒ", "ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯è¡¨ç¾", "è‰²å½©åŠ¹æœ",
                "ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°åŠ¹æœ", "ç‰¹æ®ŠåŠ¹æœ", "ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³"
            ],
            "environmental_scenes": [
                "è‡ªç„¶é¢¨æ™¯", "éƒ½å¸‚é¢¨æ™¯", "å®¤å†…ç©ºé–“", "ã‚¹ã‚¿ã‚¸ã‚ª",
                "ãƒ©ã‚¤ãƒ–ä¼šå ´", "ç‰¹åˆ¥ãªå ´æ‰€"
            ]
        }
    
    def _load_visual_categories(self) -> Dict[str, List[str]]:
        """è¦–è¦šè¦ç´ ã‚«ãƒ†ã‚´ãƒªã‚’èª­ã¿è¾¼ã¿"""
        return {
            "musical_elements": [
                "æ¥½å™¨", "ãƒã‚¤ã‚¯", "ã‚¢ãƒ³ãƒ—", "ãƒ˜ãƒƒãƒ‰ãƒ›ãƒ³", "æ¥½è­œ",
                "ãƒ¬ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ©Ÿå™¨", "ãƒ”ã‚¢ãƒ", "ã‚®ã‚¿ãƒ¼", "ãƒ‰ãƒ©ãƒ "
            ],
            "human_elements": [
                "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ", "ãƒãƒ³ãƒ‰ãƒ¡ãƒ³ãƒãƒ¼", "è¦³å®¢", "ã‚¹ã‚¿ãƒƒãƒ•",
                "ãƒ€ãƒ³ã‚µãƒ¼", "ã‚³ãƒ¼ãƒ©ã‚¹", "å­ä¾›", "å¤§äºº"
            ],
            "technical_elements": [
                "ç…§æ˜", "ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯", "ã‚¨ãƒ•ã‚§ã‚¯ãƒˆ", "ç·¨é›†æŠ€æ³•",
                "è‰²èª¿æ•´", "æ§‹å›³", "ãƒ•ãƒ¬ãƒ¼ãƒŸãƒ³ã‚°", "å‹•ã"
            ],
            "emotional_elements": [
                "è¡¨æƒ…", "ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼", "é›°å›²æ°—", "æ„Ÿæƒ…è¡¨ç¾",
                "ã‚¨ãƒãƒ«ã‚®ãƒ¼", "è¦ªå¯†æ„Ÿ", "åŠ›å¼·ã•", "å„ªã—ã•"
            ],
            "environmental_elements": [
                "èƒŒæ™¯", "ã‚»ãƒƒãƒˆ", "å°é“å…·", "è¡£è£…", "ãƒ¡ã‚¤ã‚¯",
                "è£…é£¾", "å»ºç¯‰", "è‡ªç„¶", "å¤©å€™"
            ]
        }
    
    def build_comprehensive_context(self, video_id: str) -> Dict[str, Any]:
        """
        å‹•ç”»+å…¨ç”»åƒã®åŒ…æ‹¬çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            video_id: YouTubeå‹•ç”»ID
            
        Returns:
            åŒ…æ‹¬çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¾æ›¸
        """
        try:
            if not self.youtube_manager:
                return {"error": "YouTubeKnowledgeManagerãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"}
            
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ” åŒ…æ‹¬çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰é–‹å§‹: {video_id}")
            
            # Step 1: åŸºæœ¬ãƒ‡ãƒ¼ã‚¿åé›†
            video_data = self._collect_video_data(video_id)
            if not video_data:
                return {"error": "å‹•ç”»ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
            
            # Step 2: ç”»åƒåˆ†æãƒ‡ãƒ¼ã‚¿çµ±åˆ
            images_analysis = self._integrate_image_analyses(video_data['images_data'])
            
            # Step 3: è¦–è¦šçš„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ§‹ç¯‰
            visual_narrative = self._build_visual_narrative(images_analysis)
            
            # Step 4: ä¼šè©±ãƒˆãƒ”ãƒƒã‚¯ç”Ÿæˆ
            conversation_topics = self._generate_discussion_topics(video_data, images_analysis)
            
            # Step 5: ç·åˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            comprehensive_context = {
                "video_id": video_id,
                "video_info": video_data['video_metadata'],
                "images_analysis": images_analysis,
                "visual_narrative": visual_narrative,
                "conversation_topics": conversation_topics,
                "context_summary": self._generate_context_summary(
                    video_data['video_metadata'], 
                    images_analysis, 
                    visual_narrative
                ),
                "generation_timestamp": datetime.now().isoformat()
            }
            
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âœ… åŒ…æ‹¬çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰å®Œäº†")
            return comprehensive_context
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ åŒ…æ‹¬çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def build_image_specific_context(self, video_id: str, image_id: str) -> Dict[str, Any]:
        """
        ç‰¹å®šç”»åƒãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            video_id: YouTubeå‹•ç”»ID
            image_id: å¯¾è±¡ç”»åƒID
            
        Returns:
            ç”»åƒç‰¹åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            if not self.youtube_manager:
                return {"error": "YouTubeKnowledgeManagerãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"}
            
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ¯ ç”»åƒç‰¹åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰: {image_id}")
            
            # å‹•ç”»æƒ…å ±å–å¾—
            video_data = self._collect_video_data(video_id)
            if not video_data:
                return {"error": "å‹•ç”»ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
            
            # å¯¾è±¡ç”»åƒã®åˆ†æçµæœå–å¾—
            target_image = None
            for img_data in video_data['images_data']:
                if img_data.get('image_id') == image_id:
                    target_image = img_data
                    break
            
            if not target_image:
                return {"error": f"ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_id}"}
            
            # ç”»åƒåˆ†æçµæœå–å¾—
            analysis_result = self.youtube_manager.get_image_analysis_result(video_id, image_id)
            if not analysis_result:
                return {"error": "ç”»åƒåˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
            
            # ä»–ã®ç”»åƒã¨ã®é–¢ä¿‚æ€§åˆ†æ
            other_images = [img for img in video_data['images_data'] if img.get('image_id') != image_id]
            relationships = self._analyze_single_image_relationships(target_image, other_images)
            
            # ç”»åƒç‰¹åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            image_context = {
                "video_id": video_id,
                "image_id": image_id,
                "video_info": video_data['video_metadata'],
                "image_analysis": analysis_result,
                "image_metadata": target_image,
                "relationships": relationships,
                "context_in_video": self._determine_image_context_in_video(
                    target_image, video_data['video_metadata']
                ),
                "key_discussion_points": self._extract_image_discussion_points(analysis_result),
                "generation_timestamp": datetime.now().isoformat()
            }
            
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âœ… ç”»åƒç‰¹åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰å®Œäº†")
            return image_context
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ç”»åƒç‰¹åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def create_conversation_context(self, video_id: str, query: str = None, template_type: str = "general_video_discussion") -> str:
        """
        ä¼šè©±ç”¨ã®æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            video_id: YouTubeå‹•ç”»ID
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            template_type: ä½¿ç”¨ã™ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—
            
        Returns:
            ä¼šè©±ç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—
        """
        try:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ’¬ ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: {template_type}")
            
            # åŒ…æ‹¬çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
            comprehensive_context = self.build_comprehensive_context(video_id)
            if "error" in comprehensive_context:
                return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å‹•ç”»ã®æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚({comprehensive_context['error']})"
            
            # ã‚¯ã‚¨ãƒªæ„å›³è§£æ
            query_intent = self._analyze_query_intent(query) if query else "general"
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠ
            if query_intent != "general":
                template_type = self._select_template_by_intent(query_intent)
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
            conversation_text = self._apply_conversation_template(
                comprehensive_context, 
                template_type,
                query
            )
            
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âœ… ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
            return conversation_text
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚({str(e)})"
    
    def _analyze_query_intent(self, query: str) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªã®æ„å›³ã‚’åˆ†æ"""
        try:
            if not query:
                return "general"
            
            query_lower = query.lower()
            
            # æ„å›³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒãƒƒãƒ”ãƒ³ã‚°
            intent_patterns = {
                "image_focus": [
                    "ã“ã®ç”»åƒ", "ç”»åƒã«ã¤ã„ã¦", "å†™çœŸã«ã¤ã„ã¦", "ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ",
                    "ã‚·ãƒ¼ãƒ³ã«ã¤ã„ã¦", "æ˜ åƒã«ã¤ã„ã¦", "è¦‹ãˆã‚‹", "å†™ã£ã¦ã„ã‚‹"
                ],
                "visual_analysis": [
                    "æ˜ åƒè¡¨ç¾", "æ¼”å‡º", "æ’®å½±", "ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯", "æ§‹å›³", "è‰²å½©",
                    "ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°", "ã‚¨ãƒ•ã‚§ã‚¯ãƒˆ", "è¦–è¦šçš„", "ã‚¢ãƒ¼ãƒˆ"
                ],
                "music_video_comprehensive": [
                    "æ¥½æ›²", "éŸ³æ¥½", "æ­Œè©", "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ", "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                    "æ­Œã£ã¦", "æ¼”å¥", "ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯", "ãƒ“ãƒ‡ã‚ª"
                ],
                "mood_atmosphere": [
                    "é›°å›²æ°—", "ãƒ ãƒ¼ãƒ‰", "æ„Ÿã˜", "å°è±¡", "æ°—æŒã¡", "æ„Ÿæƒ…",
                    "æš–ã‹ã„", "å†·ãŸã„", "æ˜ã‚‹ã„", "æš—ã„", "å„ªã—ã„"
                ]
            }
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            for intent, patterns in intent_patterns.items():
                if any(pattern in query_lower for pattern in patterns):
                    return intent
            
            return "general"
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ã‚¯ã‚¨ãƒªæ„å›³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return "general"
    
    def _select_template_by_intent(self, intent: str) -> str:
        """æ„å›³ã«åŸºã¥ã„ã¦ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠ"""
        try:
            # æ„å›³ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒãƒƒãƒ”ãƒ³ã‚°
            intent_template_mapping = {
                "image_focus": "specific_image_focus",
                "visual_analysis": "visual_analysis",
                "music_video_comprehensive": "music_video_comprehensive",
                "mood_atmosphere": "visual_analysis",
                "general": "general_video_discussion"
            }
            
            return intent_template_mapping.get(intent, "general_video_discussion")
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            return "general_video_discussion"
    
    def _apply_conversation_template(self, comprehensive_context: Dict, template_type: str, query: str = None) -> str:
        """ä¼šè©±ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        try:
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—
            template = self.conversation_templates.get(template_type, self.conversation_templates["general_video_discussion"])
            
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            video_info = comprehensive_context.get('video_info', {})
            images_analysis = comprehensive_context.get('images_analysis', {})
            visual_narrative = comprehensive_context.get('visual_narrative', {})
            conversation_topics = comprehensive_context.get('conversation_topics', [])
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°ã®æ§‹ç¯‰
            template_vars = {
                # åŸºæœ¬æƒ…å ±
                'title': video_info.get('title', 'æ¥½æ›²'),
                'artist': video_info.get('channel_title', 'ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ'),
                'video_description': video_info.get('description', 'é­…åŠ›çš„ãªæ˜ åƒä½œå“')[:100],
                
                # ç”»åƒåˆ†æçµæœ
                'image_summaries': self._format_image_summaries(images_analysis),
                'visual_characteristics': self._format_visual_characteristics(images_analysis),
                'discussion_topics': self._format_discussion_topics(conversation_topics),
                
                # è¦–è¦šçš„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼
                'narrative_flow': visual_narrative.get('story_flow', 'æ˜ åƒå±•é–‹'),
                'artistic_elements': self._format_artistic_elements(images_analysis),
                
                # ç·åˆæƒ…å ±
                'context_summary': comprehensive_context.get('context_summary', ''),
                'video_mood': self._extract_overall_mood(images_analysis),
                'visual_highlights': self._format_visual_highlights(images_analysis),
                'production_points': self._format_production_points(images_analysis),
                'conversation_topics': self._format_conversation_topics_detailed(conversation_topics)
            }
            
            # ç‰¹å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”¨ã®è¿½åŠ å¤‰æ•°
            if template_type == "specific_image_focus":
                template_vars.update({
                    'image_description': self._get_first_image_description(images_analysis),
                    'image_context_in_video': self._get_image_context_in_video(images_analysis, video_info),
                    'key_visual_elements': self._get_key_visual_elements(images_analysis)
                })
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
            conversation_text = template.format(**template_vars)
            
            # ã‚¯ã‚¨ãƒªé–¢é€£ã®è£œè¶³æƒ…å ±è¿½åŠ 
            if query and template_type != "simple_image_chat":
                conversation_text += f"\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘\n{query}\n\nã“ã®ç‚¹ã«ã¤ã„ã¦ã€ã©ã®ã‚ˆã†ã«æ„Ÿã˜ã‚‰ã‚Œã¾ã—ãŸã‹ï¼Ÿ"
            
            return conversation_text
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return f"ã€Œ{video_info.get('title', 'æ¥½æ›²')}ã€ã«ã¤ã„ã¦è©±ã—ã¾ã—ã‚‡ã†ã€‚ã©ã‚“ãªã“ã¨ãŒæ°—ã«ãªã‚Šã¾ã™ã‹ï¼Ÿ"
    
    def _format_image_summaries(self, images_analysis: Dict) -> str:
        """ç”»åƒã‚µãƒãƒªãƒ¼ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        try:
            summaries = images_analysis.get('image_summaries', [])
            if not summaries:
                return "ç”»åƒã®è©³ç´°æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€æ˜ åƒã®é­…åŠ›ã«ã¤ã„ã¦è©±ã—ã¾ã—ã‚‡ã†ã€‚"
            
            formatted_summaries = []
            for i, summary in enumerate(summaries[:3], 1):  # æœ€å¤§3ã¤ã¾ã§
                desc = summary.get('description', 'ç”»åƒã®å†…å®¹')
                formatted_summaries.append(f"â€¢ {desc}")
            
            return "\n".join(formatted_summaries)
            
        except Exception:
            return "æ˜ åƒã‹ã‚‰èª­ã¿å–ã‚Œã‚‹å°è±¡çš„ãªè¦ç´ ã«ã¤ã„ã¦è©±ã—ã¾ã—ã‚‡ã†ã€‚"
    
    def _format_visual_characteristics(self, images_analysis: Dict) -> str:
        """è¦–è¦šçš„ç‰¹å¾´ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        try:
            themes = images_analysis.get('visual_themes', [])
            common_elements = images_analysis.get('common_elements', {})
            
            characteristics = []
            
            # ä¸»è¦ãƒ†ãƒ¼ãƒ
            if themes:
                themes_text = "ã€".join(themes[:3])
                characteristics.append(f"ä¸»è¦ãƒ†ãƒ¼ãƒ: {themes_text}")
            
            # å…±é€šè¦ç´ 
            if common_elements:
                top_elements = list(common_elements.keys())[:3]
                if top_elements:
                    elements_text = "ã€".join(top_elements)
                    characteristics.append(f"ç‰¹å¾´çš„ãªè¦ç´ : {elements_text}")
            
            # ãƒ ãƒ¼ãƒ‰é€²è¡Œ
            mood_progression = images_analysis.get('mood_progression', [])
            if mood_progression:
                unique_moods = list(set(mood_progression))
                if len(unique_moods) == 1:
                    characteristics.append(f"ä¸€è²«ã—ãŸ{unique_moods[0]}ãªé›°å›²æ°—")
                else:
                    characteristics.append(f"å¤šæ§˜ãªé›°å›²æ°—ã®å¤‰åŒ–({len(unique_moods)}ãƒ‘ã‚¿ãƒ¼ãƒ³)")
            
            return "\nâ€¢ ".join(characteristics) if characteristics else "å¤šå½©ãªè¦–è¦šè¡¨ç¾"
            
        except Exception:
            return "å°è±¡çš„ãªè¦–è¦šçš„ç‰¹å¾´"
    
    def _format_discussion_topics(self, topics: List[str]) -> str:
        """è­°è«–ãƒˆãƒ”ãƒƒã‚¯ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        try:
            if not topics:
                return "â€¢ æ˜ åƒã®å°è±¡ã«ã¤ã„ã¦\nâ€¢ å¥½ããªã‚·ãƒ¼ãƒ³ã«ã¤ã„ã¦\nâ€¢ éŸ³æ¥½ã¨æ˜ åƒã®é–¢ä¿‚ã«ã¤ã„ã¦"
            
            formatted_topics = []
            for topic in topics[:4]:  # æœ€å¤§4ã¤ã¾ã§
                formatted_topics.append(f"â€¢ {topic}")
            
            return "\n".join(formatted_topics)
            
        except Exception:
            return "â€¢ æ˜ åƒã«ã¤ã„ã¦\nâ€¢ æ¥½æ›²ã«ã¤ã„ã¦"
    
    def _format_artistic_elements(self, images_analysis: Dict) -> str:
        """ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒ†ã‚£ãƒƒã‚¯è¦ç´ ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        try:
            scene_types = images_analysis.get('scene_types', [])
            if not scene_types:
                return "å‰µé€ çš„ãªæ˜ åƒè¡¨ç¾"
            
            unique_scenes = list(set(scene_types))
            if len(unique_scenes) == 1:
                return f"{unique_scenes[0]}ã‚’ä¸­å¿ƒã¨ã—ãŸæ¼”å‡º"
            else:
                return f"å¤šæ§˜ãªæ¼”å‡ºæŠ€æ³•({len(unique_scenes)}ç¨®é¡ã®ã‚·ãƒ¼ãƒ³æ§‹æˆ)"
                
        except Exception:
            return "å¤šå½©ãªæ¼”å‡ºæŠ€æ³•"
    
    def _extract_overall_mood(self, images_analysis: Dict) -> str:
        """å…¨ä½“çš„ãªãƒ ãƒ¼ãƒ‰ã®æŠ½å‡º"""
        try:
            mood_progression = images_analysis.get('mood_progression', [])
            if not mood_progression:
                return "é­…åŠ›çš„ãªé›°å›²æ°—"
            
            # æœ€ã‚‚é »å‡ºã™ã‚‹ãƒ ãƒ¼ãƒ‰ã‚’ç‰¹å®š
            from collections import Counter
            mood_counter = Counter(mood_progression)
            if mood_counter:
                dominant_mood = mood_counter.most_common(1)[0][0]
                return f"{dominant_mood}ã§å°è±¡çš„ãªé›°å›²æ°—"
            
            return "å¤šå½©ãªé›°å›²æ°—"
            
        except Exception:
            return "å°è±¡çš„ãªé›°å›²æ°—"
    
    def _format_visual_highlights(self, images_analysis: Dict) -> str:
        """è¦–è¦šçš„ãƒã‚¤ãƒ©ã‚¤ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        try:
            common_elements = images_analysis.get('common_elements', {})
            if not common_elements:
                return "å°è±¡çš„ãªæ˜ åƒè¡¨ç¾"
            
            highlights = []
            for element, count in list(common_elements.items())[:3]:
                if count > 1:
                    highlights.append(f"{element}ã®åŠ¹æœçš„ãªä½¿ç”¨")
                else:
                    highlights.append(element)
            
            return "\nâ€¢ ".join(highlights) if highlights else "å¤šå½©ãªè¦–è¦šè¡¨ç¾"
            
        except Exception:
            return "å°è±¡çš„ãªæ˜ åƒè¡¨ç¾"
    
    def _format_production_points(self, images_analysis: Dict) -> str:
        """åˆ¶ä½œãƒã‚¤ãƒ³ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        try:
            scene_types = images_analysis.get('scene_types', [])
            visual_themes = images_analysis.get('visual_themes', [])
            
            points = []
            
            # ã‚·ãƒ¼ãƒ³æ§‹æˆ
            if scene_types:
                unique_scenes = list(set(scene_types))
                if len(unique_scenes) > 1:
                    points.append(f"å¤šæ§˜ãªã‚·ãƒ¼ãƒ³æ§‹æˆ({len(unique_scenes)}ç¨®é¡)")
                else:
                    points.append(f"{unique_scenes[0]}ä¸­å¿ƒã®æ§‹æˆ")
            
            # è¦–è¦šãƒ†ãƒ¼ãƒ
            if visual_themes:
                themes_text = "ã€".join(visual_themes[:2])
                points.append(f"{themes_text}ã‚’æ´»ã‹ã—ãŸæ¼”å‡º")
            
            # ç·åˆçš„ãªåˆ¶ä½œã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
            total_images = images_analysis.get('total_images', 0)
            if total_images > 0:
                points.append(f"è©³ç´°ãªæ˜ åƒæ§‹æˆ({total_images}æšã®ç”»åƒè§£æ)")
            
            return "\nâ€¢ ".join(points) if points else "ä¸å¯§ãªæ˜ åƒåˆ¶ä½œ"
            
        except Exception:
            return "å°è±¡çš„ãªåˆ¶ä½œæŠ€æ³•"
    
    def _format_conversation_topics_detailed(self, topics: List[str]) -> str:
        """è©³ç´°ãªä¼šè©±ãƒˆãƒ”ãƒƒã‚¯ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        try:
            if not topics:
                return "â€¢ æ˜ åƒã®å°è±¡ã«ã¤ã„ã¦\nâ€¢ å¥½ããªã‚·ãƒ¼ãƒ³ã«ã¤ã„ã¦\nâ€¢ éŸ³æ¥½ã¨æ˜ åƒã®é–¢ä¿‚ã«ã¤ã„ã¦"
            
            formatted_topics = []
            for i, topic in enumerate(topics[:5], 1):
                formatted_topics.append(f"{i}. {topic}")
            
            return "\n".join(formatted_topics)
            
        except Exception:
            return "â€¢ æ˜ åƒã«ã¤ã„ã¦\nâ€¢ æ¥½æ›²ã«ã¤ã„ã¦"
    
    def _get_first_image_description(self, images_analysis: Dict) -> str:
        """æœ€åˆã®ç”»åƒã®èª¬æ˜ã‚’å–å¾—"""
        try:
            summaries = images_analysis.get('image_summaries', [])
            if summaries:
                return summaries[0].get('description', 'ç”»åƒã®å†…å®¹')
            return "ç”»åƒã®å†…å®¹"
            
        except Exception:
            return "ç”»åƒã®å†…å®¹"
    
    def _get_image_context_in_video(self, images_analysis: Dict, video_info: Dict) -> str:
        """å‹•ç”»å†…ã§ã®ç”»åƒã®æ–‡è„ˆã‚’å–å¾—"""
        try:
            title = video_info.get('title', 'æ¥½æ›²')
            total_images = images_analysis.get('total_images', 0)
            
            if total_images > 1:
                return f"ã€Œ{title}ã€ã®æ˜ åƒã®ä¸­ã§ã€ç‰¹ã«å°è±¡çš„ãªã‚·ãƒ¼ãƒ³ã®ä¸€ã¤"
            else:
                return f"ã€Œ{title}ã€ã®ä»£è¡¨çš„ãªã‚·ãƒ¼ãƒ³"
                
        except Exception:
            return "å‹•ç”»ã®å°è±¡çš„ãªä¸€å ´é¢"
    
    def _get_key_visual_elements(self, images_analysis: Dict) -> str:
        """ã‚­ãƒ¼ã¨ãªã‚‹è¦–è¦šè¦ç´ ã‚’å–å¾—"""
        try:
            common_elements = images_analysis.get('common_elements', {})
            if common_elements:
                top_elements = list(common_elements.keys())[:3]
                return "\nâ€¢ ".join(top_elements)
            
            return "å°è±¡çš„ãªè¦–è¦šè¡¨ç¾"
            
        except Exception:
            return "é­…åŠ›çš„ãªè¦–è¦šè¦ç´ "
    
    def _collect_video_data(self, video_id: str) -> Optional[Dict[str, Any]]:
        """å‹•ç”»ã®åŸºæœ¬æƒ…å ±åé›†"""
        try:
            # å‹•ç”»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
            video_metadata = self.youtube_manager.get_video_context(video_id)
            if not video_metadata:
                return None
            
            # ç”»åƒãƒ‡ãƒ¼ã‚¿å–å¾—
            images_data = self.youtube_manager.get_video_images(video_id)
            
            return {
                'video_metadata': video_metadata,
                'images_data': images_data
            }
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ å‹•ç”»ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _integrate_image_analyses(self, images_data: List[Dict]) -> Dict[str, Any]:
        """è¤‡æ•°ç”»åƒã®åˆ†æçµæœã‚’çµ±åˆ"""
        try:
            if not images_data:
                return {
                    "total_images": 0,
                    "analyzed_images": 0,
                    "image_summaries": [],
                    "common_elements": {},
                    "mood_progression": [],
                    "visual_themes": []
                }
            
            integrated = {
                "total_images": len(images_data),
                "analyzed_images": 0,
                "image_summaries": [],
                "common_elements": defaultdict(int),
                "mood_progression": [],
                "visual_themes": [],
                "scene_types": []
            }
            
            for image_data in images_data:
                # åˆ†æçµæœç¢ºèª
                if image_data.get('analysis_status') != 'completed':
                    continue
                
                analysis_result = self.youtube_manager.get_image_analysis_result(
                    image_data.get('video_id', ''), 
                    image_data.get('image_id', '')
                )
                
                if not analysis_result:
                    continue
                
                integrated["analyzed_images"] += 1
                
                # ç”»åƒã‚µãƒãƒªãƒ¼ä½œæˆ
                summary = self._create_image_summary(image_data, analysis_result)
                integrated["image_summaries"].append(summary)
                
                # å…±é€šè¦ç´ æŠ½å‡º
                elements = self._extract_visual_elements_from_analysis(analysis_result)
                for element in elements:
                    integrated["common_elements"][element] += 1
                
                # ãƒ ãƒ¼ãƒ‰é€²è¡Œ
                mood = self._extract_mood_from_analysis(analysis_result)
                if mood:
                    integrated["mood_progression"].append(mood)
                
                # ã‚·ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—
                scene_type = self._classify_scene_type_from_analysis(analysis_result)
                if scene_type:
                    integrated["scene_types"].append(scene_type)
            
            # å…±é€šè¦ç´ ã‚’é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ
            integrated["common_elements"] = dict(
                sorted(integrated["common_elements"].items(), 
                       key=lambda x: x[1], reverse=True)
            )
            
            # è¦–è¦šãƒ†ãƒ¼ãƒæŠ½å‡º
            integrated["visual_themes"] = self._extract_visual_themes(integrated)
            
            return integrated
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ç”»åƒåˆ†æçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _create_image_summary(self, image_data: Dict, analysis_result: Dict) -> Dict[str, Any]:
        """å€‹åˆ¥ç”»åƒã®ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        try:
            description = analysis_result.get('description', '')
            
            return {
                "image_id": image_data.get('image_id'),
                "description": description[:200] + "..." if len(description) > 200 else description,
                "upload_timestamp": image_data.get('upload_timestamp'),
                "user_description": image_data.get('user_description', ''),
                "analysis_confidence": "high"  # ä»Šå¾Œæ”¹å–„äºˆå®š
            }
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ç”»åƒã‚µãƒãƒªãƒ¼ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"image_id": image_data.get('image_id'), "error": str(e)}
    
    def _extract_visual_elements_from_analysis(self, analysis_result: Dict) -> List[str]:
        """åˆ†æçµæœã‹ã‚‰è¦–è¦šè¦ç´ ã‚’æŠ½å‡º"""
        elements = []
        description = analysis_result.get('description', '').lower()
        
        # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰è¦ç´ ã‚’æ¤œç´¢
        for category, keywords in self.visual_element_categories.items():
            for keyword in keywords:
                if keyword.lower() in description:
                    elements.append(keyword)
        
        return list(set(elements))  # é‡è¤‡é™¤å»
    
    def _extract_mood_from_analysis(self, analysis_result: Dict) -> Optional[str]:
        """åˆ†æçµæœã‹ã‚‰ãƒ ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        description = analysis_result.get('description', '').lower()
        
        # ãƒ ãƒ¼ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        mood_keywords = {
            "æ˜ã‚‹ã„": ["æ˜ã‚‹ã„", "æ¥½ã—ã„", "å…ƒæ°—", "ãƒã‚¸ãƒ†ã‚£ãƒ–", "è¯ã‚„ã‹"],
            "è½ã¡ç€ã„ãŸ": ["è½ã¡ç€ã„ãŸ", "é™ã‹", "ç©ã‚„ã‹", "ãƒªãƒ©ãƒƒã‚¯ã‚¹", "å®‰ã‚‰ã‹"],
            "æƒ…ç†±çš„": ["æƒ…ç†±çš„", "ç†±ã„", "æ¿€ã—ã„", "ã‚¨ãƒãƒ«ã‚®ãƒƒã‚·ãƒ¥", "åŠ›å¼·ã„"],
            "ãƒ­ãƒãƒ³ãƒãƒƒã‚¯": ["ãƒ­ãƒãƒ³ãƒãƒƒã‚¯", "ç”˜ã„", "å„ªã—ã„", "æ¸©ã‹ã„", "æ„›ã‚‰ã—ã„"],
            "ãƒ‰ãƒ©ãƒãƒãƒƒã‚¯": ["ãƒ‰ãƒ©ãƒãƒãƒƒã‚¯", "åŠ‡çš„", "æ„Ÿå‹•çš„", "å°è±¡çš„", "å¼·çƒˆ"]
        }
        
        for mood, keywords in mood_keywords.items():
            if any(keyword in description for keyword in keywords):
                return mood
        
        return None
    
    def _classify_scene_type_from_analysis(self, analysis_result: Dict) -> Optional[str]:
        """åˆ†æçµæœã‹ã‚‰ã‚·ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        description = analysis_result.get('description', '').lower()
        
        for scene_category, scene_types in self.scene_classifications.items():
            for scene_type in scene_types:
                if scene_type.lower() in description:
                    return scene_type
        
        return None
    
    def _extract_visual_themes(self, integrated_data: Dict) -> List[str]:
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¦–è¦šãƒ†ãƒ¼ãƒã‚’æŠ½å‡º"""
        themes = []
        
        # å…±é€šè¦ç´ ã‹ã‚‰ä¸»è¦ãƒ†ãƒ¼ãƒã‚’ç‰¹å®š
        common_elements = integrated_data.get('common_elements', {})
        if len(common_elements) > 0:
            # æœ€ã‚‚é »å‡ºã™ã‚‹è¦ç´ ã‚’ãƒ†ãƒ¼ãƒã¨ã—ã¦æ¡ç”¨
            top_elements = list(common_elements.keys())[:3]
            themes.extend(top_elements)
        
        # ãƒ ãƒ¼ãƒ‰é€²è¡Œã‹ã‚‰ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
        mood_progression = integrated_data.get('mood_progression', [])
        if mood_progression:
            mood_counter = Counter(mood_progression)
            dominant_mood = mood_counter.most_common(1)[0][0] if mood_counter else None
            if dominant_mood:
                themes.append(f"{dominant_mood}ãªé›°å›²æ°—")
        
        # ã‚·ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã‹ã‚‰ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
        scene_types = integrated_data.get('scene_types', [])
        if scene_types:
            scene_counter = Counter(scene_types)
            dominant_scene = scene_counter.most_common(1)[0][0] if scene_counter else None
            if dominant_scene:
                themes.append(f"{dominant_scene}ä¸­å¿ƒ")
        
        return list(set(themes))  # é‡è¤‡é™¤å»
    
    def _build_visual_narrative(self, images_analysis: Dict) -> Dict[str, Any]:
        """ç”»åƒã‹ã‚‰è¦–è¦šçš„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ§‹ç¯‰"""
        try:
            image_summaries = images_analysis.get('image_summaries', [])
            if not image_summaries:
                return {"story_flow": "ç”»åƒãŒä¸è¶³ã—ã¦ãŠã‚Šã€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“"}
            
            # ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ•ãƒ­ãƒ¼æ¨å®š
            story_flow = self._estimate_story_flow(image_summaries)
            
            # ç¹°ã‚Šè¿”ã—ãƒ†ãƒ¼ãƒ
            recurring_themes = images_analysis.get('visual_themes', [])
            
            # è¦–è¦šçš„é€²è¡Œ
            visual_progression = self._analyze_visual_progression(images_analysis)
            
            # ç‰©èªã‚¢ãƒ¼ã‚¯
            narrative_arc = self._generate_narrative_arc(story_flow, recurring_themes)
            
            return {
                "story_flow": story_flow,
                "recurring_themes": recurring_themes,
                "visual_progression": visual_progression,
                "narrative_arc": narrative_arc
            }
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ è¦–è¦šçš„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _estimate_story_flow(self, image_summaries: List[Dict]) -> str:
        """ç”»åƒã‹ã‚‰ç‰©èªã®æµã‚Œã‚’æ¨å®š"""
        try:
            if len(image_summaries) == 1:
                return "å˜ä¸€ã‚·ãƒ¼ãƒ³ãƒ•ã‚©ãƒ¼ã‚«ã‚¹"
            elif len(image_summaries) == 2:
                return "äºŒéƒ¨æ§‹æˆï¼ˆå°å…¥â†’å±•é–‹ï¼‰"
            elif len(image_summaries) >= 3:
                return "ä¸‰å¹•æ§‹æˆï¼ˆå°å…¥â†’å±•é–‹â†’ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹ï¼‰"
            else:
                return "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ§‹é€ ä¸æ˜"
                
        except Exception:
            return "ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ•ãƒ­ãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼"
    
    def _analyze_visual_progression(self, images_analysis: Dict) -> str:
        """è¦–è¦šçš„é€²è¡Œã®åˆ†æ"""
        try:
            mood_progression = images_analysis.get('mood_progression', [])
            
            if not mood_progression:
                return "é€²è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ä¸æ˜"
            
            if len(mood_progression) == 1:
                return f"ä¸€è²«ã—ãŸ{mood_progression[0]}ãªé›°å›²æ°—"
            
            # ãƒ ãƒ¼ãƒ‰ã®å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
            if mood_progression[0] != mood_progression[-1]:
                return f"{mood_progression[0]}ã‹ã‚‰{mood_progression[-1]}ã¸ã®å¤‰åŒ–"
            else:
                return f"å…¨ä½“çš„ã«{mood_progression[0]}ãªé›°å›²æ°—ã‚’ç¶­æŒ"
                
        except Exception:
            return "è¦–è¦šçš„é€²è¡Œåˆ†æã‚¨ãƒ©ãƒ¼"
    
    def _generate_narrative_arc(self, story_flow: str, themes: List[str]) -> str:
        """ç‰©èªã‚¢ãƒ¼ã‚¯ã®ç”Ÿæˆ"""
        try:
            if not themes:
                return f"{story_flow}ã®æ§‹é€ ã§å±•é–‹"
            
            themes_text = "ã€".join(themes[:2])  # ä¸Šä½2ã¤ã®ãƒ†ãƒ¼ãƒã‚’ä½¿ç”¨
            return f"{story_flow}ã®æ§‹é€ ã§ã€{themes_text}ã‚’ä¸­å¿ƒã¨ã—ãŸç‰©èªå±•é–‹"
            
        except Exception:
            return "ç‰©èªã‚¢ãƒ¼ã‚¯ç”Ÿæˆã‚¨ãƒ©ãƒ¼"
    
    def _generate_discussion_topics(self, video_data: Dict, images_analysis: Dict) -> List[str]:
        """ç”»åƒå†…å®¹ã‹ã‚‰ä¼šè©±ãƒˆãƒ”ãƒƒã‚¯ã‚’è‡ªå‹•ç”Ÿæˆ"""
        try:
            topics = []
            
            # å‹•ç”»æƒ…å ±ã‹ã‚‰åŸºæœ¬ãƒˆãƒ”ãƒƒã‚¯
            video_metadata = video_data.get('video_metadata', {})
            if video_metadata:
                topics.append(f"ã€Œ{video_metadata.get('title', 'æ¥½æ›²')}ã€ã®é­…åŠ›ã«ã¤ã„ã¦")
            
            # å…±é€šè¦ç´ ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ç”Ÿæˆ
            common_elements = images_analysis.get('common_elements', {})
            for element, count in list(common_elements.items())[:3]:
                if count > 1:  # è¤‡æ•°ã®ç”»åƒã«ç™»å ´ã™ã‚‹è¦ç´ 
                    topics.append(f"{element}ã®ä½¿ã„æ–¹ã‚„åŠ¹æœã«ã¤ã„ã¦")
            
            # ãƒ ãƒ¼ãƒ‰ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ç”Ÿæˆ
            mood_progression = images_analysis.get('mood_progression', [])
            if mood_progression:
                unique_moods = list(set(mood_progression))
                if len(unique_moods) > 1:
                    topics.append("æ˜ åƒã®é›°å›²æ°—ã®å¤‰åŒ–ã«ã¤ã„ã¦")
                else:
                    topics.append(f"{unique_moods[0]}ãªé›°å›²æ°—ã®æ¼”å‡ºã«ã¤ã„ã¦")
            
            # ã‚·ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ç”Ÿæˆ
            scene_types = images_analysis.get('scene_types', [])
            if scene_types:
                unique_scenes = list(set(scene_types))
                for scene in unique_scenes[:2]:  # ä¸Šä½2ã¤
                    topics.append(f"{scene}ã®è¡¨ç¾æŠ€æ³•ã«ã¤ã„ã¦")
            
            # æœ€å°é™ã®ãƒˆãƒ”ãƒƒã‚¯ä¿è¨¼
            if not topics:
                topics = [
                    "æ˜ åƒã®å°è±¡ã«ã¤ã„ã¦",
                    "å¥½ããªã‚·ãƒ¼ãƒ³ã«ã¤ã„ã¦",
                    "éŸ³æ¥½ã¨æ˜ åƒã®é–¢ä¿‚ã«ã¤ã„ã¦"
                ]
            
            return topics[:5]  # æœ€å¤§5ã¤ã¾ã§
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ è­°è«–ãƒˆãƒ”ãƒƒã‚¯ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return ["æ˜ åƒã«ã¤ã„ã¦", "æ¥½æ›²ã«ã¤ã„ã¦"]
    
    def _generate_context_summary(self, video_metadata: Dict, images_analysis: Dict, visual_narrative: Dict) -> str:
        """ç·åˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        try:
            title = video_metadata.get('title', 'æ¥½æ›²')
            artist = video_metadata.get('channel_title', 'ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ')
            
            # ç”»åƒæ•°ã¨åˆ†æçŠ¶æ³
            total_images = images_analysis.get('total_images', 0)
            analyzed_images = images_analysis.get('analyzed_images', 0)
            
            # ä¸»è¦ãƒ†ãƒ¼ãƒ
            themes = images_analysis.get('visual_themes', [])
            themes_text = "ã€".join(themes[:2]) if themes else "å¤šæ§˜ãªè¡¨ç¾"
            
            # ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ§‹é€ 
            story_flow = visual_narrative.get('story_flow', 'æ˜ åƒå±•é–‹')
            
            # ã‚µãƒãƒªãƒ¼æ§‹ç¯‰
            summary = f"{artist}ã®ã€Œ{title}ã€ã¯ã€{analyzed_images}æšã®ç”»åƒã‹ã‚‰åˆ†æã™ã‚‹ã¨ã€{themes_text}ã‚’ç‰¹å¾´ã¨ã—ãŸæ˜ åƒä½œå“ã§ã™ã€‚"
            
            if story_flow:
                summary += f" {story_flow}ã§æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€"
            
            # è¦–è¦šçš„é€²è¡Œ
            visual_progression = visual_narrative.get('visual_progression', '')
            if visual_progression:
                summary += f"{visual_progression}ãŒå°è±¡çš„ã§ã™ã€‚"
            else:
                summary += "é­…åŠ›çš„ãªæ˜ åƒè¡¨ç¾ãŒå±•é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚"
            
            return summary
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return "ã“ã®å‹•ç”»ã®æ˜ åƒã«ã¤ã„ã¦åˆ†æã—ãŸå†…å®¹ã‚’ã‚‚ã¨ã«ä¼šè©±ã—ã¾ã—ã‚‡ã†ã€‚"
    
    def _analyze_single_image_relationships(self, target_image: Dict, other_images: List[Dict]) -> Dict[str, Any]:
        """å˜ä¸€ç”»åƒã¨ä»–ç”»åƒã¨ã®é–¢ä¿‚æ€§åˆ†æ"""
        try:
            if not other_images:
                return {"relationship_type": "single_image", "connections": []}
            
            relationships = {
                "total_other_images": len(other_images),
                "temporal_position": self._estimate_temporal_position(target_image, other_images),
                "thematic_connections": [],
                "visual_similarities": []
            }
            
            # æ™‚é–“çš„ä½ç½®ã®æ¨å®šï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚åˆ»åŸºæº–ï¼‰
            target_time = target_image.get('upload_timestamp', '')
            if target_time:
                earlier_count = 0
                later_count = 0
                
                for other_img in other_images:
                    other_time = other_img.get('upload_timestamp', '')
                    if other_time:
                        if other_time < target_time:
                            earlier_count += 1
                        else:
                            later_count += 1
                
                if earlier_count == 0:
                    relationships["sequence_position"] = "æœ€åˆ"
                elif later_count == 0:
                    relationships["sequence_position"] = "æœ€å¾Œ"
                else:
                    relationships["sequence_position"] = "ä¸­é–“"
            
            return relationships
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ç”»åƒé–¢ä¿‚æ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _estimate_temporal_position(self, target_image: Dict, other_images: List[Dict]) -> str:
        """ç”»åƒã®æ™‚é–“çš„ä½ç½®ã‚’æ¨å®š"""
        # ç°¡æ˜“å®Ÿè£…ï¼šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é †åºã‹ã‚‰æ¨å®š
        try:
            target_desc = target_image.get('user_description', '').lower()
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ¨å®š
            if any(keyword in target_desc for keyword in ['æœ€åˆ', 'å†’é ­', 'ã‚¤ãƒ³ãƒˆãƒ­', 'å§‹ã¾ã‚Š']):
                return "opening"
            elif any(keyword in target_desc for keyword in ['æœ€å¾Œ', 'çµ‚ã‚ã‚Š', 'ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°', 'ç· ã‚']):
                return "closing" 
            elif any(keyword in target_desc for keyword in ['ã‚µãƒ“', 'ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹', 'ç››ã‚Šä¸ŠãŒã‚Š']):
                return "climax"
            else:
                return "middle"
                
        except Exception:
            return "unknown"
    
    def _determine_image_context_in_video(self, target_image: Dict, video_metadata: Dict) -> str:
        """å‹•ç”»å…¨ä½“ã«ãŠã‘ã‚‹ç”»åƒã®æ–‡è„ˆã‚’æ±ºå®š"""
        try:
            title = video_metadata.get('title', 'æ¥½æ›²')
            user_desc = target_image.get('user_description', '')
            
            if user_desc:
                return f"ã€Œ{title}ã€ã®{user_desc}ã¨ã—ã¦ä½ç½®ã¥ã‘ã‚‰ã‚Œã‚‹ã‚·ãƒ¼ãƒ³"
            else:
                return f"ã€Œ{title}ã€ã®ä¸€å ´é¢"
                
        except Exception:
            return "å‹•ç”»ã®ä¸€éƒ¨ã¨ã—ã¦å«ã¾ã‚Œã‚‹ã‚·ãƒ¼ãƒ³"
    
    def _extract_image_discussion_points(self, analysis_result: Dict) -> List[str]:
        """ç”»åƒåˆ†æçµæœã‹ã‚‰è­°è«–ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º"""
        try:
            points = []
            description = analysis_result.get('description', '')
            
            # é•·ã„èª¬æ˜ã‹ã‚‰é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º
            if len(description) > 100:
                # æ–‡ã‚’åˆ†å‰²ã—ã¦é‡è¦ãã†ãªæ–‡ã‚’æŠ½å‡º
                sentences = description.split('ã€‚')
                for sentence in sentences[:3]:  # æœ€åˆã®3æ–‡
                    if len(sentence.strip()) > 10:
                        points.append(sentence.strip() + "ã€‚")
            else:
                points.append(description)
            
            # æœ€å°é™ã®ãƒã‚¤ãƒ³ãƒˆä¿è¨¼
            if not points:
                points = ["ã“ã®ç”»åƒã®å†…å®¹ã«ã¤ã„ã¦", "å°è±¡çš„ãªè¦ç´ ã«ã¤ã„ã¦"]
            
            return points[:3]  # æœ€å¤§3ã¤ã¾ã§
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ è­°è«–ãƒã‚¤ãƒ³ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return ["ã“ã®ç”»åƒã«ã¤ã„ã¦"]
    
    def analyze_advanced_image_relationships(self, video_id: str) -> Dict[str, Any]:
        """
        é«˜åº¦ãªç”»åƒé–¢é€£æ€§åˆ†æ
        ç”»åƒé–“ã®æ™‚é–“çš„ãƒ»ç©ºé–“çš„ãƒ»ãƒ†ãƒ¼ãƒçš„é–¢ä¿‚æ€§ã‚’è©³ç´°åˆ†æ
        
        Args:
            video_id: YouTubeå‹•ç”»ID
            
        Returns:
            é«˜åº¦ãªé–¢é€£æ€§åˆ†æçµæœ
        """
        try:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ” é«˜åº¦ãªé–¢é€£æ€§åˆ†æé–‹å§‹: {video_id}")
            
            # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿åé›†
            video_data = self._collect_video_data(video_id)
            if not video_data:
                return {"error": "å‹•ç”»ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
            
            images_data = video_data['images_data']
            if len(images_data) < 2:
                return {
                    "relationship_type": "single_image",
                    "message": "é–¢é€£æ€§åˆ†æã«ã¯è¤‡æ•°ã®ç”»åƒãŒå¿…è¦ã§ã™"
                }
            
            # Step 1: æ™‚é–“çš„é–¢ä¿‚æ€§åˆ†æ
            temporal_analysis = self._analyze_temporal_relationships(images_data)
            
            # Step 2: è¦–è¦šçš„é¡ä¼¼æ€§åˆ†æ
            visual_similarity = self._analyze_visual_similarities(images_data, video_id)
            
            # Step 3: ãƒ†ãƒ¼ãƒçš„é–¢é€£æ€§åˆ†æ
            thematic_relationships = self._analyze_thematic_relationships(images_data, video_id)
            
            # Step 4: ç©ºé–“çš„ãƒ»æ§‹å›³é–¢é€£æ€§åˆ†æ
            spatial_analysis = self._analyze_spatial_relationships(images_data, video_id)
            
            # Step 5: æ¥½æ›²æ§‹é€ ã¨ã®å¯¾å¿œåˆ†æ
            musical_correspondence = self._analyze_musical_correspondence(images_data, video_data['video_metadata'])
            
            # Step 6: æ„Ÿæƒ…ãƒ»ãƒ ãƒ¼ãƒ‰é·ç§»åˆ†æ
            emotional_flow = self._analyze_emotional_flow(images_data, video_id)
            
            # Step 7: çµ±åˆé–¢é€£æ€§ãƒãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ
            relationship_matrix = self._generate_relationship_matrix(images_data, {
                'temporal': temporal_analysis,
                'visual': visual_similarity,
                'thematic': thematic_relationships,
                'spatial': spatial_analysis,
                'musical': musical_correspondence,
                'emotional': emotional_flow
            })
            
            # çµæœçµ±åˆ
            advanced_analysis = {
                "video_id": video_id,
                "total_images": len(images_data),
                "analysis_timestamp": datetime.now().isoformat(),
                "temporal_analysis": temporal_analysis,
                "visual_similarity": visual_similarity,
                "thematic_relationships": thematic_relationships,
                "spatial_analysis": spatial_analysis,
                "musical_correspondence": musical_correspondence,
                "emotional_flow": emotional_flow,
                "relationship_matrix": relationship_matrix,
                "overall_coherence_score": self._calculate_overall_coherence(relationship_matrix),
                "narrative_structure": self._identify_narrative_structure(relationship_matrix),
                "key_transitions": self._identify_key_transitions(relationship_matrix)
            }
            
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âœ… é«˜åº¦ãªé–¢é€£æ€§åˆ†æå®Œäº†")
            return advanced_analysis
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ é«˜åº¦ãªé–¢é€£æ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _analyze_temporal_relationships(self, images_data: List[Dict]) -> Dict[str, Any]:
        """æ™‚é–“çš„é–¢ä¿‚æ€§ã®è©³ç´°åˆ†æ"""
        try:
            sorted_images = sorted(images_data, key=lambda x: x.get('upload_timestamp', ''))
            
            temporal_analysis = {
                "sequence_length": len(sorted_images),
                "time_spans": [],
                "sequence_patterns": [],
                "temporal_clusters": []
            }
            
            # æ™‚é–“é–“éš”åˆ†æ
            for i in range(len(sorted_images) - 1):
                current_time = sorted_images[i].get('upload_timestamp', '')
                next_time = sorted_images[i + 1].get('upload_timestamp', '')
                
                if current_time and next_time:
                    # ç°¡æ˜“çš„ãªæ™‚é–“å·®è¨ˆç®—ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ datetime ãƒ‘ãƒ¼ã‚¹ãŒå¿…è¦ï¼‰
                    temporal_analysis["time_spans"].append({
                        "from_image": sorted_images[i].get('image_id'),
                        "to_image": sorted_images[i + 1].get('image_id'),
                        "interval_type": "sequential"
                    })
            
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³è­˜åˆ¥
            if len(sorted_images) >= 3:
                temporal_analysis["sequence_patterns"] = [
                    "beginning_middle_end",
                    "chronological_progression"
                ]
            elif len(sorted_images) == 2:
                temporal_analysis["sequence_patterns"] = ["before_after"]
            
            return temporal_analysis
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ æ™‚é–“çš„é–¢ä¿‚æ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _analyze_visual_similarities(self, images_data: List[Dict], video_id: str) -> Dict[str, Any]:
        """è¦–è¦šçš„é¡ä¼¼æ€§ã®è©³ç´°åˆ†æ"""
        try:
            similarity_analysis = {
                "similarity_pairs": [],
                "visual_clusters": [],
                "common_visual_themes": [],
                "style_consistency": "high"  # ç°¡æ˜“å®Ÿè£…
            }
            
            # ç”»åƒãƒšã‚¢ã®é¡ä¼¼æ€§åˆ†æ
            for i in range(len(images_data)):
                for j in range(i + 1, len(images_data)):
                    img1 = images_data[i]
                    img2 = images_data[j]
                    
                    # åˆ†æçµæœå–å¾—
                    analysis1 = self.youtube_manager.get_image_analysis_result(video_id, img1.get('image_id', '')) if self.youtube_manager else None
                    analysis2 = self.youtube_manager.get_image_analysis_result(video_id, img2.get('image_id', '')) if self.youtube_manager else None
                    
                    if analysis1 and analysis2:
                        similarity_score = self._calculate_description_similarity(
                            analysis1.get('description', ''),
                            analysis2.get('description', '')
                        )
                        
                        similarity_analysis["similarity_pairs"].append({
                            "image1_id": img1.get('image_id'),
                            "image2_id": img2.get('image_id'),
                            "similarity_score": similarity_score,
                            "similarity_type": self._classify_similarity_type(analysis1, analysis2)
                        })
            
            # è¦–è¦šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            high_similarity_pairs = [
                pair for pair in similarity_analysis["similarity_pairs"]
                if pair["similarity_score"] > 0.7
            ]
            
            if high_similarity_pairs:
                similarity_analysis["visual_clusters"] = self._form_visual_clusters(high_similarity_pairs)
            
            return similarity_analysis
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ è¦–è¦šçš„é¡ä¼¼æ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _analyze_thematic_relationships(self, images_data: List[Dict], video_id: str) -> Dict[str, Any]:
        """ãƒ†ãƒ¼ãƒçš„é–¢é€£æ€§ã®è©³ç´°åˆ†æ"""
        try:
            thematic_analysis = {
                "dominant_themes": [],
                "theme_evolution": [],
                "thematic_consistency": 0.0,
                "cross_image_themes": []
            }
            
            all_themes = []
            image_themes = {}
            
            # å„ç”»åƒã®ãƒ†ãƒ¼ãƒæŠ½å‡º
            for img_data in images_data:
                image_id = img_data.get('image_id')
                analysis_result = self.youtube_manager.get_image_analysis_result(video_id, image_id) if self.youtube_manager else None
                
                if analysis_result:
                    themes = self._extract_themes_from_analysis(analysis_result)
                    image_themes[image_id] = themes
                    all_themes.extend(themes)
            
            # ãƒ†ãƒ¼ãƒé »åº¦åˆ†æ
            theme_counter = Counter(all_themes)
            thematic_analysis["dominant_themes"] = [
                {"theme": theme, "frequency": count}
                for theme, count in theme_counter.most_common(5)
            ]
            
            # ãƒ†ãƒ¼ãƒé€²åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
            if len(image_themes) >= 2:
                theme_sequence = []
                for img_data in sorted(images_data, key=lambda x: x.get('upload_timestamp', '')):
                    img_id = img_data.get('image_id')
                    if img_id in image_themes:
                        theme_sequence.append(image_themes[img_id])
                
                thematic_analysis["theme_evolution"] = self._analyze_theme_sequence(theme_sequence)
            
            # ãƒ†ãƒ¼ãƒä¸€è²«æ€§è¨ˆç®—
            if theme_counter:
                total_themes = sum(theme_counter.values())
                max_frequency = max(theme_counter.values())
                thematic_analysis["thematic_consistency"] = max_frequency / total_themes
            
            # ç”»åƒé–“å…±é€šãƒ†ãƒ¼ãƒ
            if len(image_themes) > 1:
                thematic_analysis["cross_image_themes"] = self._find_cross_image_themes(image_themes)
            
            return thematic_analysis
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ãƒ†ãƒ¼ãƒçš„é–¢é€£æ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _analyze_spatial_relationships(self, images_data: List[Dict], video_id: str) -> Dict[str, Any]:
        """ç©ºé–“çš„ãƒ»æ§‹å›³é–¢é€£æ€§åˆ†æ"""
        try:
            spatial_analysis = {
                "composition_patterns": [],
                "spatial_continuity": 0.0,
                "camera_movement_inference": [],
                "framing_analysis": []
            }
            
            # å„ç”»åƒã®ç©ºé–“çš„ç‰¹å¾´æŠ½å‡º
            spatial_features = []
            for img_data in images_data:
                image_id = img_data.get('image_id')
                analysis_result = self.youtube_manager.get_image_analysis_result(video_id, image_id) if self.youtube_manager else None
                
                if analysis_result:
                    spatial_feature = self._extract_spatial_features(analysis_result)
                    spatial_features.append({
                        "image_id": image_id,
                        "features": spatial_feature
                    })
            
            # æ§‹å›³ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            if spatial_features:
                composition_types = [f["features"].get("composition_type", "unknown") for f in spatial_features]
                spatial_analysis["composition_patterns"] = list(set(composition_types))
            
            # ç©ºé–“çš„é€£ç¶šæ€§è¨ˆç®—
            if len(spatial_features) > 1:
                continuity_scores = []
                for i in range(len(spatial_features) - 1):
                    score = self._calculate_spatial_continuity(
                        spatial_features[i]["features"],
                        spatial_features[i + 1]["features"]
                    )
                    continuity_scores.append(score)
                
                spatial_analysis["spatial_continuity"] = sum(continuity_scores) / len(continuity_scores)
            
            # ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯æ¨è«–
            spatial_analysis["camera_movement_inference"] = self._infer_camera_movements(spatial_features)
            
            return spatial_analysis
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ç©ºé–“çš„é–¢é€£æ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _analyze_musical_correspondence(self, images_data: List[Dict], video_metadata: Dict) -> Dict[str, Any]:
        """æ¥½æ›²æ§‹é€ ã¨ã®å¯¾å¿œåˆ†æ"""
        try:
            musical_analysis = {
                "song_structure_mapping": [],
                "tempo_visual_correlation": "moderate",
                "lyrical_visual_alignment": [],
                "musical_narrative_sync": 0.0
            }
            
            # æ¥½æ›²æƒ…å ±ã‹ã‚‰æ§‹é€ æ¨å®š
            title = video_metadata.get('title', '')
            description = video_metadata.get('description', '')
            
            # æ¥½æ›²æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¨å®š
            estimated_structure = self._estimate_song_structure(title, description, len(images_data))
            musical_analysis["song_structure_mapping"] = estimated_structure
            
            # ç”»åƒã¨æ¥½æ›²éƒ¨åˆ†ã®å¯¾å¿œ
            if len(images_data) >= 2:
                for i, img_data in enumerate(images_data):
                    structure_part = self._map_image_to_song_part(i, len(images_data), estimated_structure)
                    musical_analysis["lyrical_visual_alignment"].append({
                        "image_id": img_data.get('image_id'),
                        "song_part": structure_part,
                        "position_ratio": i / (len(images_data) - 1) if len(images_data) > 1 else 0
                    })
            
            # éŸ³æ¥½ãƒŠãƒ©ãƒ†ã‚£ãƒ–åŒæœŸåº¦è¨ˆç®—
            if musical_analysis["lyrical_visual_alignment"]:
                musical_analysis["musical_narrative_sync"] = self._calculate_narrative_sync(
                    musical_analysis["lyrical_visual_alignment"]
                )
            
            return musical_analysis
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ æ¥½æ›²å¯¾å¿œåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _analyze_emotional_flow(self, images_data: List[Dict], video_id: str) -> Dict[str, Any]:
        """æ„Ÿæƒ…ãƒ»ãƒ ãƒ¼ãƒ‰é·ç§»åˆ†æ"""
        try:
            emotional_analysis = {
                "emotion_sequence": [],
                "mood_transitions": [],
                "emotional_arc": "",
                "emotional_intensity_curve": []
            }
            
            # å„ç”»åƒã®æ„Ÿæƒ…åˆ†æ
            emotions = []
            for img_data in sorted(images_data, key=lambda x: x.get('upload_timestamp', '')):
                image_id = img_data.get('image_id')
                analysis_result = self.youtube_manager.get_image_analysis_result(video_id, image_id) if self.youtube_manager else None
                
                if analysis_result:
                    emotion = self._extract_emotion_from_analysis(analysis_result)
                    intensity = self._calculate_emotional_intensity(analysis_result)
                    
                    emotions.append({
                        "image_id": image_id,
                        "emotion": emotion,
                        "intensity": intensity
                    })
            
            emotional_analysis["emotion_sequence"] = emotions
            
            # ãƒ ãƒ¼ãƒ‰é·ç§»åˆ†æ
            if len(emotions) > 1:
                for i in range(len(emotions) - 1):
                    transition = {
                        "from_emotion": emotions[i]["emotion"],
                        "to_emotion": emotions[i + 1]["emotion"],
                        "transition_type": self._classify_mood_transition(
                            emotions[i]["emotion"],
                            emotions[i + 1]["emotion"]
                        )
                    }
                    emotional_analysis["mood_transitions"].append(transition)
            
            # æ„Ÿæƒ…ã‚¢ãƒ¼ã‚¯ç‰¹å®š
            emotional_analysis["emotional_arc"] = self._identify_emotional_arc(emotions)
            
            # æ„Ÿæƒ…å¼·åº¦ã‚«ãƒ¼ãƒ–
            emotional_analysis["emotional_intensity_curve"] = [
                e["intensity"] for e in emotions
            ]
            
            return emotional_analysis
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ æ„Ÿæƒ…ãƒ•ãƒ­ãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _generate_relationship_matrix(self, images_data: List[Dict], analysis_results: Dict) -> Dict[str, Any]:
        """é–¢é€£æ€§ãƒãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ"""
        try:
            num_images = len(images_data)
            matrix = {
                "size": num_images,
                "image_ids": [img.get('image_id') for img in images_data],
                "relationships": []
            }
            
            # å…¨ãƒšã‚¢çµ„ã¿åˆã‚ã›ã®é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            for i in range(num_images):
                for j in range(i + 1, num_images):
                    img1_id = images_data[i].get('image_id')
                    img2_id = images_data[j].get('image_id')
                    
                    # å„æ¬¡å…ƒã§ã®é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
                    temporal_score = self._get_temporal_score(i, j, analysis_results.get('temporal', {}))
                    visual_score = self._get_visual_score(img1_id, img2_id, analysis_results.get('visual', {}))
                    thematic_score = self._get_thematic_score(img1_id, img2_id, analysis_results.get('thematic', {}))
                    spatial_score = self._get_spatial_score(i, j, analysis_results.get('spatial', {}))
                    emotional_score = self._get_emotional_score(i, j, analysis_results.get('emotional', {}))
                    
                    # ç·åˆé–¢é€£æ€§ã‚¹ã‚³ã‚¢
                    overall_score = (temporal_score + visual_score + thematic_score + spatial_score + emotional_score) / 5
                    
                    matrix["relationships"].append({
                        "image1_id": img1_id,
                        "image2_id": img2_id,
                        "temporal_score": temporal_score,
                        "visual_score": visual_score,
                        "thematic_score": thematic_score,
                        "spatial_score": spatial_score,
                        "emotional_score": emotional_score,
                        "overall_score": overall_score,
                        "relationship_strength": "strong" if overall_score > 0.7 else "moderate" if overall_score > 0.4 else "weak"
                    })
            
            return matrix
            
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ é–¢é€£æ€§ãƒãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _calculate_description_similarity(self, desc1: str, desc2: str) -> float:
        """èª¬æ˜æ–‡ã®é¡ä¼¼åº¦è¨ˆç®—ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰"""
        try:
            words1 = set(desc1.lower().split())
            words2 = set(desc2.lower().split())
            
            if not words1 or not words2:
                return 0.0
            
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            return intersection / union if union > 0 else 0.0
        except Exception:
            return 0.0
    
    def _classify_similarity_type(self, analysis1: Dict, analysis2: Dict) -> str:
        """é¡ä¼¼æ€§ã‚¿ã‚¤ãƒ—ã®åˆ†é¡"""
        try:
            desc1 = analysis1.get('description', '').lower()
            desc2 = analysis2.get('description', '').lower()
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼æ€§åˆ†é¡
            if any(word in desc1 and word in desc2 for word in ['äººç‰©', 'ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ', 'æ­Œ', 'æ¼”å¥']):
                return "äººç‰©ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é¡ä¼¼"
            elif any(word in desc1 and word in desc2 for word in ['èƒŒæ™¯', 'ç’°å¢ƒ', 'å ´æ‰€', 'ã‚»ãƒƒãƒˆ']):
                return "ç’°å¢ƒãƒ»èƒŒæ™¯é¡ä¼¼"
            elif any(word in desc1 and word in desc2 for word in ['è‰²', 'ç…§æ˜', 'é›°å›²æ°—', 'ãƒ©ã‚¤ãƒˆ']):
                return "è¦–è¦šåŠ¹æœé¡ä¼¼"
            else:
                return "ä¸€èˆ¬çš„é¡ä¼¼"
        except Exception:
            return "ä¸æ˜"
    
    def _extract_themes_from_analysis(self, analysis_result: Dict) -> List[str]:
        """åˆ†æçµæœã‹ã‚‰ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º"""
        try:
            description = analysis_result.get('description', '').lower()
            themes = []
            
            # ãƒ†ãƒ¼ãƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
            theme_keywords = {
                'éŸ³æ¥½': ['éŸ³æ¥½', 'æ¥½å™¨', 'æ¼”å¥', 'æ­Œ', 'ãƒã‚¤ã‚¯', 'ã‚®ã‚¿ãƒ¼', 'ãƒ”ã‚¢ãƒ', 'ãƒ‰ãƒ©ãƒ '],
                'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹': ['ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹', 'ãƒ©ã‚¤ãƒ–', 'ã‚¹ãƒ†ãƒ¼ã‚¸', 'è¦³å®¢', 'ã‚³ãƒ³ã‚µãƒ¼ãƒˆ'],
                'ç…§æ˜ãƒ»æ˜ åƒ': ['ç…§æ˜', 'ãƒ©ã‚¤ãƒˆ', 'ã‚¨ãƒ•ã‚§ã‚¯ãƒˆ', 'æ˜ åƒ', 'ã‚«ãƒ¡ãƒ©'],
                'äººç‰©ãƒ»è¡¨æƒ…': ['äººç‰©', 'ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ', 'è¡¨æƒ…', 'æ„Ÿæƒ…', 'ç¬‘é¡”'],
                'ç’°å¢ƒãƒ»èƒŒæ™¯': ['èƒŒæ™¯', 'ç’°å¢ƒ', 'å ´æ‰€', 'ã‚»ãƒƒãƒˆ', 'ã‚¹ã‚¿ã‚¸ã‚ª']
            }
            
            for theme, keywords in theme_keywords.items():
                if any(keyword in description for keyword in keywords):
                    themes.append(theme)
            
            return themes if themes else ['ä¸€èˆ¬']
        except Exception:
            return ['ä¸€èˆ¬']
    
    def _extract_emotion_from_analysis(self, analysis_result: Dict) -> str:
        """åˆ†æçµæœã‹ã‚‰æ„Ÿæƒ…ã‚’æŠ½å‡º"""
        try:
            description = analysis_result.get('description', '').lower()
            
            # æ„Ÿæƒ…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
            emotion_keywords = {
                'å–œã³': ['æ˜ã‚‹ã„', 'æ¥½ã—ã„', 'å¬‰ã—ã„', 'ç¬‘é¡”', 'ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ã‚¨ãƒãƒ«ã‚®ãƒƒã‚·ãƒ¥'],
                'æƒ…ç†±': ['æƒ…ç†±çš„', 'ç†±ã„', 'æ¿€ã—ã„', 'åŠ›å¼·ã„', 'ãƒ‘ãƒ¯ãƒ•ãƒ«'],
                'è½ã¡ç€ã': ['è½ã¡ç€ã„ãŸ', 'é™ã‹', 'ç©ã‚„ã‹', 'ãƒªãƒ©ãƒƒã‚¯ã‚¹', 'å®‰ã‚‰ã‹'],
                'æ„Ÿå‹•': ['æ„Ÿå‹•çš„', 'ç¾ã—ã„', 'å°è±¡çš„', 'å¿ƒã«éŸ¿ã', 'ç´ æ™´ã‚‰ã—ã„'],
                'ç¥ç§˜': ['ç¥ç§˜çš„', 'å¹»æƒ³çš„', 'ä¸æ€è­°', 'ãƒŸã‚¹ãƒ†ãƒªã‚¢ã‚¹']
            }
            
            for emotion, keywords in emotion_keywords.items():
                if any(keyword in description for keyword in keywords):
                    return emotion
            
            return 'ä¸­æ€§'
        except Exception:
            return 'ä¸­æ€§'
    
    def _calculate_emotional_intensity(self, analysis_result: Dict) -> float:
        """æ„Ÿæƒ…å¼·åº¦ã®è¨ˆç®—"""
        try:
            description = analysis_result.get('description', '').lower()
            
            # å¼·åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            high_intensity = ['éå¸¸ã«', 'æ¥µã‚ã¦', 'å¼·çƒˆ', 'åœ§å€’çš„', 'æ¿€ã—ã']
            medium_intensity = ['ã‚„ã‚„', 'ãã“ãã“', 'é©åº¦ã«', 'ã»ã©ã‚ˆã']
            
            if any(word in description for word in high_intensity):
                return 0.9
            elif any(word in description for word in medium_intensity):
                return 0.6
            else:
                return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸­ç¨‹åº¦
        except Exception:
            return 0.5
    
    def _calculate_overall_coherence(self, relationship_matrix: Dict) -> float:
        """å…¨ä½“çš„ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        try:
            relationships = relationship_matrix.get('relationships', [])
            if not relationships:
                return 0.0
            
            overall_scores = [r['overall_score'] for r in relationships]
            return sum(overall_scores) / len(overall_scores)
        except Exception:
            return 0.0
    
    def _identify_narrative_structure(self, relationship_matrix: Dict) -> str:
        """ç‰©èªæ§‹é€ ã®ç‰¹å®š"""
        try:
            coherence = self._calculate_overall_coherence(relationship_matrix)
            size = relationship_matrix.get('size', 0)
            
            if coherence > 0.8:
                return "é«˜ã„ä¸€è²«æ€§ã‚’æŒã¤çµ±åˆçš„ãƒŠãƒ©ãƒ†ã‚£ãƒ–"
            elif coherence > 0.6:
                if size >= 3:
                    return "æ˜ç¢ºãªä¸‰å¹•æ§‹æˆ"
                else:
                    return "äºŒéƒ¨æ§‹æˆã®å±•é–‹"
            elif coherence > 0.4:
                return "ç·©ã‚„ã‹ãªé–¢é€£æ€§ã‚’æŒã¤ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ§‹æˆ"
            else:
                return "ç‹¬ç«‹ã—ãŸã‚·ãƒ¼ãƒ³é›†åˆ"
        except Exception:
            return "æ§‹é€ åˆ†æä¸å¯"
    
    def _identify_key_transitions(self, relationship_matrix: Dict) -> List[Dict]:
        """é‡è¦ãªè»¢æ›ç‚¹ã®ç‰¹å®š"""
        try:
            relationships = relationship_matrix.get('relationships', [])
            if len(relationships) < 2:
                return []
            
            # é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®å¤‰åŒ–ãŒå¤§ãã„ç®‡æ‰€ã‚’è»¢æ›ç‚¹ã¨ã™ã‚‹
            transitions = []
            for i, rel in enumerate(relationships[:-1]):
                current_score = rel['overall_score']
                next_score = relationships[i + 1]['overall_score']
                
                score_change = abs(next_score - current_score)
                if score_change > 0.3:  # é–¾å€¤
                    transitions.append({
                        "transition_point": i + 1,
                        "from_image": rel['image1_id'],
                        "to_image": rel['image2_id'],
                        "change_magnitude": score_change,
                        "transition_type": "dramatic_shift" if score_change > 0.5 else "moderate_shift"
                    })
            
            return transitions[:3]  # æœ€å¤§3ã¤ã¾ã§
        except Exception:
            return []
    
    # ç°¡æ˜“ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤ï¼ˆå®Ÿè£…ã‚’ç°¡ç•¥åŒ–ï¼‰
    def _form_visual_clusters(self, pairs): return []
    def _analyze_theme_sequence(self, sequence): return []
    def _find_cross_image_themes(self, themes): return []
    def _extract_spatial_features(self, analysis): return {"composition_type": "center"}
    def _calculate_spatial_continuity(self, f1, f2): return 0.5
    def _infer_camera_movements(self, features): return []
    def _estimate_song_structure(self, title, desc, num_images): return ["intro", "verse", "chorus"]
    def _map_image_to_song_part(self, index, total, structure): return structure[min(index, len(structure)-1)]
    def _calculate_narrative_sync(self, alignment): return 0.7
    def _classify_mood_transition(self, e1, e2): return "gradual" if e1 == e2 else "contrast"
    def _identify_emotional_arc(self, emotions): return "ä¸Šæ˜‡å‹" if len(emotions) > 1 else "å®‰å®šå‹"
    def _get_temporal_score(self, i, j, temporal): return abs(i - j) / 10
    def _get_visual_score(self, id1, id2, visual): return 0.6
    def _get_thematic_score(self, id1, id2, thematic): return 0.7
    def _get_spatial_score(self, i, j, spatial): return 0.5
    def _get_emotional_score(self, i, j, emotional): return 0.6


def test_video_image_context_builder():
    """VideoImageContextBuilderã®åŸºæœ¬å‹•ä½œãƒ†ã‚¹ãƒˆ"""
    print("=== VideoImageContextBuilder ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    
    try:
        # åˆæœŸåŒ–
        context_builder = VideoImageContextBuilder()
        print("âœ… VideoImageContextBuilderåˆæœŸåŒ–æˆåŠŸ")
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç¢ºèª
        templates = context_builder.conversation_templates
        print(f"âœ… ä¼šè©±ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ•°: {len(templates)}")
        
        # ã‚«ãƒ†ã‚´ãƒªç¢ºèª
        scene_categories = context_builder.scene_classifications
        visual_categories = context_builder.visual_element_categories
        print(f"âœ… ã‚·ãƒ¼ãƒ³ã‚«ãƒ†ã‚´ãƒªæ•°: {len(scene_categories)}")
        print(f"âœ… è¦–è¦šè¦ç´ ã‚«ãƒ†ã‚´ãƒªæ•°: {len(visual_categories)}")
        
        print("âœ… VideoImageContextBuilderåŸºæœ¬æ©Ÿèƒ½æ­£å¸¸")
        return True
        
    except Exception as e:
        print(f"âŒ VideoImageContextBuilderãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False


if __name__ == "__main__":
    test_video_image_context_builder()