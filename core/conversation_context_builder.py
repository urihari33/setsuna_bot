#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã‚·ã‚¹ãƒ†ãƒ  - ã›ã¤ãªBotçµ±åˆç”¨
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‹ã‚‰å‹•ç”»é–¢é€£ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
"""

import re
from typing import Dict, List, Any, Optional, Tuple
from .youtube_knowledge_manager import YouTubeKnowledgeManager
from .video_conversation_history import VideoConversationHistory
from .topic_learning_system import TopicLearningSystem
from .personalized_recommendation_engine import PersonalizedRecommendationEngine
from .context_understanding_system import ContextUnderstandingSystem
from .multi_turn_conversation_manager import MultiTurnConversationManager


class ConversationContextBuilder:
    """ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.knowledge_manager = YouTubeKnowledgeManager()
        self.conversation_history = VideoConversationHistory()
        
        # Phase 2-B-2: å­¦ç¿’ãƒ»æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ è¿½åŠ 
        try:
            self.topic_learning = TopicLearningSystem()
            self.personalized_engine = PersonalizedRecommendationEngine(
                self.topic_learning,
                self.conversation_history,
                self.knowledge_manager
            )
            print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âœ… å€‹äººåŒ–æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ çµ±åˆå®Œäº†")
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âš ï¸ å€‹äººåŒ–æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.topic_learning = None
            self.personalized_engine = None
        
        # Phase 2-B-3: æ–‡è„ˆç†è§£ãƒ»ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‚·ã‚¹ãƒ†ãƒ è¿½åŠ 
        try:
            self.context_understanding = ContextUnderstandingSystem()
            self.multi_turn_manager = MultiTurnConversationManager()
            print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âœ… æ–‡è„ˆç†è§£ãƒ»ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‚·ã‚¹ãƒ†ãƒ çµ±åˆå®Œäº†")
        except Exception as e:
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âš ï¸ æ–‡è„ˆç†è§£ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.context_understanding = None
            self.multi_turn_manager = None
        
        # å‹•ç”»é–¢é€£è³ªå•ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ”¹å–„ç‰ˆï¼‰
        self.video_query_patterns = [
            # çŸ¥è­˜ç¢ºèªç³»
            r'(.+)(çŸ¥ã£ã¦|è¦‹ãŸ|èã„ãŸ)?(ã‚‹?|ã“ã¨ã‚ã‚‹?|ã“ã¨?)?',
            r'(.+)(ã®|ã£ã¦ã„ã†)?(å‹•ç”»|æ›²|æ­Œ|MV|ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ãƒ“ãƒ‡ã‚ª)(.+)?',
            r'(.+)(ã£ã¦|ã¨ã„ã†)(ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼|ãƒãƒ£ãƒ³ãƒãƒ«|ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ)',
            
            # æ¨è–¦ãƒ»è³ªå•ç³»
            r'(ä½•ã‹|ã©ã‚“ãª|ãŠã™ã™ã‚ã®?)(.+)?(å‹•ç”»|æ›²|æ­Œ)(.+)?',
            r'(æœ€è¿‘|ä»Š|æ–°ã—ã„)(.+)?(è¦‹ãŸ|èã„ãŸ)(.+)?(å‹•ç”»|æ›²)',
            
            # å…·ä½“çš„è¨€åŠç³»
            r'(.+)ã«ã¤ã„ã¦',
            r'(.+)ã£ã¦(ã©ã†|ã©ã‚“ãª)',
            r'(.+)ã®(å°è±¡|æ„Ÿæƒ³|åˆ†æ)',
            
            # ç¶™ç¶šä¼šè©±ãƒ»å˜ç´”è¨€åŠç³»ï¼ˆæ–°è¦è¿½åŠ ï¼‰
            r'(ã˜ã‚ƒã‚|ãã‚Œã˜ã‚ƒ|ã§ã¯)?(.+)(ã¯|ã£ã¦)?$',  # ã€Œã˜ã‚ƒã‚ ã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼ã¯ã€ç­‰
            r'^(.+)(ã¯|ã£ã¦)$',  # ã€Œã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼ã¯ã€ç­‰ã®å˜ç´”è¨€åŠ
            r'(ãã£ã¡ã®|ãã®|ã‚ã®)(.+)',  # æ–‡è„ˆå‚ç…§
        ]
        
        print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âœ… ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _convert_katakana_to_english(self, katakana: str) -> List[str]:
        """
        ã‚«ã‚¿ã‚«ãƒŠã‚’å¯èƒ½æ€§ã®ã‚ã‚‹è‹±èªã«å¤‰æ›
        
        Args:
            katakana: ã‚«ã‚¿ã‚«ãƒŠæ–‡å­—åˆ—
            
        Returns:
            å¤‰æ›å€™è£œã®ãƒªã‚¹ãƒˆ
        """
        # ã‚ˆãä½¿ã‚ã‚Œã‚‹å¤‰æ›ãƒ‘ã‚¿ãƒ¼ãƒ³
        common_conversions = {
            'ãƒˆãƒªãƒ‹ãƒ†ã‚£': ['TRINITY', 'trinity'],
            'ã‚¨ãƒƒã‚¯ã‚¹ã‚ªãƒ¼ã‚¨ãƒƒã‚¯ã‚¹ã‚ªãƒ¼': ['XOXO', 'xoxo'],
            'ãƒœã‚«ãƒ­': ['VOCALOID', 'vocaloid'],
            'ãƒœã‚¤ã‚¹': ['VOICE', 'voice'],
            'ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯': ['MUSIC', 'music'],
            'ãƒ“ãƒ‡ã‚ª': ['VIDEO', 'video'],
            'ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼': ['CREATOR', 'creator'],
            'ãƒãƒ£ãƒ³ãƒãƒ«': ['CHANNEL', 'channel'],
            'ã‚³ãƒ©ãƒœ': ['COLLABORATION', 'collaboration', 'collab'],
            'ã‚ªãƒªã‚¸ãƒŠãƒ«': ['ORIGINAL', 'original'],
            'ã‚«ãƒãƒ¼': ['COVER', 'cover'],
        }
        
        candidates = []
        
        # ç›´æ¥å¤‰æ›
        if katakana in common_conversions:
            candidates.extend(common_conversions[katakana])
        
        # éƒ¨åˆ†ãƒãƒƒãƒãƒ³ã‚°
        for jp, en_list in common_conversions.items():
            if jp in katakana or katakana in jp:
                candidates.extend(en_list)
        
        return candidates

    def _extract_keywords(self, text: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        """
        keywords = []
        text_clean = text.strip()
        
        # å›ºæœ‰åè©å€™è£œï¼ˆã‚«ã‚¿ã‚«ãƒŠã€è‹±æ•°å­—ã€ç‰¹æ®Šè¨˜å·çµ„ã¿åˆã‚ã›ï¼‰
        proper_noun_patterns = [
            r'[ã‚¡-ãƒ¶ãƒ¼]+',  # ã‚«ã‚¿ã‚«ãƒŠ
            r'[A-Za-z][A-Za-z0-9]*',  # è‹±æ•°å­—
            r'â–½â–²[^â–²â–½]*â–²â–½',  # TRiNITYå½¢å¼
            r'ã«ã˜ã•ã‚“ã˜',  # é‡è¦ãªå›ºæœ‰åè©
            r'VOICEVOX|ãƒœã‚¤ã‚¹ãƒœãƒƒã‚¯ã‚¹',  # ãã®ä»–å›ºæœ‰åè©
        ]
        
        for pattern in proper_noun_patterns:
            matches = re.findall(pattern, text_clean)
            for match in matches:
                if len(match) > 1:  # 1æ–‡å­—ã¯é™¤å¤–
                    keywords.append(match)
                    
                    # ã‚«ã‚¿ã‚«ãƒŠã®å ´åˆã€è‹±èªå¤‰æ›å€™è£œã‚‚è¿½åŠ 
                    if re.match(r'[ã‚¡-ãƒ¶ãƒ¼]+', match):
                        english_candidates = self._convert_katakana_to_english(match)
                        keywords.extend(english_candidates)
        
        # æ¥½æ›²ãƒ»å‹•ç”»é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        content_keywords = [
            r'([^ã®\s]{2,})(ã®)?(å‹•ç”»|æ›²|æ­Œ|MV|ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ãƒ“ãƒ‡ã‚ª)',  # ã€ŒXXXã®å‹•ç”»ã€
            r'([^ã£\s]{2,})(ã£ã¦)(æ›²|æ­Œ)',  # ã€ŒXXXã£ã¦æ›²ã€
            r'([^ã«\s]{2,})(ã«ã¤ã„ã¦)',  # ã€ŒXXXã«ã¤ã„ã¦ã€
        ]
        
        # ä¸€èˆ¬çš„ãªå‹•ç”»é–¢é€£è¡¨ç¾ã®å ´åˆã¯ã€å½¢å®¹è©ã‚‚å«ã‚ã‚‹
        general_video_patterns = [
            r'(æœ€è¿‘|æ–°ã—ã„|é¢ç™½ã„|è‰¯ã„|ãŠã™ã™ã‚)',  # ä¸€èˆ¬çš„ãªå½¢å®¹è©
            r'(å‹•ç”»|æ›²|æ­Œ|MV)',  # å‹•ç”»é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        ]
        
        # å‹•ç”»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã®ä¸€èˆ¬çš„ãªå½¢å®¹è©æŠ½å‡º
        if any(keyword in text_clean for keyword in ['å‹•ç”»', 'æ›²', 'æ­Œ', 'MV']):
            for pattern in general_video_patterns:
                matches = re.findall(pattern, text_clean)
                for match in matches:
                    if len(match) > 1:
                        keywords.append(match)
        
        for pattern in content_keywords:
            matches = re.findall(pattern, text_clean)
            for match in matches:
                if isinstance(match, tuple):
                    keyword = match[0].strip()
                    if keyword and len(keyword) > 1:
                        keywords.append(keyword)
                        
                        # ã“ã®å ´åˆã‚‚è‹±èªå¤‰æ›ã‚’è©¦è¡Œ
                        if re.match(r'[ã‚¡-ãƒ¶ãƒ¼]+', keyword):
                            english_candidates = self._convert_katakana_to_english(keyword)
                            keywords.extend(english_candidates)
        
        # é‡è¤‡é™¤å»ã¨é•·ã„é †ã‚½ãƒ¼ãƒˆï¼ˆã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆï¼‰
        unique_keywords = list(set(keywords))
        unique_keywords.sort(key=len, reverse=True)
        
        return unique_keywords[:8]  # è‹±èªå¤‰æ›å€™è£œãŒå¢—ãˆã‚‹ã®ã§æœ€å¤§8å€‹ã¾ã§

    def detect_video_queries(self, text: str) -> List[Dict[str, Any]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»é–¢é€£ã‚¯ã‚¨ãƒªã‚’æ¤œå‡º
        
        Args:
            text: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æ¤œå‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
        """
        queries = []
        text_clean = text.strip()
        
        # ç›´æ¥çš„ãªå‹•ç”»é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
        video_keywords = ['å‹•ç”»', 'æ›²', 'æ­Œ', 'MV', 'ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ãƒ“ãƒ‡ã‚ª', 'ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼', 'ãƒãƒ£ãƒ³ãƒãƒ«']
        has_video_keyword = any(keyword in text_clean for keyword in video_keywords)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        extracted_keywords = self._extract_keywords(text_clean)
        
        # çŸ¥è­˜ç¢ºèªãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        knowledge_patterns = [
            r'(.+)çŸ¥ã£ã¦(ã‚‹|ã¾ã™|ã„ã‚‹|ã¾ã™ã‹)?',
            r'(.+)(è¦‹ãŸ|èã„ãŸ)ã“ã¨ã‚ã‚‹?',
            r'(.+)ã£ã¦(çŸ¥ã£ã¦|èã„ã¦)(ã‚‹|ã„ã‚‹|ã¾ã™ã‹)?',
            r'(.+)ã‚ã‚‹\?',
            r'(.+)ã«ã¤ã„ã¦(.+)çŸ¥ã£ã¦'
        ]
        
        for pattern in knowledge_patterns:
            matches = re.findall(pattern, text_clean)
            for match in matches:
                # ãƒãƒƒãƒã—ãŸéƒ¨åˆ†ã‹ã‚‰ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
                if isinstance(match, tuple):
                    query_text = match[0].strip()
                else:
                    query_text = match.strip()
                
                if query_text and len(query_text) > 1:
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚’å„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å…ƒã®ã‚¯ã‚¨ãƒª
                    search_terms = extracted_keywords if extracted_keywords else [query_text]
                    
                    for term in search_terms:
                        queries.append({
                            'type': 'knowledge_check',
                            'query': term,
                            'original_text': text_clean,
                            'confidence': 0.9 if term in extracted_keywords else 0.6
                        })
        
        # æ¨è–¦ãƒ»è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        recommendation_patterns = [
            r'(ä½•ã‹|ã©ã‚“ãª|ãŠã™ã™ã‚)(.+)?(å‹•ç”»|æ›²|æ­Œ)',
            r'(é¢ç™½ã„|è‰¯ã„|æ–°ã—ã„)(.+)?(å‹•ç”»|æ›²|æ­Œ)',
            r'(æœ€è¿‘|ä»Š)(.+)?(è¦‹ãŸ|èã„ãŸ)'
        ]
        
        for pattern in recommendation_patterns:
            if re.search(pattern, text_clean):
                # æ¨è–¦ç³»ã‚‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚’ä½¿ç”¨
                search_terms = extracted_keywords if extracted_keywords else [text_clean]
                
                for term in search_terms:
                    queries.append({
                        'type': 'recommendation',
                        'query': term,
                        'original_text': text_clean,
                        'confidence': 0.8 if term in extracted_keywords else 0.5
                    })
        
        # æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã€å‹•ç”»é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆ
        if extracted_keywords and has_video_keyword and not queries:
            for keyword in extracted_keywords:
                queries.append({
                    'type': 'general_search',
                    'query': keyword,
                    'original_text': text_clean,
                    'confidence': 0.7
                })
        
        # æ–°è¦è¿½åŠ : ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹ãŒå‹•ç”»é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„å ´åˆã§ã‚‚ã€
        # å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°æ¤œç´¢å¯¾è±¡ã¨ã—ã¦æ‰±ã†
        if extracted_keywords and not queries:
            for keyword in extracted_keywords:
                if self._is_specific_query(keyword):
                    queries.append({
                        'type': 'contextual_search',
                        'query': keyword,
                        'original_text': text_clean,
                        'confidence': 0.6
                    })
        
        return queries
    
    def _is_specific_query(self, query: str) -> bool:
        """
        ã‚¯ã‚¨ãƒªãŒå…·ä½“çš„ãªå›ºæœ‰åè©ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹å–„ç‰ˆï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            
        Returns:
            å…·ä½“çš„ãªã‚¯ã‚¨ãƒªã‹ã©ã†ã‹
        """
        # ä¸€èˆ¬çš„ã™ãã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã‚‰ã®ã¿ã®å ´åˆã¯æ¤œç´¢ã—ãªã„ï¼‰
        generic_keywords = {
            'å‹•ç”»', 'æ›²', 'æ­Œ', 'MV', 'ãƒŸãƒ¥ãƒ¼ã‚¸ãƒƒã‚¯ãƒ“ãƒ‡ã‚ª',
            'æœ€è¿‘', 'æ–°ã—ã„', 'é¢ç™½ã„', 'è‰¯ã„', 'ãŠã™ã™ã‚', 
            'ã„ã„', 'ã™ã”ã„', 'äººæ°—', 'æœ‰å', 'æœ€è¿‘è¦‹ãŸ',
            'ä½•ã‹', 'ã©ã‚“ãª', 'ã‚ã‚‹', 'ãªã„', 'ã‚„ã¤', 'ã‚‚ã®',
            'ã¨ã‹', 'ã¨ã', 'ã“ã“', 'ãã“', 'ã‚ãã“'
        }
        
        print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] å…·ä½“æ€§ãƒã‚§ãƒƒã‚¯: '{query}'")
        
        # é«˜ä¿¡é ¼åº¦å›ºæœ‰åè©ãƒ‘ã‚¿ãƒ¼ãƒ³
        high_confidence_patterns = [
            r'ã«ã˜ã•ã‚“ã˜',      # é‡è¦ãªå›ºæœ‰åè©
            r'â–½â–².*â–²â–½',       # TRiNITYå½¢å¼
            r'[A-Z][A-Z]+',    # å¤§æ–‡å­—ã®ç•¥èªï¼ˆä¾‹ï¼šXOXOï¼‰
            r'[A-Za-z]{4,}',   # 4æ–‡å­—ä»¥ä¸Šã®è‹±èªï¼ˆä¸€èˆ¬çš„ãªè‹±èªé™¤å¤–ï¼‰
        ]
        
        # ä¸­ä¿¡é ¼åº¦å›ºæœ‰åè©ãƒ‘ã‚¿ãƒ¼ãƒ³
        medium_confidence_patterns = [
            r'[ã‚¡-ãƒ¶ãƒ¼]{3,}',  # 3æ–‡å­—ä»¥ä¸Šã®ã‚«ã‚¿ã‚«ãƒŠ
            r'[A-Za-z]{3}',    # 3æ–‡å­—ã®è‹±èª
        ]
        
        # ä½ä¿¡é ¼åº¦å›ºæœ‰åè©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè¿½åŠ æ¤œè¨¼è¦ï¼‰
        low_confidence_patterns = [
            r'[ã‚¡-ãƒ¶ãƒ¼]{2}',   # 2æ–‡å­—ã®ã‚«ã‚¿ã‚«ãƒŠ
            r'[A-Za-z]{2}',    # 2æ–‡å­—ã®è‹±èª
        ]
        
        # ä¸€èˆ¬çš„ãªè‹±èªã‚’é™¤å¤–
        common_english_words = {
            'music', 'video', 'cover', 'song', 'new', 'old', 
            'good', 'bad', 'nice', 'cool', 'hot', 'big', 'small',
            'up', 'down', 'in', 'out', 'on', 'off', 'all', 'any'
        }
        
        confidence_score = 0
        found_patterns = []
        
        # é«˜ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
        for pattern in high_confidence_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                if match.lower() not in common_english_words:
                    confidence_score += 10
                    found_patterns.append(f"é«˜ä¿¡é ¼åº¦: {match}")
        
        # ä¸­ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
        for pattern in medium_confidence_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                if match.lower() not in common_english_words:
                    confidence_score += 5
                    found_patterns.append(f"ä¸­ä¿¡é ¼åº¦: {match}")
        
        # ä½ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
        for pattern in low_confidence_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                if match.lower() not in common_english_words:
                    confidence_score += 2
                    found_patterns.append(f"ä½ä¿¡é ¼åº¦: {match}")
        
        # æ—¥æœ¬èªå›ºæœ‰åè©å€™è£œï¼ˆ3æ–‡å­—ä»¥ä¸Šã®ã²ã‚‰ãŒãªãƒ»æ¼¢å­—ï¼‰
        japanese_name_pattern = r'[ã-ã‚–ä¸€-é¾¯]{3,}'
        japanese_matches = re.findall(japanese_name_pattern, query)
        for match in japanese_matches:
            if match not in generic_keywords and match not in ['ã‚ã‚ŠãŒã¨ã†', 'ãŠã¯ã‚ˆã†', 'ã“ã‚“ã«ã¡ã¯', 'ã“ã‚“ã°ã‚“ã¯']:
                confidence_score += 8
                found_patterns.append(f"æ—¥æœ¬èªå›ºæœ‰åè©: {match}")
        
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        if found_patterns:
            print(f"  â†’ ç™ºè¦‹ãƒ‘ã‚¿ãƒ¼ãƒ³: {', '.join(found_patterns)}")
            print(f"  â†’ ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {confidence_score}")
        
        # åˆ¤å®šåŸºæº–: ã‚¹ã‚³ã‚¢5ä»¥ä¸Šã§å…·ä½“çš„ã¨ã¿ãªã™
        is_specific = confidence_score >= 5
        
        if is_specific:
            print(f"  â†’ å…·ä½“çš„ã‚¯ã‚¨ãƒªï¼ˆã‚¹ã‚³ã‚¢: {confidence_score}ï¼‰")
        else:
            print(f"  â†’ ä¸€èˆ¬çš„ã‚¯ã‚¨ãƒªï¼ˆã‚¹ã‚³ã‚¢: {confidence_score}ï¼‰")
        
        return is_specific
    
    def _analyze_user_reaction_from_input(self, user_input: str) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‹ã‚‰åå¿œã‚’åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
            
        Returns:
            åå¿œã‚¿ã‚¤ãƒ— (positive/neutral/negative)
        """
        user_lower = user_input.lower()
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–åå¿œãƒ‘ã‚¿ãƒ¼ãƒ³
        positive_patterns = [
            r'(ã„ã„|è‰¯ã„|å¥½ã|æ°—ã«å…¥|ç´ æ™´ã‚‰ã—ã„|æœ€é«˜|ã™ã”ã„)',
            r'(ã‚‚ã†ä¸€åº¦|ã¾ãŸ|ç¹°ã‚Šè¿”|ãƒªãƒ”ãƒ¼ãƒˆ)',
            r'(ã‚ã‚ŠãŒã¨ã†|ã‚µãƒ³ã‚­ãƒ¥ãƒ¼)',
            r'(æ„Ÿå‹•|æ³£ã‘|å¿ƒã«éŸ¿|ç´ æ•µ|ãã‚Œã„|ç¾ã—ã„)'
        ]
        
        # ãƒã‚¬ãƒ†ã‚£ãƒ–åå¿œãƒ‘ã‚¿ãƒ¼ãƒ³
        negative_patterns = [
            r'(å«Œã„|ãƒ€ãƒ¡|è‰¯ããªã„|å¾®å¦™|ã‚¤ãƒã‚¤ãƒ)',
            r'(é£½ããŸ|ã‚‚ã†ã„ã„|é•ã†)',
            r'(ã¤ã¾ã‚‰ãªã„|é¢ç™½ããªã„|é€€å±ˆ)'
        ]
        
        for pattern in positive_patterns:
            if re.search(pattern, user_input):
                return "positive"
        
        for pattern in negative_patterns:
            if re.search(pattern, user_input):
                return "negative"
        
        return "neutral"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    def build_video_context(self, queries: List[Dict[str, Any]], max_videos: int = 3) -> Optional[Dict[str, Any]]:
        """
        æ¤œå‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‹ã‚‰å‹•ç”»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        
        Args:
            queries: æ¤œå‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
            max_videos: æœ€å¤§å‹•ç”»æ•°
            
        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not queries:
            return None
        
        # é‡è¤‡é™¤å»
        unique_queries = []
        seen_queries = set()
        for query_info in queries:
            query = query_info['query']
            if query not in seen_queries:
                unique_queries.append(query_info)
                seen_queries.add(query)
        
        # å…·ä½“çš„ãªã‚¯ã‚¨ãƒªã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        specific_queries = [q for q in unique_queries if self._is_specific_query(q['query'])]
        
        if not specific_queries:
            print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âš ï¸ å…·ä½“çš„ãªå›ºæœ‰åè©ãªã— - å€‹äººåŒ–æ¨è–¦ã‚’è©¦è¡Œ")
            
            # å…ƒã®å…¥åŠ›ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ’ãƒ³ãƒˆã‚’æŠ½å‡º
            context_hint = ""
            for query_info in unique_queries:
                context_hint += " " + query_info['query']
            
            # Phase 2-B-2: å€‹äººåŒ–æ¨è–¦ã‚’è©¦è¡Œ
            if self.personalized_engine:
                try:
                    personalized_result = self.personalized_engine.get_personalized_recommendations(
                        context_hint.strip(), limit=2
                    )
                    
                    if personalized_result["recommendations"]:
                        print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ§  å€‹äººåŒ–æ¨è–¦æˆåŠŸ: {len(personalized_result['recommendations'])}ä»¶")
                        
                        recommended_videos = []
                        for rec in personalized_result["recommendations"]:
                            video_data = rec['video_data']
                            metadata = video_data.get('metadata', {})
                            
                            video_info = {
                                'video_id': rec['video_id'],
                                'title': metadata.get('title', ''),
                                'channel': metadata.get('channel_title', ''),
                                'analysis_status': video_data.get('analysis_status', 'unknown'),
                                'search_score': rec.get('preference_score', 0.5) * 100,  # å—œå¥½ã‚¹ã‚³ã‚¢ã‚’æ¤œç´¢ã‚¹ã‚³ã‚¢å½¢å¼ã«å¤‰æ›
                                'query_type': 'personalized_recommendation',
                                'matched_query': 'å€‹äººåŒ–æ¨è–¦',
                                'recommendation_reason': rec.get('recommendation_reason', '')
                            }
                            
                            # åˆ†æçµæœã‚’è¿½åŠ 
                            if 'creative_insight' in video_data:
                                insight = video_data['creative_insight']
                                video_info['creators'] = insight.get('creators', [])
                                video_info['music_analysis'] = insight.get('music_analysis', {})
                                video_info['lyrics'] = insight.get('lyrics', {})
                            
                            recommended_videos.append(video_info)
                        
                        for video in recommended_videos:
                            print(f"  - {video['title'][:50]}... (å—œå¥½ã‚¹ã‚³ã‚¢: {video['search_score']:.1f})")
                        
                        return {
                            'search_terms': [context_hint.strip()],
                            'videos': recommended_videos,
                            'total_found': len(recommended_videos),
                            'recommendation_type': personalized_result["recommendation_type"],
                            'user_analysis': personalized_result["user_analysis"]
                        }
                        
                except Exception as e:
                    print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âš ï¸ å€‹äººåŒ–æ¨è–¦å¤±æ•—: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦
            print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ² ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦ã‚’å®Ÿè¡Œ")
            random_recommendations = self.knowledge_manager.get_random_recommendation(
                context_hint=context_hint.strip(), 
                limit=2
            )
            
            if not random_recommendations:
                print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦ã‚‚å¤±æ•—")
                return None
            
            # æ¨è–¦å‹•ç”»ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«å¤‰æ›
            recommended_videos = []
            for rec in random_recommendations:
                video_data = rec['data']
                metadata = video_data.get('metadata', {})
                
                video_info = {
                    'video_id': rec['video_id'],
                    'title': metadata.get('title', ''),
                    'channel': metadata.get('channel_title', ''),
                    'analysis_status': video_data.get('analysis_status', 'unknown'),
                    'search_score': rec['score'],
                    'query_type': 'random_recommendation',
                    'matched_query': 'ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦'
                }
                
                # åˆ†æçµæœã‚’è¿½åŠ 
                if 'creative_insight' in video_data:
                    insight = video_data['creative_insight']
                    video_info['creators'] = insight.get('creators', [])
                    video_info['music_analysis'] = insight.get('music_analysis', {})
                    video_info['lyrics'] = insight.get('lyrics', {})
                
                recommended_videos.append(video_info)
            
            print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ² ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦æˆåŠŸ: {len(recommended_videos)}ä»¶")
            for video in recommended_videos:
                print(f"  - {video['title'][:50]}... (é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢: {video['search_score']})")
            
            return {
                'search_terms': [context_hint.strip()],
                'videos': recommended_videos,
                'total_found': len(recommended_videos),
                'recommendation_type': 'random'
            }
        
        all_videos = []
        search_terms = []
        
        # å„ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦å‹•ç”»æ¤œç´¢
        for query_info in specific_queries:
            query = query_info['query']
            search_terms.append(query)
            
            # å‹•ç”»æ¤œç´¢å®Ÿè¡Œ
            search_results = self.knowledge_manager.search_videos(query, limit=max_videos)
            
            for result in search_results:
                # ä½ã‚¹ã‚³ã‚¢çµæœã‚’é™¤å¤–ï¼ˆã‚¹ã‚³ã‚¢10æœªæº€ï¼‰
                if result['score'] < 10:
                    continue
                    
                video_data = result['data']
                metadata = video_data.get('metadata', {})
                
                video_info = {
                    'video_id': result['video_id'],
                    'title': metadata.get('title', ''),
                    'channel': metadata.get('channel_title', ''),
                    'analysis_status': video_data.get('analysis_status', 'unknown'),
                    'search_score': result['score'],
                    'query_type': query_info['type'],
                    'matched_query': query
                }
                
                # åˆ†æçµæœã‚’è¿½åŠ 
                if 'creative_insight' in video_data:
                    insight = video_data['creative_insight']
                    video_info['creators'] = insight.get('creators', [])
                    video_info['music_analysis'] = insight.get('music_analysis', {})
                    video_info['lyrics'] = insight.get('lyrics', {})
                
                all_videos.append(video_info)
        
        if not all_videos:
            return None
        
        # é‡è¤‡é™¤å»ã¨ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
        unique_videos = {}
        for video in all_videos:
            video_id = video['video_id']
            if video_id not in unique_videos or video['search_score'] > unique_videos[video_id]['search_score']:
                unique_videos[video_id] = video
        
        sorted_videos = sorted(
            unique_videos.values(),
            key=lambda x: x['search_score'],
            reverse=True
        )[:max_videos]
        
        return {
            'search_terms': search_terms,
            'videos': sorted_videos,
            'total_found': len(unique_videos)
        }
    
    def format_for_setsuna(self, context: Dict[str, Any]) -> str:
        """
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã›ã¤ãªç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ï¼ˆæ„Ÿæƒ…è¡¨ç¾å¼·åŒ–ç‰ˆï¼‰
        
        Args:
            context: å‹•ç”»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ã›ã¤ãªç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not context or not context.get('videos'):
            return ""
        
        formatted_parts = []
        
        # å‹•ç”»æƒ…å ±ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆä¼šè©±å±¥æ­´çµ±åˆç‰ˆï¼‰
        for video in context['videos']:
            video_info = []
            
            # åŸºæœ¬æƒ…å ±
            video_id = video.get('video_id', '')
            full_title = video.get('title', '')
            channel = video.get('channel', '')
            analysis_status = video.get('analysis_status', 'unknown')
            
            # ã‚¿ã‚¤ãƒˆãƒ«ç°¡ç•¥åŒ–
            main_title = self.knowledge_manager._extract_main_title(full_title)
            
            # ä¼šè©±å±¥æ­´ã®å–å¾—
            conversation_context = self.conversation_history.get_conversation_context(video_id)
            conversation_hints = []
            if conversation_context:
                conversation_hints = self.conversation_history.generate_conversation_hints(video_id)
            
            # ç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ã‚’ä½¿ç”¨ï¼ˆãŸã ã—ã€å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚‚ä¿æŒï¼‰
            video_info.append(f"æ¥½æ›²å: {main_title}")
            if main_title != full_title and len(full_title) <= 80:
                video_info.append(f"ãƒ•ãƒ«ã‚¿ã‚¤ãƒˆãƒ«: {full_title}")
            
            if channel:
                video_info.append(f"ãƒãƒ£ãƒ³ãƒãƒ«: {channel}")
            
            # ä¼šè©±å±¥æ­´æƒ…å ±ã®è¿½åŠ 
            if conversation_context:
                familiarity_level = conversation_context['familiarity_level']
                conversation_count = conversation_context['conversation_count']
                recency = conversation_context['recency']
                
                if familiarity_level != 'new':
                    if familiarity_level == 'very_familiar':
                        video_info.append(f"ãŠãªã˜ã¿ã®æ¥½æ›²ï¼ˆ{conversation_count}å›ä¼šè©±ï¼‰")
                    elif familiarity_level == 'familiar':
                        video_info.append(f"å‰ã«ã‚‚è©±ã—ãŸæ¥½æ›²ï¼ˆ{conversation_count}å›ä¼šè©±ï¼‰")
                    else:
                        video_info.append(f"è©±ã—ãŸã“ã¨ãŒã‚ã‚‹æ¥½æ›²ï¼ˆ{conversation_count}å›ä¼šè©±ï¼‰")
                    
                    if recency == 'today':
                        video_info.append("ä»Šæ—¥ã‚‚è©±é¡Œã«ä¸Šã£ãŸ")
                    elif recency == 'recent':
                        video_info.append("æœ€è¿‘è©±ã—ãŸ")
            
            # åˆ†æçµæœãŒã‚ã‚‹å ´åˆ
            if analysis_status == 'completed':
                # ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼æƒ…å ±
                creators = video.get('creators', [])
                if creators:
                    creator_names = [c.get('name', '') for c in creators if c.get('name')]
                    if creator_names:
                        video_info.append(f"ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼: {', '.join(creator_names)}")
                
                # æ¥½æ›²åˆ†æ
                music_analysis = video.get('music_analysis', {})
                emotion_hints = []
                if music_analysis:
                    if music_analysis.get('genre'):
                        video_info.append(f"ã‚¸ãƒ£ãƒ³ãƒ«: {music_analysis['genre']}")
                    if music_analysis.get('mood'):
                        mood = music_analysis['mood']
                        video_info.append(f"ãƒ ãƒ¼ãƒ‰: {mood}")
                        emotion_hints.append(f"mood_{mood}")
                
                # æ­Œè©æƒ…å ±
                lyrics = video.get('lyrics', {})
                if lyrics:
                    if lyrics.get('theme'):
                        theme = lyrics['theme']
                        video_info.append(f"ãƒ†ãƒ¼ãƒ: {theme}")
                        emotion_hints.append(f"theme_{theme}")
                    if lyrics.get('main_message'):
                        video_info.append(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {lyrics['main_message']}")
                
                # æ„Ÿæƒ…è¡¨ç¾ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ 
                if emotion_hints:
                    video_info.append(f"æ„Ÿæƒ…ãƒ’ãƒ³ãƒˆ: {', '.join(emotion_hints)}")
                
                video_info.append("ï¼ˆåˆ†ææ¸ˆã¿ï¼‰")
            else:
                video_info.append("ï¼ˆèã„ãŸã“ã¨ã¯ã‚ã‚‹ï¼‰")
            
            # ä¼šè©±ãƒ’ãƒ³ãƒˆã®è¿½åŠ 
            if conversation_hints:
                video_info.append(f"ä¼šè©±ãƒ’ãƒ³ãƒˆ: {', '.join(conversation_hints[:2])}")  # æœ€å¤§2ã¤ã¾ã§
            
            formatted_parts.append(" / ".join(video_info))
        
        # æ¨è–¦ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸè¡¨ç¾æŒ‡ç¤ºã‚’è¿½åŠ ï¼ˆå€‹äººåŒ–æ¨è–¦å¯¾å¿œï¼‰
        recommendation_type = context.get('recommendation_type', 'search')
        user_analysis = context.get('user_analysis', {})
        
        if recommendation_type == 'familiar':
            formatted_parts.append("ã€è¡¨ç¾æŒ‡ç¤ºã€‘é¦´æŸ“ã¿æ¨è–¦: ã€Œã„ã¤ã‚‚ã®XXXã ã‘ã©ã€ã€ŒãŠæ°—ã«å…¥ã‚Šã®XXXã€ãªã©è¦ªã—ã¿ã®ã‚ã‚‹æ¨è–¦è¡¨ç¾ã‚’ä½¿ç”¨")
        elif recommendation_type == 'novel':
            formatted_parts.append("ã€è¡¨ç¾æŒ‡ç¤ºã€‘æ–°è¦æ¨è–¦: ã€Œåˆã‚ã¦ã ã‘ã©å¥½ã¿ã«åˆã„ãã†ãªXXXã€ã€Œæ–°ã—ãè¦‹ã¤ã‘ãŸXXXã€ãªã©ç™ºè¦‹ã®æ¨è–¦è¡¨ç¾ã‚’ä½¿ç”¨")
        elif recommendation_type == 'specific':
            formatted_parts.append("ã€è¡¨ç¾æŒ‡ç¤ºã€‘å—œå¥½ãƒãƒƒãƒæ¨è–¦: ã€Œã”å¸Œæœ›ã®ã€œç³»ãªã‚‰XXXã€ã€Œã€œãŒãŠå¥½ã¿ãªã‚‰XXXã€ãªã©çš„ç¢ºãªæ¨è–¦è¡¨ç¾ã‚’ä½¿ç”¨")
        elif recommendation_type == 'mixed':
            formatted_parts.append("ã€è¡¨ç¾æŒ‡ç¤ºã€‘å€‹äººåŒ–æ¨è–¦: å­¦ç¿’ã—ãŸå—œå¥½ã‚’æ´»ã‹ã—ã¦ã€Œã‚ãªãŸãªã‚‰XXXãŒæ°—ã«å…¥ã‚Šãã†ã€ãªã©å€‹äººçš„æ¨è–¦è¡¨ç¾ã‚’ä½¿ç”¨")
        elif recommendation_type == 'random':
            formatted_parts.append("ã€è¡¨ç¾æŒ‡ç¤ºã€‘ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦: ã€Œæœ€è¿‘è¦‹ãŸä¸­ã§ã¯ã€œã‹ãªã€ã€Œå€‹äººçš„ã«ã¯ã€œãŒæ°—ã«å…¥ã£ã¦ã‚‹ã€ãªã©è‡ªç„¶ãªæ¨è–¦è¡¨ç¾ã‚’ä½¿ç”¨")
        else:
            formatted_parts.append("ã€è¡¨ç¾æŒ‡ç¤ºã€‘æ¤œç´¢çµæœ: ã€Œã€œã«ã¤ã„ã¦çŸ¥ã£ã¦ã‚‹ã‚ˆã€ã€Œã€œãªã‚‰èã„ãŸã“ã¨ãŒã‚ã‚‹ã€ãªã©çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®è¡¨ç¾ã‚’ä½¿ç”¨")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœã«åŸºã¥ãè¿½åŠ æŒ‡ç¤º
        if user_analysis.get('familiarity_preference') == 'familiar':
            formatted_parts.append("ã€è¿½åŠ æŒ‡ç¤ºã€‘ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯é¦´æŸ“ã¿ã®ã‚ã‚‹ã‚‚ã®ã‚’æ±‚ã‚ã¦ã„ã‚‹ãŸã‚ã€è¦ªã—ã¿ã‚„ã™ã„è¡¨ç¾ã‚’é‡è¦–")
        elif user_analysis.get('familiarity_preference') == 'new':
            formatted_parts.append("ã€è¿½åŠ æŒ‡ç¤ºã€‘ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æ–°ã—ã„ã‚‚ã®ã‚’æ±‚ã‚ã¦ã„ã‚‹ãŸã‚ã€ç™ºè¦‹ãƒ»æ–°é®®ã•ã‚’å¼·èª¿ã—ãŸè¡¨ç¾ã‚’ä½¿ç”¨")
        
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        result = "\n".join([f"â€¢ {info}" for info in formatted_parts])
        
        return result
    
    def process_user_input(self, user_input: str) -> Optional[str]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å‡¦ç†ã—ã¦å‹•ç”»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆæ–‡è„ˆç†è§£å¼·åŒ–ç‰ˆï¼‰
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
            
        Returns:
            ã›ã¤ãªç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ï¼ˆNone if å‹•ç”»é–¢é€£ã§ãªã„ï¼‰
        """
        print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ” å…¥åŠ›åˆ†æ: '{user_input}'")
        
        # Phase 2-B-3: æ–‡è„ˆç†è§£åˆ†æ
        context_analysis = None
        if self.context_understanding:
            try:
                context_analysis = self.context_understanding.analyze_input_context(user_input)
                print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ§  æ–‡è„ˆåˆ†æå®Œäº†: ä»£åè©{len(context_analysis.get('pronoun_references', []))}ä»¶, ç¶™ç¶šæ€§{len(context_analysis.get('continuity_indicators', []))}ä»¶")
                
                # å‚ç…§è§£æ±ºã®å®Ÿè¡Œ
                if context_analysis.get("requires_resolution"):
                    resolution_result = self.context_understanding.resolve_references(context_analysis)
                    print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ”— å‚ç…§è§£æ±º: ä¿¡é ¼åº¦{resolution_result.get('confidence', 0):.2f}")
                    
                    # è§£æ±ºã•ã‚ŒãŸå‚ç…§ãŒã‚ã‚Œã°ã€ãã‚Œã‚’åŸºã«ã‚¯ã‚¨ãƒªã‚’è£œå¼·
                    if resolution_result.get("suggested_topics"):
                        print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ’¡ æ–‡è„ˆã‹ã‚‰è©±é¡Œã‚’ç‰¹å®š")
                        for suggestion in resolution_result["suggested_topics"]:
                            topic_data = suggestion["topic_data"]
                            print(f"  - {topic_data.get('title', 'unknown')} (ä¿¡é ¼åº¦: {suggestion['confidence']:.2f})")
                        
                        # ç‰¹å®šã•ã‚ŒãŸè©±é¡Œã‚’ç›´æ¥ä½¿ç”¨
                        return self._process_contextual_reference(user_input, resolution_result, context_analysis)
            except Exception as e:
                print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âš ï¸ æ–‡è„ˆç†è§£ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å¾“æ¥ã®ã‚¯ã‚¨ãƒªæ¤œå‡º
        queries = self.detect_video_queries(user_input)
        print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ“ æ¤œå‡ºã‚¯ã‚¨ãƒª: {len(queries)}ä»¶")
        for i, query in enumerate(queries):
            print(f"  {i+1}. {query['type']}: '{query['query']}' (ä¿¡é ¼åº¦: {query['confidence']})")
        
        if not queries:
            print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ å‹•ç”»é–¢é€£ã‚¯ã‚¨ãƒªãªã—")
            
            # Phase 2-B-3: æ–‡è„ˆç†è§£ã§ä¼šè©±è¨˜æ†¶æ›´æ–°ï¼ˆå‹•ç”»ãªã—ã§ã‚‚è¨˜éŒ²ï¼‰
            if self.context_understanding:
                self.context_understanding.update_conversation_memory(user_input, context_analysis or {})
            
            return None
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context = self.build_video_context(queries)
        if not context:
            print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âŒ ãƒãƒƒãƒã™ã‚‹å‹•ç”»ãªã—")
            return None
        
        print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âœ… å‹•ç”»ç™ºè¦‹: {len(context['videos'])}ä»¶")
        for video in context['videos']:
            print(f"  - {video['title'][:50]}... (ã‚¹ã‚³ã‚¢: {video['search_score']})")
        
        # æ¤œå‡ºã•ã‚ŒãŸå‹•ç”»æƒ…å ±ã‚’æº–å‚™
        mentioned_videos = []
        for video in context['videos']:
            video_id = video['video_id']
            video_title = self.knowledge_manager._extract_main_title(video.get('title', ''))
            
            mentioned_videos.append({
                "video_id": video_id,
                "title": video_title,
                "channel": video.get('channel', ''),
                "genre": video.get('genre', ''),
                "search_score": video.get('search_score', 0)
            })
            
            # å¾“æ¥ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            self.knowledge_manager.add_to_cache(video_id, user_input)
            
            # Phase 2-B-1: ä¼šè©±å±¥æ­´è¨˜éŒ²
            self.conversation_history.record_conversation(video_id, video_title, user_input)
            
            # Phase 2-B-2: å—œå¥½å­¦ç¿’
            if self.topic_learning:
                try:
                    # å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    full_video_data = self.knowledge_manager.get_video_context(video_id)
                    if full_video_data:
                        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åå¿œã‚’è‡ªå‹•åˆ†æï¼ˆã‚ˆã‚Šè©³ç´°ãªåˆ†æã¯å¾Œã§äººæ‰‹è¿½åŠ å¯èƒ½ï¼‰
                        reaction = self._analyze_user_reaction_from_input(user_input)
                        self.topic_learning.learn_from_interaction(
                            full_video_data, reaction, user_input, video_title
                        )
                except Exception as e:
                    print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âš ï¸ å—œå¥½å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
        
        # Phase 2-B-3: æ–‡è„ˆç†è§£ãƒ»ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ç®¡ç†
        if self.context_understanding:
            self.context_understanding.update_conversation_memory(user_input, context_analysis or {}, mentioned_videos)
        
        if self.multi_turn_manager:
            try:
                turn_result = self.multi_turn_manager.add_turn(
                    user_input, context_analysis or {}, "", mentioned_videos
                )
                print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ­ å¯¾è©±çŠ¶æ…‹: {turn_result.get('previous_state')} â†’ {turn_result.get('new_state')}")
                
                # ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«åæ˜ 
                conversation_context = self.multi_turn_manager.get_conversation_context_for_response()
                context['conversation_context'] = conversation_context
                
            except Exception as e:
                print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âš ï¸ ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ç®¡ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã›ã¤ãªç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_context = self.format_for_setsuna(context)
        print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ“„ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:")
        print(formatted_context[:200] + "..." if len(formatted_context) > 200 else formatted_context)
        
        return formatted_context
    
    def _process_contextual_reference(self, user_input: str, resolution_result: Dict[str, Any], 
                                    context_analysis: Dict[str, Any]) -> Optional[str]:
        """
        æ–‡è„ˆå‚ç…§ã«åŸºã¥ãå‡¦ç†
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
            resolution_result: å‚ç…§è§£æ±ºçµæœ
            context_analysis: æ–‡è„ˆåˆ†æçµæœ
            
        Returns:
            ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—
        """
        print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ”— æ–‡è„ˆå‚ç…§å‡¦ç†é–‹å§‹")
        
        # è§£æ±ºã•ã‚ŒãŸè©±é¡Œã‹ã‚‰å‹•ç”»æƒ…å ±ã‚’æ§‹ç¯‰
        videos = []
        for suggestion in resolution_result.get("suggested_topics", []):
            topic_data = suggestion["topic_data"]
            
            video_info = {
                "video_id": topic_data.get("video_id", ""),
                "title": topic_data.get("title", ""),
                "channel": topic_data.get("channel", ""),
                "genre": topic_data.get("genre", ""),
                "search_score": suggestion["confidence"] * 100,  # ä¿¡é ¼åº¦ã‚’ã‚¹ã‚³ã‚¢ã«å¤‰æ›
                "query_type": "contextual_reference",
                "matched_query": "æ–‡è„ˆå‚ç…§",
                "reference_type": suggestion.get("confidence", 0.5)
            }
            
            videos.append(video_info)
        
        if not videos:
            return None
        
        # æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context = {
            "search_terms": ["æ–‡è„ˆå‚ç…§"],
            "videos": videos,
            "total_found": len(videos),
            "context_type": "reference_resolution",
            "resolution_confidence": resolution_result.get("confidence", 0.0),
            "conversation_context": None
        }
        
        # åŒæ§˜ã®è¨˜éŒ²ãƒ»å­¦ç¿’å‡¦ç†
        mentioned_videos = []
        for video in videos:
            video_id = video["video_id"]
            video_title = video["title"]
            
            if video_id:  # æœ‰åŠ¹ãªå‹•ç”»IDãŒã‚ã‚‹å ´åˆã®ã¿
                mentioned_videos.append({
                    "video_id": video_id,
                    "title": video_title,
                    "channel": video.get("channel", ""),
                    "genre": video.get("genre", ""),
                    "search_score": video.get("search_score", 0)
                })
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¿½åŠ 
                self.knowledge_manager.add_to_cache(video_id, user_input)
                
                # ä¼šè©±å±¥æ­´è¨˜éŒ²
                self.conversation_history.record_conversation(video_id, video_title, user_input)
                
                # å—œå¥½å­¦ç¿’
                if self.topic_learning:
                    try:
                        full_video_data = self.knowledge_manager.get_video_context(video_id)
                        if full_video_data:
                            reaction = self._analyze_user_reaction_from_input(user_input)
                            self.topic_learning.learn_from_interaction(
                                full_video_data, reaction, user_input, video_title
                            )
                    except Exception as e:
                        print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âš ï¸ æ–‡è„ˆå‚ç…§ã§ã®å—œå¥½å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ–‡è„ˆç†è§£ãƒ»ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ç®¡ç†
        if self.context_understanding:
            self.context_understanding.update_conversation_memory(user_input, context_analysis, mentioned_videos)
        
        if self.multi_turn_manager:
            try:
                turn_result = self.multi_turn_manager.add_turn(
                    user_input, context_analysis, "", mentioned_videos
                )
                print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ğŸ­ æ–‡è„ˆå‚ç…§ã§ã®å¯¾è©±çŠ¶æ…‹: {turn_result.get('previous_state')} â†’ {turn_result.get('new_state')}")
                
                conversation_context = self.multi_turn_manager.get_conversation_context_for_response()
                context['conversation_context'] = conversation_context
                
            except Exception as e:
                print(f"[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âš ï¸ æ–‡è„ˆå‚ç…§ã§ã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ç®¡ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆç‰¹åˆ¥ãªæ–‡è„ˆå‚ç…§æŒ‡ç¤ºã‚’è¿½åŠ ï¼‰
        formatted_context = self.format_for_setsuna(context)
        
        # æ–‡è„ˆå‚ç…§ã®å ´åˆã®è¿½åŠ æŒ‡ç¤º
        contextual_instructions = [
            "ã€æ–‡è„ˆå‚ç…§ã€‘ã“ã®å¿œç­”ã¯å‰ã®ä¼šè©±ã‹ã‚‰ã®æ–‡è„ˆå‚ç…§ã«åŸºã¥ã„ã¦ã„ã¾ã™",
            f"ã€å‚ç…§ä¿¡é ¼åº¦ã€‘{resolution_result.get('confidence', 0.0):.2f}",
            "ã€è¡¨ç¾æŒ‡ç¤ºã€‘ã€Œã•ã£ãã®ã€œã€ã€Œãã®ã€œã«ã¤ã„ã¦ã€ãªã©æ–‡è„ˆã‚’æ˜ç¤ºã—ãŸè‡ªç„¶ãªå¿œç­”ã‚’ä½¿ç”¨"
        ]
        
        # æ¤œå‡ºã•ã‚ŒãŸä»£åè©æƒ…å ±ã‚’è¿½åŠ 
        pronouns = context_analysis.get("pronoun_references", [])
        if pronouns:
            pronoun_texts = [p.get("text", "") for p in pronouns]
            contextual_instructions.append(f"ã€æ¤œå‡ºä»£åè©ã€‘{', '.join(pronoun_texts)}")
        
        # ç¶™ç¶šæ€§æƒ…å ±ã‚’è¿½åŠ 
        continuity = context_analysis.get("continuity_indicators", [])
        if continuity:
            continuity_types = [c.get("type", "") for c in continuity]
            contextual_instructions.append(f"ã€ç¶™ç¶šæ€§ã€‘{', '.join(set(continuity_types))}")
        
        final_context = formatted_context + "\n" + "\n".join([f"â€¢ {inst}" for inst in contextual_instructions])
        
        print("[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] âœ… æ–‡è„ˆå‚ç…§å‡¦ç†å®Œäº†")
        return final_context


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    builder = ConversationContextBuilder()
    
    test_inputs = [
        "TRiNITYã®å‹•ç”»çŸ¥ã£ã¦ã‚‹ï¼Ÿ",
        "ã«ã˜ã•ã‚“ã˜ã®æ­Œã£ã¦ã¿ãŸå‹•ç”»ã‚ã‚‹ï¼Ÿ",
        "æœ€è¿‘é¢ç™½ã„å‹•ç”»è¦‹ãŸï¼Ÿ",
        "XOXOã£ã¦æ›²ã«ã¤ã„ã¦æ•™ãˆã¦"
    ]
    
    for test_input in test_inputs:
        print(f"\nğŸ” å…¥åŠ›: '{test_input}'")
        
        # ã‚¯ã‚¨ãƒªæ¤œå‡ºãƒ†ã‚¹ãƒˆ
        queries = builder.detect_video_queries(test_input)
        print(f"æ¤œå‡ºã‚¯ã‚¨ãƒª: {len(queries)}ä»¶")
        for query in queries:
            print(f"  - {query['type']}: '{query['query']}' (ä¿¡é ¼åº¦: {query['confidence']})")
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
        context_text = builder.process_user_input(test_input)
        if context_text:
            print(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context_text}")
        else:
            print("ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: ãªã—")