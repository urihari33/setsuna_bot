#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KnowledgeIntegrationSystem - Phase 2B-2
è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ã®çŸ¥è­˜çµ±åˆãƒ»æ·±ã„æ´å¯Ÿç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
import openai
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import uuid
import hashlib
from collections import defaultdict
import networkx as nx
import numpy as np
from difflib import SequenceMatcher
from .config_manager import get_config_manager

# Windowsç’°å¢ƒã®ãƒ‘ã‚¹è¨­å®š
if os.name == 'nt':
    DATA_DIR = Path("D:/setsuna_bot/data/activity_knowledge")
else:
    DATA_DIR = Path("/mnt/d/setsuna_bot/data/activity_knowledge")

@dataclass
class IntegratedKnowledge:
    """çµ±åˆçŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    knowledge_id: str
    integration_type: str  # "cross_session", "temporal_evolution", "concept_synthesis"
    source_sessions: List[str]
    source_knowledge_items: List[str]
    
    # çµ±åˆçµæœ
    synthesized_content: str
    key_insights: List[str]
    confidence_score: float  # 0.0-1.0
    novelty_score: float    # æ–°è¦æ€§ã‚¹ã‚³ã‚¢
    
    # é–¢é€£æ€§ãƒ»é–¢ä¿‚æ€§
    related_concepts: List[str]
    contradiction_analysis: Dict[str, Any]
    evolution_trends: List[Dict[str, Any]]
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    created_at: datetime
    integration_method: str  # "gpt_synthesis", "pattern_analysis", "network_analysis"
    quality_metrics: Dict[str, float]
    
    # æ´»ç”¨å¯èƒ½æ€§
    application_domains: List[str]
    actionable_insights: List[str]
    future_research_directions: List[str]

@dataclass
class KnowledgePattern:
    """çŸ¥è­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    pattern_id: str
    pattern_type: str  # "recurring_theme", "causal_relationship", "temporal_trend"
    description: str
    supporting_evidence: List[str]
    confidence: float
    occurrences: List[Dict[str, Any]]  # å‡ºç¾ç®‡æ‰€ãƒ»æ–‡è„ˆ
    
@dataclass
class ConceptEvolution:
    """æ¦‚å¿µé€²åŒ–ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    concept_name: str
    evolution_timeline: List[Dict[str, Any]]  # æ™‚ç³»åˆ—å¤‰åŒ–
    trend_direction: str  # "increasing", "decreasing", "stable", "cyclical"
    key_turning_points: List[Dict[str, Any]]
    prediction_confidence: float

class KnowledgeIntegrationSystem:
    """çŸ¥è­˜çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.integration_dir = DATA_DIR / "integrated_knowledge"
        self.integration_dir.mkdir(parents=True, exist_ok=True)
        
        # OpenAIè¨­å®š
        self.openai_client = None
        self._initialize_openai()
        
        # GPTè¨­å®š
        self.gpt_config = {
            "model": "gpt-4-turbo-preview",
            "temperature": 0.3,  # åˆ†æçš„æ€è€ƒé‡è¦–
            "max_tokens": 2000
        }
        
        # çµ±åˆè¨­å®š
        self.integration_config = {
            "min_sessions_for_integration": 2,
            "similarity_threshold": 0.6,
            "confidence_threshold": 0.7,
            "max_integration_scope": 10,  # æœ€å¤§çµ±åˆã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°
            "enable_temporal_analysis": True,
            "enable_contradiction_detection": True,
            "enable_trend_prediction": True
        }
        
        # çŸ¥è­˜ã‚°ãƒ©ãƒ•
        self.knowledge_graph = nx.MultiDiGraph()
        
        # çµ±åˆæ¸ˆã¿çŸ¥è­˜
        self.integrated_knowledge: Dict[str, IntegratedKnowledge] = {}
        self.knowledge_patterns: Dict[str, KnowledgePattern] = {}
        self.concept_evolutions: Dict[str, ConceptEvolution] = {}
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.similarity_cache: Dict[str, float] = {}
        self.integration_cache: Dict[str, Dict] = {}
        
        self._load_existing_integrations()
        
        print("[çŸ¥è­˜çµ±åˆ] âœ… KnowledgeIntegrationSystemåˆæœŸåŒ–å®Œäº†")
    
    def _initialize_openai(self):
        """OpenAI APIåˆæœŸåŒ–"""
        try:
            # ConfigManagerçµŒç”±ã§OpenAIè¨­å®šå–å¾—
            config = get_config_manager()
            openai_key = config.get_openai_key()
            
            if openai_key:
                openai.api_key = openai_key
                self.openai_client = openai
                
                # æ¥ç¶šãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                try:
                    # ç°¡å˜ãªæ¥ç¶šãƒ†ã‚¹ãƒˆ
                    test_response = openai.models.list()
                    if test_response:
                        print("[çŸ¥è­˜çµ±åˆ] âœ… OpenAI APIè¨­å®šãƒ»æ¥ç¶šç¢ºèªå®Œäº†")
                        return True
                except Exception as api_error:
                    print(f"[çŸ¥è­˜çµ±åˆ] âŒ OpenAI APIæ¥ç¶šå¤±æ•—: {api_error}")
                    self.openai_client = None
                    return False
            else:
                print("[çŸ¥è­˜çµ±åˆ] âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                print("  .envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
                self.openai_client = None
                return False
                
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ OpenAI APIåˆæœŸåŒ–å¤±æ•—: {e}")
            self.openai_client = None
            return False
    
    def _load_existing_integrations(self):
        """æ—¢å­˜çµ±åˆçµæœã®èª­ã¿è¾¼ã¿"""
        try:
            # çµ±åˆçŸ¥è­˜èª­ã¿è¾¼ã¿
            integration_files = list(self.integration_dir.glob("integration_*.json"))
            for file_path in integration_files:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for item_data in data.get("integrated_knowledge", []):
                        item = IntegratedKnowledge(
                            knowledge_id=item_data["knowledge_id"],
                            integration_type=item_data["integration_type"],
                            source_sessions=item_data["source_sessions"],
                            source_knowledge_items=item_data["source_knowledge_items"],
                            synthesized_content=item_data["synthesized_content"],
                            key_insights=item_data["key_insights"],
                            confidence_score=item_data["confidence_score"],
                            novelty_score=item_data["novelty_score"],
                            related_concepts=item_data["related_concepts"],
                            contradiction_analysis=item_data["contradiction_analysis"],
                            evolution_trends=item_data["evolution_trends"],
                            created_at=datetime.fromisoformat(item_data["created_at"]),
                            integration_method=item_data["integration_method"],
                            quality_metrics=item_data["quality_metrics"],
                            application_domains=item_data["application_domains"],
                            actionable_insights=item_data["actionable_insights"],
                            future_research_directions=item_data["future_research_directions"]
                        )
                        self.integrated_knowledge[item.knowledge_id] = item
            
            print(f"[çŸ¥è­˜çµ±åˆ] ğŸ“š æ—¢å­˜çµ±åˆçŸ¥è­˜èª­ã¿è¾¼ã¿: {len(self.integrated_knowledge)}ä»¶")
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    
    def integrate_multi_session_knowledge(self, 
                                        session_ids: List[str],
                                        session_data: Dict[str, Dict],
                                        integration_scope: str = "comprehensive") -> List[IntegratedKnowledge]:
        """
        è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³çŸ¥è­˜çµ±åˆ
        
        Args:
            session_ids: çµ±åˆå¯¾è±¡ã‚»ãƒƒã‚·ãƒ§ãƒ³IDãƒªã‚¹ãƒˆ
            session_data: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿è¾æ›¸
            integration_scope: çµ±åˆç¯„å›² ("basic", "comprehensive", "deep")
            
        Returns:
            çµ±åˆçŸ¥è­˜ãƒªã‚¹ãƒˆ
        """
        try:
            print(f"[çŸ¥è­˜çµ±åˆ] ğŸ”— è¤‡æ•°ã‚»ãƒƒã‚·ãƒ§ãƒ³çŸ¥è­˜çµ±åˆé–‹å§‹: {len(session_ids)}ã‚»ãƒƒã‚·ãƒ§ãƒ³")
            
            if len(session_ids) < self.integration_config["min_sessions_for_integration"]:
                print("[çŸ¥è­˜çµ±åˆ] âš ï¸ çµ±åˆã«ã¯æœ€ä½2ã‚»ãƒƒã‚·ãƒ§ãƒ³å¿…è¦")
                return []
            
            # 1. çŸ¥è­˜ã‚°ãƒ©ãƒ•æ§‹ç¯‰
            self._build_knowledge_graph(session_ids, session_data)
            
            # 2. ã‚¯ãƒ­ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æ
            cross_session_results = self._perform_cross_session_analysis(session_ids, session_data)
            
            # 3. æ™‚ç³»åˆ—é€²åŒ–åˆ†æ
            temporal_results = []
            if self.integration_config["enable_temporal_analysis"]:
                temporal_results = self._perform_temporal_evolution_analysis(session_ids, session_data)
            
            # 4. æ¦‚å¿µåˆæˆåˆ†æ
            concept_synthesis_results = self._perform_concept_synthesis(session_ids, session_data)
            
            # 5. çµ±åˆçµæœãƒãƒ¼ã‚¸
            all_integrations = cross_session_results + temporal_results + concept_synthesis_results
            
            # 6. å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_integrations = self._filter_integration_quality(all_integrations)
            
            # 7. çµ±åˆçµæœä¿å­˜
            self._save_integrations(filtered_integrations)
            
            print(f"[çŸ¥è­˜çµ±åˆ] âœ… çµ±åˆå®Œäº†: {len(filtered_integrations)}ä»¶ã®çµ±åˆçŸ¥è­˜ç”Ÿæˆ")
            
            return filtered_integrations
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ çŸ¥è­˜çµ±åˆå¤±æ•—: {e}")
            return []
    
    def _build_knowledge_graph(self, session_ids: List[str], session_data: Dict[str, Dict]):
        """çŸ¥è­˜ã‚°ãƒ©ãƒ•æ§‹ç¯‰"""
        try:
            print("[çŸ¥è­˜çµ±åˆ] ğŸ•¸ï¸ çŸ¥è­˜ã‚°ãƒ©ãƒ•æ§‹ç¯‰é–‹å§‹")
            
            # ã‚°ãƒ©ãƒ•ã‚¯ãƒªã‚¢ï¼ˆæ–°è¦æ§‹ç¯‰ï¼‰
            self.knowledge_graph.clear()
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ¥ã«çŸ¥è­˜ãƒãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸è¿½åŠ 
            for session_id in session_ids:
                session = session_data.get(session_id, {})
                knowledge_items = session.get("knowledge_items", [])
                
                # çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒãƒ¼ãƒ‰ã¨ã—ã¦è¿½åŠ 
                for item in knowledge_items:
                    node_id = item.get("item_id", f"item_{uuid.uuid4().hex[:8]}")
                    
                    self.knowledge_graph.add_node(node_id, **{
                        "session_id": session_id,
                        "content": item.get("content", ""),
                        "categories": item.get("categories", []),
                        "keywords": item.get("keywords", []),
                        "entities": item.get("entities", []),
                        "importance_score": item.get("importance_score", 0.5),
                        "reliability_score": item.get("reliability_score", 0.7)
                    })
                
                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é–¢ä¿‚æ€§ã‚’ã‚¨ãƒƒã‚¸ã¨ã—ã¦è¿½åŠ 
                self._add_entity_relationships(session_id, knowledge_items)
            
            print(f"[çŸ¥è­˜çµ±åˆ] âœ… çŸ¥è­˜ã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†: {self.knowledge_graph.number_of_nodes()}ãƒãƒ¼ãƒ‰, {self.knowledge_graph.number_of_edges()}ã‚¨ãƒƒã‚¸")
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ çŸ¥è­˜ã‚°ãƒ©ãƒ•æ§‹ç¯‰å¤±æ•—: {e}")
    
    def _add_entity_relationships(self, session_id: str, knowledge_items: List[Dict]):
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–¢ä¿‚æ€§è¿½åŠ """
        try:
            # åŒä¸€ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å…±èµ·é–¢ä¿‚
            entity_cooccurrence = defaultdict(list)
            
            for item in knowledge_items:
                entities = item.get("entities", [])
                item_id = item.get("item_id", "")
                
                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšã‚¢ç”Ÿæˆ
                for i, entity1 in enumerate(entities):
                    for entity2 in entities[i+1:]:
                        if entity1 != entity2:
                            # å…±èµ·é–¢ä¿‚è¨˜éŒ²
                            entity_cooccurrence[(entity1, entity2)].append(item_id)
            
            # å…±èµ·é »åº¦ã«åŸºã¥ãã‚¨ãƒƒã‚¸è¿½åŠ 
            for (entity1, entity2), item_ids in entity_cooccurrence.items():
                if len(item_ids) >= 1:  # æœ€ä½1å›ä»¥ä¸Šå…±èµ·
                    self.knowledge_graph.add_edge(
                        entity1, entity2,
                        relationship_type="co_occurs_in",
                        session_id=session_id,
                        strength=len(item_ids),
                        supporting_items=item_ids
                    )
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âš ï¸ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–¢ä¿‚æ€§è¿½åŠ å¤±æ•—: {e}")
    
    def _perform_cross_session_analysis(self, session_ids: List[str], session_data: Dict[str, Dict]) -> List[IntegratedKnowledge]:
        """ã‚¯ãƒ­ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æ"""
        try:
            print("[çŸ¥è­˜çµ±åˆ] ğŸ” ã‚¯ãƒ­ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æé–‹å§‹")
            
            cross_session_integrations = []
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–“ã®æ¦‚å¿µé‡è¤‡ãƒ»é–¢é€£æ€§åˆ†æ
            concept_overlaps = self._find_concept_overlaps(session_ids, session_data)
            
            # é‡è¤‡æ¦‚å¿µã®çµ±åˆ
            for overlap in concept_overlaps:
                if overlap["similarity_score"] >= self.integration_config["similarity_threshold"]:
                    integration = self._create_cross_session_integration(overlap, session_data)
                    if integration:
                        cross_session_integrations.append(integration)
            
            # è£œå®Œçš„çŸ¥è­˜ã®çµ±åˆ
            complementary_knowledge = self._find_complementary_knowledge(session_ids, session_data)
            for complement in complementary_knowledge:
                integration = self._create_complementary_integration(complement, session_data)
                if integration:
                    cross_session_integrations.append(integration)
            
            print(f"[çŸ¥è­˜çµ±åˆ] âœ… ã‚¯ãƒ­ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æå®Œäº†: {len(cross_session_integrations)}ä»¶")
            
            return cross_session_integrations
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ ã‚¯ãƒ­ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æå¤±æ•—: {e}")
            return []
    
    def _find_concept_overlaps(self, session_ids: List[str], session_data: Dict[str, Dict]) -> List[Dict]:
        """æ¦‚å¿µé‡è¤‡æ¤œå‡º"""
        overlaps = []
        
        try:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–“ã®çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ æ¯”è¼ƒ
            for i, session_id1 in enumerate(session_ids):
                for session_id2 in session_ids[i+1:]:
                    session1_items = session_data.get(session_id1, {}).get("knowledge_items", [])
                    session2_items = session_data.get(session_id2, {}).get("knowledge_items", [])
                    
                    # ã‚¢ã‚¤ãƒ†ãƒ é–“é¡ä¼¼åº¦è¨ˆç®—
                    for item1 in session1_items:
                        for item2 in session2_items:
                            similarity = self._calculate_knowledge_similarity(item1, item2)
                            
                            if similarity >= self.integration_config["similarity_threshold"]:
                                overlaps.append({
                                    "session1": session_id1,
                                    "session2": session_id2,
                                    "item1": item1,
                                    "item2": item2,
                                    "similarity_score": similarity,
                                    "overlap_type": "concept_overlap"
                                })
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âš ï¸ æ¦‚å¿µé‡è¤‡æ¤œå‡ºå¤±æ•—: {e}")
        
        return overlaps
    
    def _calculate_knowledge_similarity(self, item1: Dict, item2: Dict) -> float:
        """çŸ¥è­˜é¡ä¼¼åº¦è¨ˆç®—"""
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cache_key = f"{item1.get('item_id', '')}_{item2.get('item_id', '')}"
            if cache_key in self.similarity_cache:
                return self.similarity_cache[cache_key]
            
            # è¤‡æ•°è¦ç´ ã§ã®é¡ä¼¼åº¦è¨ˆç®—
            similarities = []
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é¡ä¼¼åº¦
            content1 = item1.get("content", "")
            content2 = item2.get("content", "")
            if content1 and content2:
                content_sim = SequenceMatcher(None, content1.lower(), content2.lower()).ratio()
                similarities.append(("content", content_sim, 0.4))
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¡ä¼¼åº¦
            keywords1 = set(item1.get("keywords", []))
            keywords2 = set(item2.get("keywords", []))
            if keywords1 and keywords2:
                keyword_sim = len(keywords1 & keywords2) / len(keywords1 | keywords2)
                similarities.append(("keywords", keyword_sim, 0.3))
            
            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¡ä¼¼åº¦
            entities1 = set(item1.get("entities", []))
            entities2 = set(item2.get("entities", []))
            if entities1 and entities2:
                entity_sim = len(entities1 & entities2) / len(entities1 | entities2)
                similarities.append(("entities", entity_sim, 0.2))
            
            # ã‚«ãƒ†ã‚´ãƒªé¡ä¼¼åº¦
            categories1 = set(item1.get("categories", []))
            categories2 = set(item2.get("categories", []))
            if categories1 and categories2:
                category_sim = len(categories1 & categories2) / len(categories1 | categories2)
                similarities.append(("categories", category_sim, 0.1))
            
            # é‡ã¿ä»˜ãå¹³å‡
            if similarities:
                weighted_sum = sum(sim * weight for _, sim, weight in similarities)
                total_weight = sum(weight for _, _, weight in similarities)
                final_similarity = weighted_sum / total_weight
            else:
                final_similarity = 0.0
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            self.similarity_cache[cache_key] = final_similarity
            
            return final_similarity
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âš ï¸ é¡ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
            return 0.0
    
    def _find_complementary_knowledge(self, session_ids: List[str], session_data: Dict[str, Dict]) -> List[Dict]:
        """è£œå®Œçš„çŸ¥è­˜æ¤œå‡º"""
        complementary_sets = []
        
        try:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–“ã®ã‚«ãƒ†ã‚´ãƒªãƒ»ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æ
            session_domains = {}
            for session_id in session_ids:
                knowledge_items = session_data.get(session_id, {}).get("knowledge_items", [])
                
                domains = set()
                for item in knowledge_items:
                    domains.update(item.get("categories", []))
                
                session_domains[session_id] = domains
            
            # è£œå®Œé–¢ä¿‚æ¤œå‡º
            for session_id1 in session_ids:
                for session_id2 in session_ids:
                    if session_id1 != session_id2:
                        domains1 = session_domains[session_id1]
                        domains2 = session_domains[session_id2]
                        
                        # è£œå®Œæ€§è©•ä¾¡ï¼ˆé‡è¤‡ãŒå°‘ãªãã€çµ„ã¿åˆã‚ã›ã§åŒ…æ‹¬çš„ï¼‰
                        overlap = len(domains1 & domains2)
                        union = len(domains1 | domains2)
                        
                        if overlap < len(domains1) * 0.5 and union > len(domains1) * 1.2:
                            complementary_sets.append({
                                "session1": session_id1,
                                "session2": session_id2,
                                "complementary_type": "domain_complement",
                                "overlap_ratio": overlap / len(domains1) if domains1 else 0,
                                "expansion_ratio": union / len(domains1) if domains1 else 0
                            })
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âš ï¸ è£œå®Œçš„çŸ¥è­˜æ¤œå‡ºå¤±æ•—: {e}")
        
        return complementary_sets
    
    def _create_cross_session_integration(self, overlap: Dict, session_data: Dict[str, Dict]) -> Optional[IntegratedKnowledge]:
        """ã‚¯ãƒ­ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±åˆä½œæˆ"""
        try:
            # GPT-4ã«ã‚ˆã‚‹çµ±åˆåˆ†æ
            if self.openai_client:
                integration_content = self._synthesize_with_gpt(overlap, session_data)
            else:
                integration_content = self._synthesize_without_gpt(overlap, session_data)
            
            if not integration_content:
                return None
            
            knowledge_id = f"cross_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            
            integration = IntegratedKnowledge(
                knowledge_id=knowledge_id,
                integration_type="cross_session",
                source_sessions=[overlap["session1"], overlap["session2"]],
                source_knowledge_items=[
                    overlap["item1"].get("item_id", ""),
                    overlap["item2"].get("item_id", "")
                ],
                synthesized_content=integration_content["synthesized_content"],
                key_insights=integration_content["key_insights"],
                confidence_score=overlap["similarity_score"],
                novelty_score=integration_content.get("novelty_score", 0.5),
                related_concepts=integration_content.get("related_concepts", []),
                contradiction_analysis=integration_content.get("contradiction_analysis", {}),
                evolution_trends=integration_content.get("evolution_trends", []),
                created_at=datetime.now(),
                integration_method="gpt_synthesis" if self.openai_client else "pattern_analysis",
                quality_metrics=integration_content.get("quality_metrics", {}),
                application_domains=integration_content.get("application_domains", []),
                actionable_insights=integration_content.get("actionable_insights", []),
                future_research_directions=integration_content.get("future_research_directions", [])
            )
            
            return integration
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ ã‚¯ãƒ­ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±åˆä½œæˆå¤±æ•—: {e}")
            return None
    
    def _synthesize_with_gpt(self, overlap: Dict, session_data: Dict[str, Dict]) -> Optional[Dict]:
        """GPT-4ã«ã‚ˆã‚‹çµ±åˆåˆæˆ"""
        try:
            item1 = overlap["item1"]
            item2 = overlap["item2"]
            
            prompt = f"""
ä»¥ä¸‹ã®2ã¤ã®é–¢é€£ã™ã‚‹çŸ¥è­˜ã‚’çµ±åˆã—ã€ã‚ˆã‚Šæ·±ã„æ´å¯Ÿã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€çŸ¥è­˜1ã€‘
å†…å®¹: {item1.get('content', '')}
ã‚«ãƒ†ã‚´ãƒª: {', '.join(item1.get('categories', []))}
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(item1.get('keywords', []))}

ã€çŸ¥è­˜2ã€‘  
å†…å®¹: {item2.get('content', '')}
ã‚«ãƒ†ã‚´ãƒª: {', '.join(item2.get('categories', []))}
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(item2.get('keywords', []))}

ä»¥ä¸‹ã®å½¢å¼ã§çµ±åˆçµæœã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

{{
  "synthesized_content": "çµ±åˆã•ã‚ŒãŸçŸ¥è­˜ã®èª¬æ˜",
  "key_insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2", "æ´å¯Ÿ3"],
  "novelty_score": 0.8,
  "related_concepts": ["é–¢é€£æ¦‚å¿µ1", "é–¢é€£æ¦‚å¿µ2"],
  "contradiction_analysis": {{"conflicts": [], "resolutions": []}},
  "evolution_trends": [{{"trend": "å‚¾å‘", "evidence": "æ ¹æ‹ "}}],
  "quality_metrics": {{"coherence": 0.9, "completeness": 0.8}},
  "application_domains": ["å¿œç”¨åˆ†é‡1", "å¿œç”¨åˆ†é‡2"],
  "actionable_insights": ["å®Ÿè¡Œå¯èƒ½ãªæ´å¯Ÿ1", "å®Ÿè¡Œå¯èƒ½ãªæ´å¯Ÿ2"],
  "future_research_directions": ["ä»Šå¾Œã®ç ”ç©¶æ–¹å‘1", "ä»Šå¾Œã®ç ”ç©¶æ–¹å‘2"]
}}
"""
            
            response = self.openai_client.chat.completions.create(
                model=self.gpt_config["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=self.gpt_config["temperature"],
                max_tokens=self.gpt_config["max_tokens"]
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # JSONæŠ½å‡º
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                json_text = response_text[json_start:json_end].strip()
            elif "{" in response_text and "}" in response_text:
                json_start = response_text.find("{")
                json_end = response_text.rfind("}") + 1
                json_text = response_text[json_start:json_end]
            else:
                return None
            
            return json.loads(json_text)
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ GPTçµ±åˆåˆæˆå¤±æ•—: {e}")
            return None
    
    def _synthesize_without_gpt(self, overlap: Dict, session_data: Dict[str, Dict]) -> Dict:
        """GPTç„¡ã—ã§ã®çµ±åˆåˆæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        item1 = overlap["item1"]
        item2 = overlap["item2"]
        
        # åŸºæœ¬çš„ãªçµ±åˆ
        combined_content = f"{item1.get('content', '')} {item2.get('content', '')}"
        combined_keywords = list(set(item1.get('keywords', []) + item2.get('keywords', [])))
        combined_categories = list(set(item1.get('categories', []) + item2.get('categories', [])))
        
        return {
            "synthesized_content": f"2ã¤ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸé–¢é€£çŸ¥è­˜ã®çµ±åˆ: {combined_content[:200]}...",
            "key_insights": [
                f"å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(combined_keywords[:3])}",
                f"é–¢é€£ã‚«ãƒ†ã‚´ãƒª: {', '.join(combined_categories[:3])}"
            ],
            "novelty_score": 0.6,
            "related_concepts": combined_keywords[:5],
            "contradiction_analysis": {"conflicts": [], "resolutions": []},
            "evolution_trends": [],
            "quality_metrics": {"coherence": 0.7, "completeness": 0.6},
            "application_domains": combined_categories,
            "actionable_insights": ["çµ±åˆçŸ¥è­˜ã®è©³ç´°åˆ†æãŒæ¨å¥¨"],
            "future_research_directions": ["é–¢é€£åˆ†é‡ã®è¿½åŠ èª¿æŸ»"]
        }
    
    def _create_complementary_integration(self, complement: Dict, session_data: Dict[str, Dict]) -> Optional[IntegratedKnowledge]:
        """è£œå®Œçš„çµ±åˆä½œæˆ"""
        try:
            session1_data = session_data.get(complement["session1"], {})
            session2_data = session_data.get(complement["session2"], {})
            
            # è£œå®Œçš„çŸ¥è­˜ã®çµ±åˆ
            knowledge_id = f"comp_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            
            integration = IntegratedKnowledge(
                knowledge_id=knowledge_id,
                integration_type="cross_session",
                source_sessions=[complement["session1"], complement["session2"]],
                source_knowledge_items=[],
                synthesized_content=f"è£œå®Œçš„ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±åˆ: {complement['complementary_type']}",
                key_insights=[
                    f"ã‚»ãƒƒã‚·ãƒ§ãƒ³é–“ã®è£œå®Œæ€§: {complement['expansion_ratio']:.2f}",
                    "ç•°ãªã‚‹è¦³ç‚¹ã‹ã‚‰ã®åŒ…æ‹¬çš„ç†è§£"
                ],
                confidence_score=0.7,
                novelty_score=0.6,
                related_concepts=[],
                contradiction_analysis={},
                evolution_trends=[],
                created_at=datetime.now(),
                integration_method="pattern_analysis",
                quality_metrics={"coherence": 0.7, "completeness": 0.8},
                application_domains=[],
                actionable_insights=["çµ±åˆã•ã‚ŒãŸåŒ…æ‹¬çš„ãªç†è§£ã®æ´»ç”¨"],
                future_research_directions=["è£œå®Œåˆ†é‡ã®æ›´ãªã‚‹æ¢æ±‚"]
            )
            
            return integration
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ è£œå®Œçš„çµ±åˆä½œæˆå¤±æ•—: {e}")
            return None
    
    def _perform_temporal_evolution_analysis(self, session_ids: List[str], session_data: Dict[str, Dict]) -> List[IntegratedKnowledge]:
        """æ™‚ç³»åˆ—é€²åŒ–åˆ†æ"""
        try:
            print("[çŸ¥è­˜çµ±åˆ] â° æ™‚ç³»åˆ—é€²åŒ–åˆ†æé–‹å§‹")
            
            temporal_integrations = []
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆ
            sessions_with_time = []
            for session_id in session_ids:
                session_info = session_data.get(session_id, {})
                session_metadata = session_info.get("session_metadata", {})
                created_at = session_metadata.get("created_at")
                if created_at:
                    sessions_with_time.append((session_id, datetime.fromisoformat(created_at)))
            
            sessions_with_time.sort(key=lambda x: x[1])
            
            # æ¦‚å¿µã®æ™‚ç³»åˆ—å¤‰åŒ–åˆ†æ
            concept_evolution = self._analyze_concept_evolution(sessions_with_time, session_data)
            
            for evolution in concept_evolution:
                integration = self._create_temporal_integration(evolution, session_data)
                if integration:
                    temporal_integrations.append(integration)
            
            print(f"[çŸ¥è­˜çµ±åˆ] âœ… æ™‚ç³»åˆ—é€²åŒ–åˆ†æå®Œäº†: {len(temporal_integrations)}ä»¶")
            
            return temporal_integrations
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ æ™‚ç³»åˆ—é€²åŒ–åˆ†æå¤±æ•—: {e}")
            return []
    
    def _analyze_concept_evolution(self, sessions_with_time: List[Tuple], session_data: Dict[str, Dict]) -> List[ConceptEvolution]:
        """æ¦‚å¿µé€²åŒ–åˆ†æ"""
        evolutions = []
        
        try:
            # æ¦‚å¿µã®å‡ºç¾ãƒ»å¤‰åŒ–ã‚’è¿½è·¡
            concept_timeline = defaultdict(list)
            
            for session_id, timestamp in sessions_with_time:
                knowledge_items = session_data.get(session_id, {}).get("knowledge_items", [])
                
                for item in knowledge_items:
                    entities = item.get("entities", [])
                    keywords = item.get("keywords", [])
                    
                    for concept in entities + keywords:
                        concept_timeline[concept].append({
                            "session_id": session_id,
                            "timestamp": timestamp,
                            "context": item.get("content", "")[:100],
                            "importance": item.get("importance_score", 0.5)
                        })
            
            # é€²åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            for concept, timeline in concept_timeline.items():
                if len(timeline) >= 2:  # æœ€ä½2å›å‡ºç¾
                    evolution = ConceptEvolution(
                        concept_name=concept,
                        evolution_timeline=timeline,
                        trend_direction=self._determine_trend_direction(timeline),
                        key_turning_points=self._find_turning_points(timeline),
                        prediction_confidence=0.7
                    )
                    evolutions.append(evolution)
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âš ï¸ æ¦‚å¿µé€²åŒ–åˆ†æå¤±æ•—: {e}")
        
        return evolutions
    
    def _determine_trend_direction(self, timeline: List[Dict]) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ¤å®š"""
        if len(timeline) < 2:
            return "stable"
        
        # é‡è¦åº¦ã®å¤‰åŒ–ã‚’åˆ†æ
        importance_values = [entry["importance"] for entry in timeline]
        
        if len(importance_values) >= 3:
            # ç·šå½¢å›å¸°ã«ã‚ˆã‚‹å‚¾å‘åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
            x = list(range(len(importance_values)))
            slope = np.polyfit(x, importance_values, 1)[0]
            
            if slope > 0.1:
                return "increasing"
            elif slope < -0.1:
                return "decreasing"
            else:
                return "stable"
        else:
            # å˜ç´”æ¯”è¼ƒ
            if importance_values[-1] > importance_values[0]:
                return "increasing"
            elif importance_values[-1] < importance_values[0]:
                return "decreasing"
            else:
                return "stable"
    
    def _find_turning_points(self, timeline: List[Dict]) -> List[Dict]:
        """è»¢æ›ç‚¹æ¤œå‡º"""
        turning_points = []
        
        if len(timeline) >= 3:
            importance_values = [entry["importance"] for entry in timeline]
            
            # æ¥µå€¤æ¤œå‡º
            for i in range(1, len(importance_values) - 1):
                prev_val = importance_values[i-1]
                curr_val = importance_values[i]
                next_val = importance_values[i+1]
                
                # æ¥µå¤§å€¤
                if curr_val > prev_val and curr_val > next_val:
                    turning_points.append({
                        "type": "peak",
                        "session_id": timeline[i]["session_id"],
                        "timestamp": timeline[i]["timestamp"].isoformat(),
                        "importance": curr_val
                    })
                
                # æ¥µå°å€¤
                elif curr_val < prev_val and curr_val < next_val:
                    turning_points.append({
                        "type": "valley",
                        "session_id": timeline[i]["session_id"],
                        "timestamp": timeline[i]["timestamp"].isoformat(),
                        "importance": curr_val
                    })
        
        return turning_points
    
    def _create_temporal_integration(self, evolution: ConceptEvolution, session_data: Dict[str, Dict]) -> Optional[IntegratedKnowledge]:
        """æ™‚ç³»åˆ—çµ±åˆä½œæˆ"""
        try:
            knowledge_id = f"temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            
            source_sessions = [entry["session_id"] for entry in evolution.evolution_timeline]
            
            integration = IntegratedKnowledge(
                knowledge_id=knowledge_id,
                integration_type="temporal_evolution",
                source_sessions=source_sessions,
                source_knowledge_items=[],
                synthesized_content=f"æ¦‚å¿µã€Œ{evolution.concept_name}ã€ã®æ™‚ç³»åˆ—é€²åŒ–åˆ†æ",
                key_insights=[
                    f"ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘: {evolution.trend_direction}",
                    f"è»¢æ›ç‚¹æ•°: {len(evolution.key_turning_points)}",
                    f"å‡ºç¾ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {len(evolution.evolution_timeline)}"
                ],
                confidence_score=evolution.prediction_confidence,
                novelty_score=0.7,
                related_concepts=[evolution.concept_name],
                contradiction_analysis={},
                evolution_trends=[{
                    "concept": evolution.concept_name,
                    "trend": evolution.trend_direction,
                    "timeline": [asdict(entry) for entry in evolution.evolution_timeline]
                }],
                created_at=datetime.now(),
                integration_method="temporal_analysis",
                quality_metrics={"coherence": 0.8, "completeness": 0.7},
                application_domains=["ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ", "äºˆæ¸¬"],
                actionable_insights=[f"æ¦‚å¿µã€Œ{evolution.concept_name}ã€ã®{evolution.trend_direction}å‚¾å‘ã‚’æ´»ç”¨"],
                future_research_directions=[f"ã€Œ{evolution.concept_name}ã€ã®å°†æ¥å‹•å‘äºˆæ¸¬"]
            )
            
            return integration
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ æ™‚ç³»åˆ—çµ±åˆä½œæˆå¤±æ•—: {e}")
            return None
    
    def _perform_concept_synthesis(self, session_ids: List[str], session_data: Dict[str, Dict]) -> List[IntegratedKnowledge]:
        """æ¦‚å¿µåˆæˆåˆ†æ"""
        try:
            print("[çŸ¥è­˜çµ±åˆ] ğŸ§  æ¦‚å¿µåˆæˆåˆ†æé–‹å§‹")
            
            synthesis_integrations = []
            
            # æ¦‚å¿µé–“ã®é–¢ä¿‚æ€§åˆ†æ
            concept_relationships = self._analyze_concept_relationships(session_ids, session_data)
            
            # æ–°ã—ã„æ´å¯Ÿã®åˆæˆ
            for relationship in concept_relationships:
                if relationship["strength"] >= 0.6:
                    integration = self._create_synthesis_integration(relationship, session_data)
                    if integration:
                        synthesis_integrations.append(integration)
            
            print(f"[çŸ¥è­˜çµ±åˆ] âœ… æ¦‚å¿µåˆæˆåˆ†æå®Œäº†: {len(synthesis_integrations)}ä»¶")
            
            return synthesis_integrations
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ æ¦‚å¿µåˆæˆåˆ†æå¤±æ•—: {e}")
            return []
    
    def _analyze_concept_relationships(self, session_ids: List[str], session_data: Dict[str, Dict]) -> List[Dict]:
        """æ¦‚å¿µé–¢ä¿‚æ€§åˆ†æ"""
        relationships = []
        
        try:
            # å…¨æ¦‚å¿µæŠ½å‡º
            all_concepts = set()
            concept_contexts = defaultdict(list)
            
            for session_id in session_ids:
                knowledge_items = session_data.get(session_id, {}).get("knowledge_items", [])
                
                for item in knowledge_items:
                    entities = item.get("entities", [])
                    keywords = item.get("keywords", [])
                    
                    for concept in entities + keywords:
                        all_concepts.add(concept)
                        concept_contexts[concept].append({
                            "session_id": session_id,
                            "content": item.get("content", ""),
                            "categories": item.get("categories", [])
                        })
            
            # æ¦‚å¿µãƒšã‚¢ã®é–¢ä¿‚æ€§åˆ†æ
            concept_list = list(all_concepts)
            for i, concept1 in enumerate(concept_list):
                for concept2 in concept_list[i+1:]:
                    relationship_strength = self._calculate_concept_relationship_strength(
                        concept1, concept2, concept_contexts
                    )
                    
                    if relationship_strength > 0.3:
                        relationships.append({
                            "concept1": concept1,
                            "concept2": concept2,
                            "strength": relationship_strength,
                            "relationship_type": "semantic_association"
                        })
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âš ï¸ æ¦‚å¿µé–¢ä¿‚æ€§åˆ†æå¤±æ•—: {e}")
        
        return relationships
    
    def _calculate_concept_relationship_strength(self, concept1: str, concept2: str, concept_contexts: Dict) -> float:
        """æ¦‚å¿µé–¢ä¿‚æ€§å¼·åº¦è¨ˆç®—"""
        try:
            contexts1 = concept_contexts.get(concept1, [])
            contexts2 = concept_contexts.get(concept2, [])
            
            if not contexts1 or not contexts2:
                return 0.0
            
            # å…±èµ·åˆ†æ
            cooccurrence_count = 0
            total_contexts = len(contexts1) + len(contexts2)
            
            for ctx1 in contexts1:
                for ctx2 in contexts2:
                    # åŒä¸€ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§ã®å…±èµ·
                    if ctx1["session_id"] == ctx2["session_id"]:
                        cooccurrence_count += 1
                    
                    # ã‚«ãƒ†ã‚´ãƒªå…±é€šæ€§
                    common_categories = set(ctx1["categories"]) & set(ctx2["categories"])
                    if common_categories:
                        cooccurrence_count += 0.5
            
            return min(1.0, cooccurrence_count / total_contexts) if total_contexts > 0 else 0.0
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âš ï¸ æ¦‚å¿µé–¢ä¿‚æ€§å¼·åº¦è¨ˆç®—å¤±æ•—: {e}")
            return 0.0
    
    def _create_synthesis_integration(self, relationship: Dict, session_data: Dict[str, Dict]) -> Optional[IntegratedKnowledge]:
        """åˆæˆçµ±åˆä½œæˆ"""
        try:
            knowledge_id = f"synth_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            
            integration = IntegratedKnowledge(
                knowledge_id=knowledge_id,
                integration_type="concept_synthesis",
                source_sessions=[],  # é–¢ä¿‚æ€§åˆ†æã®ãŸã‚ç‰¹å®šã‚»ãƒƒã‚·ãƒ§ãƒ³ãªã—
                source_knowledge_items=[],
                synthesized_content=f"æ¦‚å¿µã€Œ{relationship['concept1']}ã€ã¨ã€Œ{relationship['concept2']}ã€ã®é–¢ä¿‚æ€§åˆ†æ",
                key_insights=[
                    f"æ¦‚å¿µé–“é–¢ä¿‚æ€§å¼·åº¦: {relationship['strength']:.2f}",
                    f"é–¢ä¿‚æ€§ã‚¿ã‚¤ãƒ—: {relationship['relationship_type']}"
                ],
                confidence_score=relationship["strength"],
                novelty_score=0.6,
                related_concepts=[relationship["concept1"], relationship["concept2"]],
                contradiction_analysis={},
                evolution_trends=[],
                created_at=datetime.now(),
                integration_method="network_analysis",
                quality_metrics={"coherence": 0.7, "completeness": 0.6},
                application_domains=["æ¦‚å¿µãƒãƒƒãƒ—", "é–¢ä¿‚æ€§åˆ†æ"],
                actionable_insights=[f"æ¦‚å¿µé–¢ä¿‚æ€§ã®æ´»ç”¨ã«ã‚ˆã‚‹æ·±ã„ç†è§£"],
                future_research_directions=[f"é–¢é€£æ¦‚å¿µç¾¤ã®ä½“ç³»çš„åˆ†æ"]
            )
            
            return integration
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ åˆæˆçµ±åˆä½œæˆå¤±æ•—: {e}")
            return None
    
    def _filter_integration_quality(self, integrations: List[IntegratedKnowledge]) -> List[IntegratedKnowledge]:
        """çµ±åˆå“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filtered = []
        
        for integration in integrations:
            # å“è³ªé–¾å€¤ãƒã‚§ãƒƒã‚¯
            if (integration.confidence_score >= self.integration_config["confidence_threshold"] and
                len(integration.key_insights) >= 1 and
                integration.synthesized_content):
                filtered.append(integration)
        
        # é‡è¤‡æ’é™¤ãƒ»å„ªå…ˆåº¦ã‚½ãƒ¼ãƒˆ
        filtered.sort(key=lambda x: x.confidence_score * x.novelty_score, reverse=True)
        
        return filtered[:self.integration_config["max_integration_scope"]]
    
    def _save_integrations(self, integrations: List[IntegratedKnowledge]):
        """çµ±åˆçµæœä¿å­˜"""
        try:
            if not integrations:
                return
            
            current_month = datetime.now().strftime("%Y%m")
            integration_file = self.integration_dir / f"integration_{current_month}.json"
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            if integration_file.exists():
                with open(integration_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                data = {
                    "month": current_month,
                    "integrated_knowledge": [],
                    "integration_statistics": {}
                }
            
            # æ–°ã—ã„çµ±åˆçµæœè¿½åŠ 
            for integration in integrations:
                integration_dict = asdict(integration)
                data["integrated_knowledge"].append(integration_dict)
                
                # ãƒ¡ãƒ¢ãƒªå†…ã«ã‚‚ä¿å­˜
                self.integrated_knowledge[integration.knowledge_id] = integration
            
            # çµ±è¨ˆæ›´æ–°
            data["integration_statistics"] = {
                "total_integrations": len(data["integrated_knowledge"]),
                "by_type": self._get_integration_type_stats(data["integrated_knowledge"]),
                "average_confidence": self._get_average_confidence(data["integrated_knowledge"]),
                "last_updated": datetime.now().isoformat()
            }
            
            # ä¿å­˜
            with open(integration_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
        except Exception as e:
            print(f"[çŸ¥è­˜çµ±åˆ] âŒ çµ±åˆçµæœä¿å­˜å¤±æ•—: {e}")
    
    def _get_integration_type_stats(self, integrations: List[Dict]) -> Dict[str, int]:
        """çµ±åˆã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ"""
        type_stats = defaultdict(int)
        for integration in integrations:
            integration_type = integration.get("integration_type", "unknown")
            type_stats[integration_type] += 1
        return dict(type_stats)
    
    def _get_average_confidence(self, integrations: List[Dict]) -> float:
        """å¹³å‡ä¿¡é ¼åº¦è¨ˆç®—"""
        if not integrations:
            return 0.0
        
        total_confidence = sum(integration.get("confidence_score", 0.0) for integration in integrations)
        return total_confidence / len(integrations)
    
    def get_integration_summary(self, session_ids: List[str] = None) -> Dict[str, Any]:
        """çµ±åˆã‚µãƒãƒªãƒ¼å–å¾—"""
        if session_ids:
            # ç‰¹å®šã‚»ãƒƒã‚·ãƒ§ãƒ³ã®çµ±åˆçµæœ
            relevant_integrations = [
                integration for integration in self.integrated_knowledge.values()
                if any(sid in integration.source_sessions for sid in session_ids)
            ]
        else:
            # å…¨çµ±åˆçµæœ
            relevant_integrations = list(self.integrated_knowledge.values())
        
        return {
            "total_integrations": len(relevant_integrations),
            "by_type": self._get_integration_type_stats([asdict(i) for i in relevant_integrations]),
            "average_confidence": np.mean([i.confidence_score for i in relevant_integrations]) if relevant_integrations else 0.0,
            "average_novelty": np.mean([i.novelty_score for i in relevant_integrations]) if relevant_integrations else 0.0,
            "top_insights": [
                {
                    "knowledge_id": integration.knowledge_id,
                    "type": integration.integration_type,
                    "insights": integration.key_insights[:2],
                    "confidence": integration.confidence_score
                }
                for integration in sorted(relevant_integrations, 
                                        key=lambda x: x.confidence_score * x.novelty_score, 
                                        reverse=True)[:5]
            ]
        }


# ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    print("=== KnowledgeIntegrationSystem ãƒ†ã‚¹ãƒˆ ===")
    
    system = KnowledgeIntegrationSystem()
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
    test_session_data = {
        "session_001": {
            "session_metadata": {"created_at": "2025-01-05T10:00:00"},
            "knowledge_items": [
                {
                    "item_id": "item_001",
                    "content": "AIéŸ³æ¥½ç”ŸæˆæŠ€è¡“ã¯Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŸºç›¤ã¨ã—ã¦ã„ã‚‹",
                    "categories": ["AIæŠ€è¡“", "éŸ³æ¥½ç”Ÿæˆ"],
                    "keywords": ["Transformer", "AI", "éŸ³æ¥½ç”Ÿæˆ"],
                    "entities": ["Transformer", "AIæŠ€è¡“"],
                    "importance_score": 0.8
                }
            ]
        },
        "session_002": {
            "session_metadata": {"created_at": "2025-01-06T15:00:00"},
            "knowledge_items": [
                {
                    "item_id": "item_002",
                    "content": "Transformerã«ã‚ˆã‚‹éŸ³æ¥½ç”Ÿæˆã¯é«˜å“è³ªãªçµæœã‚’ç”Ÿæˆã™ã‚‹",
                    "categories": ["AIæŠ€è¡“", "å“è³ªè©•ä¾¡"],
                    "keywords": ["Transformer", "é«˜å“è³ª", "éŸ³æ¥½ç”Ÿæˆ"],
                    "entities": ["Transformer", "å“è³ªè©•ä¾¡"],
                    "importance_score": 0.7
                }
            ]
        }
    }
    
    # çŸ¥è­˜çµ±åˆãƒ†ã‚¹ãƒˆ
    print("\nğŸ”— çŸ¥è­˜çµ±åˆãƒ†ã‚¹ãƒˆ:")
    integrations = system.integrate_multi_session_knowledge(
        session_ids=["session_001", "session_002"],
        session_data=test_session_data
    )
    
    print(f"\nğŸ“Š çµ±åˆçµæœ: {len(integrations)}ä»¶")
    for integration in integrations:
        print(f"\nçµ±åˆID: {integration.knowledge_id}")
        print(f"ã‚¿ã‚¤ãƒ—: {integration.integration_type}")
        print(f"å†…å®¹: {integration.synthesized_content}")
        print(f"æ´å¯Ÿ: {integration.key_insights}")
        print(f"ä¿¡é ¼åº¦: {integration.confidence_score:.2f}")
    
    # çµ±åˆã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“ˆ çµ±åˆã‚µãƒãƒªãƒ¼:")
    summary = system.get_integration_summary(["session_001", "session_002"])
    print(f"  ç·çµ±åˆæ•°: {summary['total_integrations']}")
    print(f"  ã‚¿ã‚¤ãƒ—åˆ¥: {summary['by_type']}")
    print(f"  å¹³å‡ä¿¡é ¼åº¦: {summary['average_confidence']:.2f}")
    print(f"  å¹³å‡æ–°è¦æ€§: {summary['average_novelty']:.2f}")