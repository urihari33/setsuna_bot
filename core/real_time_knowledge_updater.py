#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŸ¥è­˜æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ  - Phase 2Eå®Ÿè£…
ä¼šè©±ä¸­ã®æ–°æƒ…å ±è‡ªå‹•æ¤œå‡ºãƒ»æŠ½å‡ºãƒ»å‹•çš„çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
from datetime import datetime, timedelta
import re
import hashlib
import difflib

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# é–¢é€£ã‚·ã‚¹ãƒ†ãƒ 
try:
    from core.knowledge_graph_system import KnowledgeGraphSystem
    from core.semantic_search_engine import SemanticSearchEngine
    DEPENDENCIES_AVAILABLE = True
except ImportError:
    DEPENDENCIES_AVAILABLE = False

# Windowsãƒ‘ã‚¹è¨­å®š
if os.name == 'nt':
    DATA_DIR = Path("D:/setsuna_bot/youtube_knowledge_system/data")
    UPDATE_CACHE_DIR = Path("D:/setsuna_bot/realtime_update_cache")
else:
    DATA_DIR = Path("/mnt/d/setsuna_bot/youtube_knowledge_system/data")
    UPDATE_CACHE_DIR = Path("/mnt/d/setsuna_bot/realtime_update_cache")

UPDATE_CACHE_DIR.mkdir(parents=True, exist_ok=True)

@dataclass
class NewInformation:
    """æ–°æƒ…å ±ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    info_id: str
    content: str
    info_type: str              # "artist", "song", "genre", "fact", "opinion", "relationship"
    confidence: float
    source_context: str
    extraction_method: str
    related_entities: List[str]
    validation_status: str      # "pending", "validated", "rejected"
    detected_at: str

@dataclass
class KnowledgeUpdate:
    """çŸ¥è­˜æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    update_id: str
    target_entity: str
    update_type: str            # "add", "modify", "enhance", "correct"
    old_value: Optional[Any]
    new_value: Any
    confidence: float
    supporting_evidence: List[str]
    impact_score: float         # æ›´æ–°ã®é‡è¦åº¦
    applied_at: Optional[str]

@dataclass
class ConflictResolution:
    """çŸ›ç›¾è§£æ±ºãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    conflict_id: str
    conflicting_info: List[str]
    resolution_strategy: str    # "merge", "replace", "flag", "ignore"
    resolution_reasoning: str
    confidence: float
    resolved_at: str

class RealTimeKnowledgeUpdater:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŸ¥è­˜æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
        self.knowledge_db_path = DATA_DIR / "unified_knowledge_db.json"
        self.new_information_path = UPDATE_CACHE_DIR / "new_information.json"
        self.knowledge_updates_path = UPDATE_CACHE_DIR / "knowledge_updates.json"
        self.conflict_resolutions_path = UPDATE_CACHE_DIR / "conflict_resolutions.json"
        self.update_log_path = UPDATE_CACHE_DIR / "update_log.json"
        
        # ä¾å­˜ã‚·ã‚¹ãƒ†ãƒ 
        if DEPENDENCIES_AVAILABLE:
            self.knowledge_graph = KnowledgeGraphSystem()
            self.semantic_search = SemanticSearchEngine()
        else:
            self.knowledge_graph = None
            self.semantic_search = None
            print("[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âš ï¸ ä¾å­˜ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # ãƒ‡ãƒ¼ã‚¿
        self.knowledge_db = {}
        self.new_information = deque(maxlen=1000)
        self.pending_updates = {}
        self.conflict_resolutions = {}
        self.update_log = []
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ç”¨
        self.current_session_info = []
        self.information_buffer = deque(maxlen=50)
        self.last_update_time = datetime.now()
        
        # æ›´æ–°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.confidence_threshold = 0.7    # è‡ªå‹•æ›´æ–°ä¿¡é ¼åº¦é–¾å€¤
        self.validation_threshold = 0.9    # æ¤œè¨¼è¦æ±‚é–¾å€¤
        self.conflict_threshold = 0.5      # çŸ›ç›¾æ¤œå‡ºé–¾å€¤
        self.update_batch_size = 10        # ãƒãƒƒãƒæ›´æ–°ã‚µã‚¤ã‚º
        
        # æƒ…å ±æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        self.extraction_patterns = self._build_extraction_patterns()
        self.entity_patterns = self._build_entity_patterns()
        self.relationship_patterns = self._build_relationship_patterns()
        
        # çµ±è¨ˆæƒ…å ±
        self.update_statistics = {
            "total_info_detected": 0,
            "total_updates_applied": 0,
            "conflicts_resolved": 0,
            "validation_requests": 0,
            "last_update_batch": None
        }
        
        self._load_existing_data()
        print("[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŸ¥è­˜æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _build_extraction_patterns(self) -> Dict[str, List[str]]:
        """æƒ…å ±æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³æ§‹ç¯‰"""
        return {
            "artist_info": [
                r"(.+)(ã¯|ãŒ)(ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ|æ­Œæ‰‹|ãƒŸãƒ¥ãƒ¼ã‚¸ã‚·ãƒ£ãƒ³|ãƒãƒ³ãƒ‰)",
                r"(.+)(ã®)(æ–°æ›²|ã‚¢ãƒ«ãƒãƒ |ã‚·ãƒ³ã‚°ãƒ«)",
                r"(.+)(ãŒ)(æ­Œã£ã¦|ä½œã£ã¦|åˆ¶ä½œã—ãŸ)"
            ],
            "song_info": [
                r"(.+)(ã¨ã„ã†|ã£ã¦ã„ã†)(æ›²|æ­Œ|æ¥½æ›²)",
                r"(.+)(ã®)(ãƒ†ãƒ¼ãƒ|æ­Œè©|ãƒ¡ãƒ­ãƒ‡ã‚£)",
                r"(.+)(ã¯|ãŒ)(äººæ°—|æœ‰å|è©±é¡Œ)"
            ],
            "genre_info": [
                r"(.+)(ã¯|ãŒ)(.+)(ç³»|é¢¨|ã£ã½ã„)",
                r"(.+)(ã‚¸ãƒ£ãƒ³ãƒ«|ã‚¹ã‚¿ã‚¤ãƒ«|ã‚«ãƒ†ã‚´ãƒª)",
                r"(.+)(ãƒ­ãƒƒã‚¯|ãƒãƒƒãƒ—|ã‚¯ãƒ©ã‚·ãƒƒã‚¯|ãƒœã‚«ãƒ­)"
            ],
            "factual_info": [
                r"(.+)(ã¯|ãŒ)(ç™ºå£²|ãƒªãƒªãƒ¼ã‚¹|å…¬é–‹)ã•ã‚ŒãŸ",
                r"(.+)(å¹´|æœˆ|æ—¥)ã«(.+)",
                r"(.+)(å›|ä¸‡|å„„)(å†ç”Ÿ|è¦–è´|ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰)"
            ],
            "opinion_info": [
                r"(.+)(ã¨æ€ã†|æ„Ÿã˜ã‚‹|è€ƒãˆã‚‹)",
                r"(.+)(å¥½ã|å«Œã„|æ°—ã«å…¥ã£ãŸ)",
                r"(.+)(ãŠã™ã™ã‚|æ¨è–¦|ææ¡ˆ)"
            ]
        }
    
    def _build_entity_patterns(self) -> Dict[str, List[str]]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³æ§‹ç¯‰"""
        return {
            "artist_names": [
                r"[ã‚¡-ãƒ¶ãƒ¼]{2,}",           # ã‚«ã‚¿ã‚«ãƒŠ2æ–‡å­—ä»¥ä¸Š
                r"[A-Za-z]{2,}",            # è‹±å­—2æ–‡å­—ä»¥ä¸Š
                r"[ä¸€-é¾¯]{2,}"              # æ¼¢å­—2æ–‡å­—ä»¥ä¸Š
            ],
            "song_titles": [
                r"ã€Œ(.+?)ã€",               # éµæ‹¬å¼§
                r"ã€(.+?)ã€",               # äºŒé‡éµæ‹¬å¼§  
                r"\"(.+?)\"",               # ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆ
                r"'(.+?)'"                  # ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆ
            ],
            "temporal_expressions": [
                r"\d{4}å¹´",                 # å¹´
                r"\d{1,2}æœˆ",               # æœˆ
                r"\d{1,2}æ—¥",               # æ—¥
                r"æœ€è¿‘|ä»Šå¹´|å»å¹´|æ˜”"         # ç›¸å¯¾æ™‚é–“
            ]
        }
    
    def _build_relationship_patterns(self) -> Dict[str, List[str]]:
        """é–¢ä¿‚æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³æ§‹ç¯‰"""
        return {
            "artist_song": [
                r"(.+)(ã®|ãŒæ­Œã†|ãŒä½œã£ãŸ)(.+)",
                r"(.+)(ã«ã‚ˆã‚‹|featuring)(.+)",
                r"(.+)(Ã—|x|feat\.?)(.+)"
            ],
            "genre_classification": [
                r"(.+)(ã¯|ãŒ)(.+)(ç³»|é¢¨|ã‚¸ãƒ£ãƒ³ãƒ«)",
                r"(.+)(ãƒ­ãƒƒã‚¯|ãƒãƒƒãƒ—|ã‚¸ãƒ£ã‚º|ã‚¯ãƒ©ã‚·ãƒƒã‚¯)"
            ],
            "temporal_relation": [
                r"(.+)(ã®å¾Œã«|ã®å‰ã«|ã¨åŒæ™‚æœŸã«)(.+)",
                r"(.+)(å¹´ã®|æœˆã®)(.+)"
            ],
            "similarity_relation": [
                r"(.+)(ã«ä¼¼ã¦ã‚‹|ã¿ãŸã„ãª|ã£ã½ã„)(.+)",
                r"(.+)(ã¨|ã‚ˆã‚Š)(.+)(ãŒä¼¼ã¦ã‚‹|ãŒè¿‘ã„)"
            ]
        }
    
    def _load_existing_data(self):
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰"""
        # çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        try:
            if self.knowledge_db_path.exists():
                with open(self.knowledge_db_path, 'r', encoding='utf-8') as f:
                    self.knowledge_db = json.load(f)
                print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] ğŸ“Š çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰")
        except Exception as e:
            print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âš ï¸ çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ–°æƒ…å ±
        try:
            if self.new_information_path.exists():
                with open(self.new_information_path, 'r', encoding='utf-8') as f:
                    info_data = json.load(f)
                    info_list = info_data.get("information", [])
                    self.new_information = deque(
                        [NewInformation(**info) for info in info_list],
                        maxlen=1000
                    )
                print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] ğŸ“Š {len(self.new_information)}ä»¶ã®æ–°æƒ…å ±ã‚’ãƒ­ãƒ¼ãƒ‰")
        except Exception as e:
            print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âš ï¸ æ–°æƒ…å ±ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ä¿ç•™ä¸­æ›´æ–°
        try:
            if self.knowledge_updates_path.exists():
                with open(self.knowledge_updates_path, 'r', encoding='utf-8') as f:
                    updates_data = json.load(f)
                    self.pending_updates = {
                        uid: KnowledgeUpdate(**update) 
                        for uid, update in updates_data.get("updates", {}).items()
                    }
                print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] ğŸ“Š {len(self.pending_updates)}ä»¶ã®ä¿ç•™æ›´æ–°ã‚’ãƒ­ãƒ¼ãƒ‰")
        except Exception as e:
            print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âš ï¸ ä¿ç•™æ›´æ–°ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
    
    def process_user_input(self, user_input: str, context: Optional[str] = None) -> List[NewInformation]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å‡¦ç†"""
        # æƒ…å ±ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        input_data = {
            "timestamp": datetime.now().isoformat(),
            "content": user_input,
            "context": context or "",
            "processed": False
        }
        self.information_buffer.append(input_data)
        
        # æ–°æƒ…å ±æ¤œå‡º
        detected_info = self._detect_new_information(user_input, context)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã«è¿½åŠ 
        self.current_session_info.extend(detected_info)
        
        # æ–°æƒ…å ±ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        for info in detected_info:
            self.new_information.append(info)
        
        # å‡¦ç†æ¸ˆã¿ãƒãƒ¼ã‚¯
        input_data["processed"] = True
        
        # çµ±è¨ˆæ›´æ–°
        self.update_statistics["total_info_detected"] += len(detected_info)
        
        return detected_info
    
    def _detect_new_information(self, text: str, context: Optional[str]) -> List[NewInformation]:
        """æ–°æƒ…å ±æ¤œå‡º"""
        detected_info = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥æƒ…å ±æŠ½å‡º
        for info_type, patterns in self.extraction_patterns.items():
            info_list = self._extract_by_patterns(text, patterns, info_type, context)
            detected_info.extend(info_list)
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
        entities = self._extract_entities(text)
        for entity in entities:
            info = self._create_entity_information(entity, text, context)
            if info:
                detected_info.append(info)
        
        # é–¢ä¿‚æ€§æŠ½å‡º
        relationships = self._extract_relationships(text)
        for relationship in relationships:
            info = self._create_relationship_information(relationship, text, context)
            if info:
                detected_info.append(info)
        
        return detected_info
    
    def _extract_by_patterns(self, text: str, patterns: List[str], info_type: str, context: Optional[str]) -> List[NewInformation]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹æŠ½å‡º"""
        extracted = []
        
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                content = match.group(0)
                confidence = self._calculate_extraction_confidence(content, pattern, text)
                
                if confidence > 0.3:  # æœ€å°ä¿¡é ¼åº¦
                    info_id = f"info_{info_type}_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
                    
                    info = NewInformation(
                        info_id=info_id,
                        content=content,
                        info_type=info_type,
                        confidence=confidence,
                        source_context=text[:200],
                        extraction_method=f"pattern_{pattern[:20]}",
                        related_entities=self._extract_entities_from_match(match),
                        validation_status="pending",
                        detected_at=datetime.now().isoformat()
                    )
                    extracted.append(info)
        
        return extracted
    
    def _calculate_extraction_confidence(self, content: str, pattern: str, full_text: str) -> float:
        """æŠ½å‡ºä¿¡é ¼åº¦è¨ˆç®—"""
        confidence = 0.5  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        
        # é•·ã•ã«ã‚ˆã‚‹èª¿æ•´
        if len(content) > 10:
            confidence += 0.1
        if len(content) > 20:
            confidence += 0.1
        
        # æ—¢çŸ¥ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ã®ç…§åˆ
        if self._contains_known_entities(content):
            confidence += 0.2
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸€è²«æ€§
        if self._is_contextually_consistent(content, full_text):
            confidence += 0.2
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹ç•°æ€§
        if len(pattern) > 30:  # è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            confidence += 0.1
        
        return min(1.0, confidence)
    
    def _contains_known_entities(self, text: str) -> bool:
        """æ—¢çŸ¥ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å«æœ‰ç¢ºèª"""
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã¨ç…§åˆ
        videos = self.knowledge_db.get("videos", {})
        
        for video_data in videos.values():
            metadata = video_data.get("metadata", {})
            title = metadata.get("title", "")
            channel = metadata.get("channel_title", "")
            
            if title.lower() in text.lower() or channel.lower() in text.lower():
                return True
        
        return False
    
    def _is_contextually_consistent(self, content: str, full_text: str) -> bool:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸€è²«æ€§ç¢ºèª"""
        # éŸ³æ¥½é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ç¢ºèª
        music_keywords = ["éŸ³æ¥½", "æ›²", "æ­Œ", "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ", "ãƒãƒ³ãƒ‰", "ã‚¢ãƒ«ãƒãƒ ", "ã‚·ãƒ³ã‚°ãƒ«"]
        
        content_has_music = any(keyword in content for keyword in music_keywords)
        context_has_music = any(keyword in full_text for keyword in music_keywords)
        
        return content_has_music or context_has_music
    
    def _extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º"""
        entities = []
        
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    entity = {
                        "type": entity_type,
                        "text": match.group(1) if match.groups() else match.group(0),
                        "position": match.span(),
                        "confidence": 0.7
                    }
                    entities.append(entity)
        
        return entities
    
    def _extract_entities_from_match(self, match) -> List[str]:
        """ãƒãƒƒãƒã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º"""
        entities = []
        
        for group in match.groups():
            if group and len(group.strip()) > 1:
                entities.append(group.strip())
        
        return entities
    
    def _create_entity_information(self, entity: Dict[str, Any], source_text: str, context: Optional[str]) -> Optional[NewInformation]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æƒ…å ±ä½œæˆ"""
        entity_text = entity["text"]
        entity_type = entity["type"]
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if self._is_duplicate_entity(entity_text, entity_type):
            return None
        
        info_id = f"entity_{entity_type}_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        
        return NewInformation(
            info_id=info_id,
            content=f"{entity_type}: {entity_text}",
            info_type="entity",
            confidence=entity["confidence"],
            source_context=source_text[:200],
            extraction_method="entity_extraction",
            related_entities=[entity_text],
            validation_status="pending",
            detected_at=datetime.now().isoformat()
        )
    
    def _is_duplicate_entity(self, entity_text: str, entity_type: str) -> bool:
        """é‡è¤‡ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ç¢ºèª"""
        # æ—¢å­˜ã®æ–°æƒ…å ±ã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        for info in self.new_information:
            if (info.info_type == "entity" and 
                entity_text.lower() in info.content.lower()):
                return True
        
        return False
    
    def _extract_relationships(self, text: str) -> List[Dict[str, Any]]:
        """é–¢ä¿‚æ€§æŠ½å‡º"""
        relationships = []
        
        for rel_type, patterns in self.relationship_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    if len(match.groups()) >= 2:
                        relationship = {
                            "type": rel_type,
                            "entity1": match.group(1).strip(),
                            "entity2": match.group(3).strip() if len(match.groups()) >= 3 else match.group(2).strip(),
                            "relation": match.group(2).strip() if len(match.groups()) >= 3 else "related_to",
                            "confidence": 0.6
                        }
                        relationships.append(relationship)
        
        return relationships
    
    def _create_relationship_information(self, relationship: Dict[str, Any], source_text: str, context: Optional[str]) -> Optional[NewInformation]:
        """é–¢ä¿‚æ€§æƒ…å ±ä½œæˆ"""
        rel_type = relationship["type"]
        entity1 = relationship["entity1"]
        entity2 = relationship["entity2"]
        relation = relationship["relation"]
        
        # é–¢ä¿‚æ€§ã®æœ‰æ„æ€§ãƒã‚§ãƒƒã‚¯
        if len(entity1) < 2 or len(entity2) < 2:
            return None
        
        info_id = f"relationship_{rel_type}_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        content = f"{entity1} {relation} {entity2}"
        
        return NewInformation(
            info_id=info_id,
            content=content,
            info_type="relationship",
            confidence=relationship["confidence"],
            source_context=source_text[:200],
            extraction_method="relationship_extraction",
            related_entities=[entity1, entity2],
            validation_status="pending",
            detected_at=datetime.now().isoformat()
        )
    
    def generate_knowledge_updates(self) -> List[KnowledgeUpdate]:
        """çŸ¥è­˜æ›´æ–°ç”Ÿæˆ"""
        print("[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] ğŸ”„ çŸ¥è­˜æ›´æ–°ã‚’ç”Ÿæˆä¸­...")
        
        updates = []
        
        # æ¤œè¨¼æ¸ˆã¿æ–°æƒ…å ±ã‹ã‚‰æ›´æ–°ã‚’ç”Ÿæˆ
        validated_info = [
            info for info in self.new_information 
            if info.validation_status == "validated" or info.confidence > self.confidence_threshold
        ]
        
        for info in validated_info:
            update = self._create_knowledge_update(info)
            if update:
                updates.append(update)
        
        # ä¿ç•™æ›´æ–°ã«è¿½åŠ 
        for update in updates:
            self.pending_updates[update.update_id] = update
        
        print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âœ… {len(updates)}ä»¶ã®çŸ¥è­˜æ›´æ–°ã‚’ç”Ÿæˆ")
        return updates
    
    def _create_knowledge_update(self, info: NewInformation) -> Optional[KnowledgeUpdate]:
        """çŸ¥è­˜æ›´æ–°ä½œæˆ"""
        # æƒ…å ±ã‚¿ã‚¤ãƒ—åˆ¥ã®æ›´æ–°æˆ¦ç•¥
        if info.info_type == "artist_info":
            return self._create_artist_update(info)
        elif info.info_type == "song_info":
            return self._create_song_update(info)
        elif info.info_type == "entity":
            return self._create_entity_update(info)
        elif info.info_type == "relationship":
            return self._create_relationship_update(info)
        else:
            return self._create_generic_update(info)
    
    def _create_artist_update(self, info: NewInformation) -> Optional[KnowledgeUpdate]:
        """ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæ›´æ–°ä½œæˆ"""
        entities = info.related_entities
        if not entities:
            return None
        
        artist_name = entities[0]
        
        # æ—¢å­˜ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæ¤œç´¢
        target_video = self._find_video_by_artist(artist_name)
        
        if target_video:
            update_id = f"update_artist_{artist_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            return KnowledgeUpdate(
                update_id=update_id,
                target_entity=target_video,
                update_type="enhance",
                old_value=None,
                new_value={"artist_info": info.content},
                confidence=info.confidence,
                supporting_evidence=[info.source_context],
                impact_score=0.6,
                applied_at=None
            )
        
        return None
    
    def _create_song_update(self, info: NewInformation) -> Optional[KnowledgeUpdate]:
        """æ¥½æ›²æ›´æ–°ä½œæˆ"""
        # æ¥½æ›²ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
        song_titles = self._extract_song_titles_from_content(info.content)
        
        for title in song_titles:
            target_video = self._find_video_by_title(title)
            
            if target_video:
                update_id = f"update_song_{title}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                return KnowledgeUpdate(
                    update_id=update_id,
                    target_entity=target_video,
                    update_type="enhance",
                    old_value=None,
                    new_value={"song_info": info.content},
                    confidence=info.confidence,
                    supporting_evidence=[info.source_context],
                    impact_score=0.7,
                    applied_at=None
                )
        
        return None
    
    def _create_entity_update(self, info: NewInformation) -> KnowledgeUpdate:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ›´æ–°ä½œæˆ"""
        update_id = f"update_entity_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        return KnowledgeUpdate(
            update_id=update_id,
            target_entity="new_entity",
            update_type="add",
            old_value=None,
            new_value={"entity": info.content, "entities": info.related_entities},
            confidence=info.confidence,
            supporting_evidence=[info.source_context],
            impact_score=0.4,
            applied_at=None
        )
    
    def _create_relationship_update(self, info: NewInformation) -> KnowledgeUpdate:
        """é–¢ä¿‚æ€§æ›´æ–°ä½œæˆ"""
        update_id = f"update_relationship_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        return KnowledgeUpdate(
            update_id=update_id,
            target_entity="knowledge_graph",
            update_type="add",
            old_value=None,
            new_value={"relationship": info.content, "entities": info.related_entities},
            confidence=info.confidence,
            supporting_evidence=[info.source_context],
            impact_score=0.5,
            applied_at=None
        )
    
    def _create_generic_update(self, info: NewInformation) -> KnowledgeUpdate:
        """æ±ç”¨æ›´æ–°ä½œæˆ"""
        update_id = f"update_generic_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        return KnowledgeUpdate(
            update_id=update_id,
            target_entity="general_knowledge",
            update_type="add",
            old_value=None,
            new_value={"info": info.content},
            confidence=info.confidence,
            supporting_evidence=[info.source_context],
            impact_score=0.3,
            applied_at=None
        )
    
    def _find_video_by_artist(self, artist_name: str) -> Optional[str]:
        """ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåã§å‹•ç”»æ¤œç´¢"""
        videos = self.knowledge_db.get("videos", {})
        
        for video_id, video_data in videos.items():
            metadata = video_data.get("metadata", {})
            channel_title = metadata.get("channel_title", "")
            custom_info = video_data.get("custom_info", {})
            manual_artist = custom_info.get("manual_artist", "")
            
            if (artist_name.lower() in channel_title.lower() or 
                artist_name.lower() in manual_artist.lower()):
                return video_id
        
        return None
    
    def _find_video_by_title(self, title: str) -> Optional[str]:
        """ã‚¿ã‚¤ãƒˆãƒ«ã§å‹•ç”»æ¤œç´¢"""
        videos = self.knowledge_db.get("videos", {})
        
        for video_id, video_data in videos.items():
            metadata = video_data.get("metadata", {})
            video_title = metadata.get("title", "")
            custom_info = video_data.get("custom_info", {})
            manual_title = custom_info.get("manual_title", "")
            
            # é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
            similarity1 = difflib.SequenceMatcher(None, title.lower(), video_title.lower()).ratio()
            similarity2 = difflib.SequenceMatcher(None, title.lower(), manual_title.lower()).ratio()
            
            if similarity1 > 0.8 or similarity2 > 0.8:
                return video_id
        
        return None
    
    def _extract_song_titles_from_content(self, content: str) -> List[str]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ¥½æ›²ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
        titles = []
        
        # å¼•ç”¨ç¬¦ã§å›²ã¾ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        quote_patterns = [r"ã€Œ(.+?)ã€", r"ã€(.+?)ã€", r"\"(.+?)\"", r"'(.+?)'"]
        
        for pattern in quote_patterns:
            matches = re.findall(pattern, content)
            titles.extend(matches)
        
        return titles
    
    def apply_knowledge_updates(self, batch_size: Optional[int] = None) -> int:
        """çŸ¥è­˜æ›´æ–°é©ç”¨"""
        if batch_size is None:
            batch_size = self.update_batch_size
        
        print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] ğŸ”„ çŸ¥è­˜æ›´æ–°ã‚’é©ç”¨ä¸­... (ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size})")
        
        applied_count = 0
        
        # é«˜ä¿¡é ¼åº¦ã®æ›´æ–°ã‹ã‚‰é©ç”¨
        sorted_updates = sorted(
            self.pending_updates.values(),
            key=lambda x: x.confidence,
            reverse=True
        )
        
        for update in sorted_updates[:batch_size]:
            if self._apply_single_update(update):
                applied_count += 1
        
        # é©ç”¨æ¸ˆã¿æ›´æ–°ã‚’å‰Šé™¤
        applied_updates = [u for u in sorted_updates[:batch_size] if u.applied_at]
        for update in applied_updates:
            if update.update_id in self.pending_updates:
                del self.pending_updates[update.update_id]
        
        # çµ±è¨ˆæ›´æ–°
        self.update_statistics["total_updates_applied"] += applied_count
        self.update_statistics["last_update_batch"] = datetime.now().isoformat()
        
        print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âœ… {applied_count}ä»¶ã®æ›´æ–°ã‚’é©ç”¨å®Œäº†")
        return applied_count
    
    def _apply_single_update(self, update: KnowledgeUpdate) -> bool:
        """å˜ä¸€æ›´æ–°é©ç”¨"""
        try:
            if update.target_entity == "knowledge_graph":
                return self._apply_graph_update(update)
            elif update.target_entity == "new_entity":
                return self._apply_entity_update(update)
            elif update.target_entity in self.knowledge_db.get("videos", {}):
                return self._apply_video_update(update)
            else:
                return self._apply_general_update(update)
        except Exception as e:
            print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âš ï¸ æ›´æ–°é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _apply_graph_update(self, update: KnowledgeUpdate) -> bool:
        """ã‚°ãƒ©ãƒ•æ›´æ–°é©ç”¨"""
        if self.knowledge_graph:
            # é–¢ä¿‚æ€§æƒ…å ±ã‚’çŸ¥è­˜ã‚°ãƒ©ãƒ•ã«è¿½åŠ 
            new_value = update.new_value
            relationship = new_value.get("relationship", "")
            entities = new_value.get("entities", [])
            
            # ç°¡æ˜“å®Ÿè£…ï¼šãƒ­ã‚°è¨˜éŒ²ã®ã¿
            self.update_log.append({
                "type": "graph_update",
                "relationship": relationship,
                "entities": entities,
                "timestamp": datetime.now().isoformat()
            })
            
            update.applied_at = datetime.now().isoformat()
            return True
        
        return False
    
    def _apply_entity_update(self, update: KnowledgeUpdate) -> bool:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ›´æ–°é©ç”¨"""
        # æ–°ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ä¸€æ™‚çš„ãªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ä¿å­˜
        if "entities" not in self.knowledge_db:
            self.knowledge_db["entities"] = {}
        
        new_value = update.new_value
        entity_content = new_value.get("entity", "")
        entities = new_value.get("entities", [])
        
        for entity in entities:
            entity_id = hashlib.md5(entity.encode()).hexdigest()[:8]
            self.knowledge_db["entities"][entity_id] = {
                "name": entity,
                "content": entity_content,
                "confidence": update.confidence,
                "added_at": datetime.now().isoformat()
            }
        
        update.applied_at = datetime.now().isoformat()
        return True
    
    def _apply_video_update(self, update: KnowledgeUpdate) -> bool:
        """å‹•ç”»æ›´æ–°é©ç”¨"""
        video_id = update.target_entity
        videos = self.knowledge_db.get("videos", {})
        
        if video_id in videos:
            video_data = videos[video_id]
            new_value = update.new_value
            
            # ã‚«ã‚¹ã‚¿ãƒ æƒ…å ±ã«è¿½åŠ 
            if "custom_info" not in video_data:
                video_data["custom_info"] = {}
            
            if "realtime_updates" not in video_data["custom_info"]:
                video_data["custom_info"]["realtime_updates"] = []
            
            video_data["custom_info"]["realtime_updates"].append({
                "content": new_value,
                "confidence": update.confidence,
                "added_at": datetime.now().isoformat()
            })
            
            update.applied_at = datetime.now().isoformat()
            return True
        
        return False
    
    def _apply_general_update(self, update: KnowledgeUpdate) -> bool:
        """æ±ç”¨æ›´æ–°é©ç”¨"""
        # æ±ç”¨æ›´æ–°ãƒ­ã‚°ã«è¨˜éŒ²
        self.update_log.append({
            "type": "general_update",
            "update_id": update.update_id,
            "content": update.new_value,
            "confidence": update.confidence,
            "timestamp": datetime.now().isoformat()
        })
        
        update.applied_at = datetime.now().isoformat()
        return True
    
    def detect_conflicts(self) -> List[ConflictResolution]:
        """çŸ›ç›¾æ¤œå‡º"""
        print("[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âš™ï¸ çŸ›ç›¾ã‚’æ¤œå‡ºä¸­...")
        
        conflicts = []
        
        # æ–°æƒ…å ±é–“ã®çŸ›ç›¾
        info_conflicts = self._detect_information_conflicts()
        conflicts.extend(info_conflicts)
        
        # æ—¢å­˜çŸ¥è­˜ã¨ã®çŸ›ç›¾
        knowledge_conflicts = self._detect_knowledge_conflicts()
        conflicts.extend(knowledge_conflicts)
        
        # çŸ›ç›¾è§£æ±ºç­–ã‚’ç”Ÿæˆ
        for conflict in conflicts:
            resolution = self._generate_conflict_resolution(conflict)
            if resolution:
                self.conflict_resolutions[resolution.conflict_id] = resolution
        
        print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âœ… {len(conflicts)}ä»¶ã®çŸ›ç›¾ã‚’æ¤œå‡º")
        return conflicts
    
    def _detect_information_conflicts(self) -> List[Dict[str, Any]]:
        """æƒ…å ±é–“çŸ›ç›¾æ¤œå‡º"""
        conflicts = []
        
        # åŒä¸€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«é–¢ã™ã‚‹çŸ›ç›¾ã™ã‚‹æƒ…å ±
        entity_info = defaultdict(list)
        
        for info in self.new_information:
            for entity in info.related_entities:
                entity_info[entity].append(info)
        
        for entity, info_list in entity_info.items():
            if len(info_list) > 1:
                # çŸ›ç›¾ãƒã‚§ãƒƒã‚¯
                for i in range(len(info_list)):
                    for j in range(i + 1, len(info_list)):
                        if self._are_conflicting(info_list[i], info_list[j]):
                            conflicts.append({
                                "type": "information_conflict",
                                "entity": entity,
                                "info1": info_list[i],
                                "info2": info_list[j]
                            })
        
        return conflicts
    
    def _detect_knowledge_conflicts(self) -> List[Dict[str, Any]]:
        """æ—¢å­˜çŸ¥è­˜çŸ›ç›¾æ¤œå‡º"""
        conflicts = []
        
        # æ–°æƒ…å ±ã¨æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çŸ›ç›¾
        for info in self.new_information:
            for entity in info.related_entities:
                existing_info = self._find_existing_entity_info(entity)
                if existing_info and self._conflicts_with_existing(info, existing_info):
                    conflicts.append({
                        "type": "knowledge_conflict",
                        "entity": entity,
                        "new_info": info,
                        "existing_info": existing_info
                    })
        
        return conflicts
    
    def _are_conflicting(self, info1: NewInformation, info2: NewInformation) -> bool:
        """æƒ…å ±çŸ›ç›¾åˆ¤å®š"""
        # åŒã˜ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«å¯¾ã™ã‚‹çŸ›ç›¾ã™ã‚‹è¨˜è¿°ã‚’ãƒã‚§ãƒƒã‚¯
        content1 = info1.content.lower()
        content2 = info2.content.lower()
        
        # å¯¾ç«‹ã™ã‚‹è¡¨ç¾
        contradictions = [
            ("å¥½ã", "å«Œã„"),
            ("è‰¯ã„", "æ‚ªã„"),
            ("äººæ°—", "ä¸äººæ°—"),
            ("æ–°ã—ã„", "å¤ã„"),
            ("æœ‰å", "ç„¡å")
        ]
        
        for pos, neg in contradictions:
            if pos in content1 and neg in content2:
                return True
            if neg in content1 and pos in content2:
                return True
        
        return False
    
    def _find_existing_entity_info(self, entity: str) -> Optional[Dict[str, Any]]:
        """æ—¢å­˜ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æƒ…å ±æ¤œç´¢"""
        videos = self.knowledge_db.get("videos", {})
        
        for video_data in videos.values():
            metadata = video_data.get("metadata", {})
            if entity.lower() in metadata.get("title", "").lower():
                return metadata
            if entity.lower() in metadata.get("channel_title", "").lower():
                return metadata
        
        return None
    
    def _conflicts_with_existing(self, new_info: NewInformation, existing_info: Dict[str, Any]) -> bool:
        """æ—¢å­˜æƒ…å ±ã¨ã®çŸ›ç›¾åˆ¤å®š"""
        # ç°¡æ˜“å®Ÿè£…ï¼šåŸºæœ¬çš„ãªçŸ›ç›¾ãƒã‚§ãƒƒã‚¯
        new_content = new_info.content.lower()
        existing_title = existing_info.get("title", "").lower()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®å¤§å¹…ãªç›¸é•
        if new_info.info_type == "song_info":
            similarity = difflib.SequenceMatcher(None, new_content, existing_title).ratio()
            return similarity < 0.3
        
        return False
    
    def _generate_conflict_resolution(self, conflict: Dict[str, Any]) -> Optional[ConflictResolution]:
        """çŸ›ç›¾è§£æ±ºç­–ç”Ÿæˆ"""
        conflict_id = f"conflict_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        
        conflict_type = conflict["type"]
        
        if conflict_type == "information_conflict":
            return self._resolve_information_conflict(conflict_id, conflict)
        elif conflict_type == "knowledge_conflict":
            return self._resolve_knowledge_conflict(conflict_id, conflict)
        
        return None
    
    def _resolve_information_conflict(self, conflict_id: str, conflict: Dict[str, Any]) -> ConflictResolution:
        """æƒ…å ±çŸ›ç›¾è§£æ±º"""
        info1 = conflict["info1"]
        info2 = conflict["info2"]
        
        # ä¿¡é ¼åº¦æ¯”è¼ƒ
        if info1.confidence > info2.confidence + 0.2:
            strategy = "replace"
            reasoning = f"é«˜ä¿¡é ¼åº¦æƒ…å ±({info1.confidence:.2f})ã‚’æ¡ç”¨"
        elif info2.confidence > info1.confidence + 0.2:
            strategy = "replace"
            reasoning = f"é«˜ä¿¡é ¼åº¦æƒ…å ±({info2.confidence:.2f})ã‚’æ¡ç”¨"
        else:
            strategy = "merge"
            reasoning = "ä¸¡æƒ…å ±ã‚’ãƒãƒ¼ã‚¸ã—ã¦ä¿æŒ"
        
        return ConflictResolution(
            conflict_id=conflict_id,
            conflicting_info=[info1.info_id, info2.info_id],
            resolution_strategy=strategy,
            resolution_reasoning=reasoning,
            confidence=0.7,
            resolved_at=datetime.now().isoformat()
        )
    
    def _resolve_knowledge_conflict(self, conflict_id: str, conflict: Dict[str, Any]) -> ConflictResolution:
        """çŸ¥è­˜çŸ›ç›¾è§£æ±º"""
        new_info = conflict["new_info"]
        
        # æ—¢å­˜çŸ¥è­˜ã®ä¿è­·ã‚’å„ªå…ˆ
        strategy = "flag"
        reasoning = "æ—¢å­˜çŸ¥è­˜ã‚’ä¿è­·ã—ã€æ–°æƒ…å ±ã«è¦æ¤œè¨¼ãƒ•ãƒ©ã‚°ã‚’è¨­å®š"
        
        return ConflictResolution(
            conflict_id=conflict_id,
            conflicting_info=[new_info.info_id],
            resolution_strategy=strategy,
            resolution_reasoning=reasoning,
            confidence=0.8,
            resolved_at=datetime.now().isoformat()
        )
    
    def save_updated_knowledge(self):
        """æ›´æ–°æ¸ˆã¿çŸ¥è­˜ä¿å­˜"""
        try:
            # çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
            with open(self.knowledge_db_path, 'w', encoding='utf-8') as f:
                json.dump(self.knowledge_db, f, ensure_ascii=False, indent=2)
            
            # æ–°æƒ…å ±ä¿å­˜
            self._save_new_information()
            
            # ä¿ç•™æ›´æ–°ä¿å­˜
            self._save_pending_updates()
            
            # çŸ›ç›¾è§£æ±ºä¿å­˜
            self._save_conflict_resolutions()
            
            print("[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] ğŸ’¾ æ›´æ–°æ¸ˆã¿çŸ¥è­˜ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            
        except Exception as e:
            print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âŒ çŸ¥è­˜ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_new_information(self):
        """æ–°æƒ…å ±ä¿å­˜"""
        try:
            info_data = {
                "information": [asdict(info) for info in self.new_information],
                "metadata": {
                    "total_info": len(self.new_information),
                    "last_updated": datetime.now().isoformat()
                }
            }
            
            with open(self.new_information_path, 'w', encoding='utf-8') as f:
                json.dump(info_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âš ï¸ æ–°æƒ…å ±ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_pending_updates(self):
        """ä¿ç•™æ›´æ–°ä¿å­˜"""
        try:
            updates_data = {
                "updates": {uid: asdict(update) for uid, update in self.pending_updates.items()},
                "metadata": {
                    "total_updates": len(self.pending_updates),
                    "last_updated": datetime.now().isoformat()
                }
            }
            
            with open(self.knowledge_updates_path, 'w', encoding='utf-8') as f:
                json.dump(updates_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âš ï¸ ä¿ç•™æ›´æ–°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_conflict_resolutions(self):
        """çŸ›ç›¾è§£æ±ºä¿å­˜"""
        try:
            resolutions_data = {
                "resolutions": {cid: asdict(resolution) for cid, resolution in self.conflict_resolutions.items()},
                "metadata": {
                    "total_resolutions": len(self.conflict_resolutions),
                    "last_updated": datetime.now().isoformat()
                }
            }
            
            with open(self.conflict_resolutions_path, 'w', encoding='utf-8') as f:
                json.dump(resolutions_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âš ï¸ çŸ›ç›¾è§£æ±ºä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_update_summary(self) -> Dict[str, Any]:
        """æ›´æ–°ã‚µãƒãƒªãƒ¼å–å¾—"""
        return {
            "statistics": self.update_statistics,
            "new_information": len(self.new_information),
            "pending_updates": len(self.pending_updates),
            "conflicts": len(self.conflict_resolutions),
            "recent_activity": [
                {
                    "timestamp": info.detected_at,
                    "type": info.info_type,
                    "content": info.content[:50] + "..." if len(info.content) > 50 else info.content,
                    "confidence": info.confidence
                }
                for info in list(self.new_information)[-5:]
            ]
        }
    
    def perform_comprehensive_update(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„æ›´æ–°å®Ÿè¡Œ"""
        print("[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] ğŸ”„ åŒ…æ‹¬çš„æ›´æ–°ã‚’å®Ÿè¡Œä¸­...")
        
        # çŸ¥è­˜æ›´æ–°ç”Ÿæˆ
        updates = self.generate_knowledge_updates()
        
        # çŸ›ç›¾æ¤œå‡º
        conflicts = self.detect_conflicts()
        
        # æ›´æ–°é©ç”¨
        applied = self.apply_knowledge_updates()
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.save_updated_knowledge()
        
        result = {
            "updates_generated": len(updates),
            "conflicts_detected": len(conflicts),
            "updates_applied": applied,
            "statistics": self.update_statistics
        }
        
        print("[ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°] âœ… åŒ…æ‹¬çš„æ›´æ–°å®Œäº†")
        return result