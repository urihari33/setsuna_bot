#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ  - ã›ã¤ãªBot Dæ¡ˆ Phase 2
é‡è¦ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§æ©Ÿèƒ½
"""

import os
import shutil
import json
import hashlib
import threading
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import zipfile
import tempfile

from logging_system import get_logger, get_monitor


class BackupManager:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, base_dir: str = "/mnt/d/setsuna_bot", 
                 backup_dir: str = "/mnt/d/setsuna_bot/backups"):
        """
        åˆæœŸåŒ–
        
        Args:
            base_dir: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            backup_dir: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.logger = get_logger()
        self.monitor = get_monitor()
        
        self.base_dir = Path(base_dir)
        self.backup_dir = Path(backup_dir)
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.backup_dir.mkdir(exist_ok=True)
        (self.backup_dir / "daily").mkdir(exist_ok=True)
        (self.backup_dir / "weekly").mkdir(exist_ok=True)
        (self.backup_dir / "monthly").mkdir(exist_ok=True)
        (self.backup_dir / "emergency").mkdir(exist_ok=True)
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«å®šç¾©
        self.backup_targets = {
            "youtube_knowledge": "youtube_knowledge_system/data/unified_knowledge_db.json",
            "user_preferences": "data/user_preferences.json",
            "conversation_context": "data/conversation_context.json",
            "multi_turn_conversations": "data/multi_turn_conversations.json",
            "video_conversation_history": "data/video_conversation_history.json",
            "setsuna_memory": "character/setsuna_memory_data.json",
            "setsuna_responses": "character/setsuna_responses.json",
            "setsuna_projects": "character/setsuna_projects.json",
            "response_cache": "response_cache/response_cache.json",
            "cache_stats": "response_cache/cache_stats.json"
        }
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
        self.scheduler_running = False
        self.scheduler_thread = None
        
        self.logger.info("backup_system", "__init__", "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†", {
            "base_dir": str(self.base_dir),
            "backup_dir": str(self.backup_dir),
            "targets_count": len(self.backup_targets)
        })
    
    @get_monitor().monitor_function("create_backup")
    def create_backup(self, backup_type: str = "manual", 
                     compress: bool = True, 
                     verify: bool = True) -> Optional[Path]:
        """
        ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
        
        Args:
            backup_type: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒ— (manual/daily/weekly/monthly/emergency)
            compress: ZIPåœ§ç¸®ã™ã‚‹ã‹
            verify: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾Œã«æ¤œè¨¼ã™ã‚‹ã‹
            
        Returns:
            Path: ä½œæˆã•ã‚ŒãŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ãƒ‘ã‚¹
        """
        self.logger.info("backup_system", "create_backup", f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆé–‹å§‹: {backup_type}")
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ±ºå®š
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if backup_type in ["daily", "weekly", "monthly"]:
                backup_path = self.backup_dir / backup_type / timestamp
            else:
                backup_path = self.backup_dir / "emergency" / f"{backup_type}_{timestamp}"
            
            backup_path.mkdir(parents=True, exist_ok=True)
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆä½œæˆ
            manifest = {
                "timestamp": timestamp,
                "backup_type": backup_type,
                "created_at": datetime.now().isoformat(),
                "files": {},
                "total_size": 0,
                "compressed": compress,
                "verified": verify
            }
            
            copied_files = 0
            total_size = 0
            
            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            for name, relative_path in self.backup_targets.items():
                source_path = self.base_dir / relative_path
                
                if not source_path.exists():
                    self.logger.warning("backup_system", "create_backup", 
                                      f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {relative_path}")
                    continue
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
                dest_path = backup_path / relative_path
                dest_path.parent.mkdir(parents=True, exist_ok=True)
                
                shutil.copy2(source_path, dest_path)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¨˜éŒ²
                file_size = source_path.stat().st_size
                file_hash = self._calculate_file_hash(source_path)
                
                manifest["files"][name] = {
                    "path": relative_path,
                    "size": file_size,
                    "hash": file_hash,
                    "modified": datetime.fromtimestamp(source_path.stat().st_mtime).isoformat()
                }
                
                copied_files += 1
                total_size += file_size
                
                self.logger.debug("backup_system", "create_backup", 
                                f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼å®Œäº†: {name}", {
                                    "source": str(source_path),
                                    "dest": str(dest_path),
                                    "size": file_size
                                })
            
            manifest["total_size"] = total_size
            manifest["files_count"] = copied_files
            
            # ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            manifest_path = backup_path / "backup_manifest.json"
            with open(manifest_path, 'w', encoding='utf-8') as f:
                json.dump(manifest, f, ensure_ascii=False, indent=2)
            
            self.logger.info("backup_system", "create_backup", 
                           f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {copied_files}ä»¶", {
                               "total_size_mb": total_size / 1024 / 1024,
                               "backup_path": str(backup_path)
                           })
            
            # ZIPåœ§ç¸®
            final_path = backup_path
            if compress:
                final_path = self._compress_backup(backup_path)
                if final_path:
                    # å…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
                    shutil.rmtree(backup_path)
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼
            if verify and final_path:
                if self._verify_backup(final_path):
                    self.logger.info("backup_system", "create_backup", "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼æˆåŠŸ")
                else:
                    self.logger.error("backup_system", "create_backup", "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼å¤±æ•—")
            
            return final_path
            
        except Exception as e:
            self.logger.error("backup_system", "create_backup", f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®SHA256ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        hash_sha256 = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            self.logger.error("backup_system", "_calculate_file_hash", f"ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def _compress_backup(self, backup_path: Path) -> Optional[Path]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ ZIP åœ§ç¸®"""
        try:
            zip_path = backup_path.with_suffix('.zip')
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=9) as zipf:
                for file_path in backup_path.rglob('*'):
                    if file_path.is_file():
                        arcname = file_path.relative_to(backup_path)
                        zipf.write(file_path, arcname)
            
            # åœ§ç¸®ç‡è¨ˆç®—
            original_size = sum(f.stat().st_size for f in backup_path.rglob('*') if f.is_file())
            compressed_size = zip_path.stat().st_size
            compression_ratio = (1 - compressed_size / original_size) * 100
            
            self.logger.info("backup_system", "_compress_backup", "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—åœ§ç¸®å®Œäº†", {
                "original_size_mb": original_size / 1024 / 1024,
                "compressed_size_mb": compressed_size / 1024 / 1024,
                "compression_ratio": compression_ratio
            })
            
            return zip_path
            
        except Exception as e:
            self.logger.error("backup_system", "_compress_backup", f"åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _verify_backup(self, backup_path: Path) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ•´åˆæ€§ã‚’æ¤œè¨¼"""
        try:
            if backup_path.suffix == '.zip':
                return self._verify_compressed_backup(backup_path)
            else:
                return self._verify_uncompressed_backup(backup_path)
        except Exception as e:
            self.logger.error("backup_system", "_verify_backup", f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _verify_compressed_backup(self, zip_path: Path) -> bool:
        """åœ§ç¸®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ¤œè¨¼"""
        try:
            with zipfile.ZipFile(zip_path, 'r') as zipf:
                # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                bad_file = zipf.testzip()
                if bad_file:
                    self.logger.error("backup_system", "_verify_compressed_backup", 
                                    f"ç ´æãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {bad_file}")
                    return False
                
                # ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
                try:
                    manifest_data = zipf.read('backup_manifest.json')
                    manifest = json.loads(manifest_data.decode('utf-8'))
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ç¢ºèª
                    expected_files = len(manifest.get("files", {})) + 1  # +1 for manifest
                    actual_files = len(zipf.namelist())
                    
                    if expected_files != actual_files:
                        self.logger.error("backup_system", "_verify_compressed_backup", 
                                        f"ãƒ•ã‚¡ã‚¤ãƒ«æ•°ä¸ä¸€è‡´: æœŸå¾…{expected_files}, å®Ÿéš›{actual_files}")
                        return False
                    
                    self.logger.debug("backup_system", "_verify_compressed_backup", 
                                    f"ZIPæ¤œè¨¼æˆåŠŸ: {actual_files}ãƒ•ã‚¡ã‚¤ãƒ«")
                    return True
                    
                except KeyError:
                    self.logger.error("backup_system", "_verify_compressed_backup", 
                                    "ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return False
                    
        except Exception as e:
            self.logger.error("backup_system", "_verify_compressed_backup", f"ZIPæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _verify_uncompressed_backup(self, backup_path: Path) -> bool:
        """éåœ§ç¸®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ¤œè¨¼"""
        try:
            manifest_path = backup_path / "backup_manifest.json"
            if not manifest_path.exists():
                self.logger.error("backup_system", "_verify_uncompressed_backup", 
                                "ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return False
            
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest = json.load(f)
            
            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
            for name, file_info in manifest.get("files", {}).items():
                file_path = backup_path / file_info["path"]
                
                if not file_path.exists():
                    self.logger.error("backup_system", "_verify_uncompressed_backup", 
                                    f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_info['path']}")
                    return False
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
                actual_size = file_path.stat().st_size
                expected_size = file_info["size"]
                
                if actual_size != expected_size:
                    self.logger.error("backup_system", "_verify_uncompressed_backup", 
                                    f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºä¸ä¸€è‡´: {file_info['path']}")
                    return False
                
                # ãƒãƒƒã‚·ãƒ¥ç¢ºèª
                actual_hash = self._calculate_file_hash(file_path)
                expected_hash = file_info["hash"]
                
                if actual_hash != expected_hash:
                    self.logger.error("backup_system", "_verify_uncompressed_backup", 
                                    f"ãƒãƒƒã‚·ãƒ¥ä¸ä¸€è‡´: {file_info['path']}")
                    return False
            
            self.logger.debug("backup_system", "_verify_uncompressed_backup", 
                            f"æ¤œè¨¼æˆåŠŸ: {len(manifest.get('files', {}))}ãƒ•ã‚¡ã‚¤ãƒ«")
            return True
            
        except Exception as e:
            self.logger.error("backup_system", "_verify_uncompressed_backup", f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def list_backups(self, backup_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§ã‚’å–å¾—"""
        try:
            backups = []
            
            search_dirs = [self.backup_dir / "emergency"]
            if backup_type:
                if backup_type in ["daily", "weekly", "monthly"]:
                    search_dirs = [self.backup_dir / backup_type]
            else:
                search_dirs.extend([
                    self.backup_dir / "daily",
                    self.backup_dir / "weekly", 
                    self.backup_dir / "monthly"
                ])
            
            for search_dir in search_dirs:
                if not search_dir.exists():
                    continue
                
                for item in search_dir.iterdir():
                    if item.is_dir():
                        manifest_path = item / "backup_manifest.json"
                    elif item.suffix == '.zip':
                        # ZIPå†…ã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆç¢ºèª
                        try:
                            with zipfile.ZipFile(item, 'r') as zipf:
                                manifest_data = zipf.read('backup_manifest.json')
                                manifest = json.loads(manifest_data.decode('utf-8'))
                        except:
                            continue
                    else:
                        continue
                    
                    if item.is_dir() and manifest_path.exists():
                        with open(manifest_path, 'r', encoding='utf-8') as f:
                            manifest = json.load(f)
                    
                    backup_info = {
                        "path": str(item),
                        "name": item.name,
                        "type": manifest.get("backup_type", "unknown"),
                        "created_at": manifest.get("created_at", ""),
                        "files_count": manifest.get("files_count", 0),
                        "total_size": manifest.get("total_size", 0),
                        "compressed": manifest.get("compressed", False),
                        "verified": manifest.get("verified", False)
                    }
                    backups.append(backup_info)
            
            # ä½œæˆæ—¥æ™‚ã§é™é †ã‚½ãƒ¼ãƒˆ
            backups.sort(key=lambda x: x["created_at"], reverse=True)
            
            return backups
            
        except Exception as e:
            self.logger.error("backup_system", "list_backups", f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def cleanup_old_backups(self, retention_days: int = 30):
        """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤"""
        try:
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            deleted_count = 0
            deleted_size = 0
            
            for backup_type in ["daily", "weekly", "monthly", "emergency"]:
                backup_type_dir = self.backup_dir / backup_type
                if not backup_type_dir.exists():
                    continue
                
                for item in backup_type_dir.iterdir():
                    # ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆæ—¥æ™‚ç¢ºèª
                    item_time = datetime.fromtimestamp(item.stat().st_mtime)
                    
                    if item_time < cutoff_date:
                        item_size = self._get_size(item)
                        
                        if item.is_dir():
                            shutil.rmtree(item)
                        else:
                            item.unlink()
                        
                        deleted_count += 1
                        deleted_size += item_size
                        
                        self.logger.info("backup_system", "cleanup_old_backups", 
                                       f"å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤: {item.name}")
            
            self.logger.info("backup_system", "cleanup_old_backups", 
                           f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {deleted_count}ä»¶å‰Šé™¤", {
                               "deleted_size_mb": deleted_size / 1024 / 1024,
                               "retention_days": retention_days
                           })
            
        except Exception as e:
            self.logger.error("backup_system", "cleanup_old_backups", f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _get_size(self, path: Path) -> int:
        """ãƒ‘ã‚¹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰ã®ã‚µã‚¤ã‚ºã‚’å–å¾—"""
        if path.is_file():
            return path.stat().st_size
        elif path.is_dir():
            return sum(f.stat().st_size for f in path.rglob('*') if f.is_file())
        return 0


# ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
if __name__ == "__main__":
    print("ğŸ§ª ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    backup_manager = BackupManager()
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆãƒ†ã‚¹ãƒˆ
    print("ğŸ“¦ ãƒ†ã‚¹ãƒˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆä¸­...")
    backup_path = backup_manager.create_backup("test", compress=True, verify=True)
    
    if backup_path:
        print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆæˆåŠŸ: {backup_path}")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§è¡¨ç¤º
        backups = backup_manager.list_backups()
        print(f"ğŸ“‹ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§: {len(backups)}ä»¶")
        for backup in backups[:3]:  # æœ€æ–°3ä»¶è¡¨ç¤º
            print(f"   - {backup['name']}: {backup['files_count']}ãƒ•ã‚¡ã‚¤ãƒ«, {backup['total_size']/1024:.1f}KB")
    else:
        print("âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—")
    
    print("âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†")